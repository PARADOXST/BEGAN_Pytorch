{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--manualSeed'], dest='manualSeed', nargs=None, const=None, default=None, type=<type 'int'>, choices=None, help='manual seed', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=True, help='cifar10 | lsun | imagenet | folder | lfw ')\n",
    "parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=16, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=64, help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batchSize=16, beta1=0.5, cuda=True, dataroot='/home/ubuntu/Python/BEGAN_Pytorch/data/work/', dataset='folder', imageSize=64, lr=0.0001, manualSeed=None, ndf=64, netD='', netG='', ngf=64, niter=25, nz=64, outf='.', workers=2)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(['--dataset', 'folder', '--dataroot', '/home/ubuntu/Python/BEGAN_Pytorch/data/work/', '--cuda', '--niter', '25'])\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  4848\n"
     ]
    }
   ],
   "source": [
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root=opt.dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Scale(opt.imageSize),\n",
    "                                   transforms.CenterCrop(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "assert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "nc = 3\n",
    "gamma = 0.5\n",
    "k = 0\n",
    "lr_decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def G_weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.002)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def D_weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class G_CNN(nn.Module):\n",
    "    def __init__(self, embedding_size, kernel_num):\n",
    "        super(G_CNN, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        FC_start_size = 8 * 8 * self.kernel_num\n",
    "        self.start = nn.Linear(embedding_size, FC_start_size)\n",
    "        #all layers\n",
    "        self.main_arch = nn.Sequential(\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #last layer\n",
    "            #nn.Conv2d(in_channels=kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        in_data = self.start(in_data.view(-1, 64))\n",
    "        in_data = in_data.view(-1, self.kernel_num, 8, 8)\n",
    "        output = self.main_arch(in_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netG = G_CNN(embedding_size=nz, kernel_num=64)\n",
    "netG.apply(G_weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class D_CNN(nn.Module):\n",
    "    def __init__(self, embedding_size, base_kernel_num):\n",
    "        super(D_CNN, self).__init__()\n",
    "        self.base_kernel_num = base_kernel_num\n",
    "        \n",
    "        #encoder w/o flatten and FC\n",
    "        self.encoder_wo_FC = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=4*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=4*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        FC_mid1_size = 8 * 8 * 4 * self.base_kernel_num\n",
    "        FC_mid2_size = 8 * 8 * self.base_kernel_num\n",
    "        self.FC1 = nn.Linear(FC_mid1_size, embedding_size)\n",
    "        self.FC2 = nn.Linear(embedding_size, FC_mid2_size)\n",
    "        self.decoder_wo_FC = nn.Sequential(\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #last layer\n",
    "            #nn.Conv2d(in_channels=base_kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        in_data = self.encoder_wo_FC(in_data)\n",
    "        \n",
    "        in_data = in_data.view(-1, 4*self.base_kernel_num * 8 * 8);\n",
    "        in_data = self.FC1(in_data)\n",
    "        in_data = self.FC2(in_data)\n",
    "        in_data = in_data.view(-1, self.base_kernel_num, 8, 8)\n",
    "        \n",
    "        output = self.decoder_wo_FC(in_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD = D_CNN(embedding_size=nz, base_kernel_num=64)\n",
    "netD.apply(D_weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "input = torch.FloatTensor(opt.batchSize, 3, opt.imageSize, opt.imageSize)\n",
    "noise = torch.FloatTensor(opt.batchSize, nz)\n",
    "fixed_noise = torch.FloatTensor(opt.batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "if opt.cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion_L1.cuda()\n",
    "    input = input.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "input = Variable(input)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/12663] Loss_D: 0.5029 Loss_G: 0.0282 Convergence: 0.7249 k= 0.000224\n",
      "[0/25][10/12663] Loss_D: 0.5050 Loss_G: 0.0686 Convergence: 0.6862 k= 0.002328\n",
      "[0/25][20/12663] Loss_D: 0.4624 Loss_G: 0.1094 Convergence: 0.5880 k= 0.003515\n",
      "[0/25][30/12663] Loss_D: 0.5074 Loss_G: 0.1382 Convergence: 0.6202 k= 0.004865\n",
      "[0/25][40/12663] Loss_D: 0.4338 Loss_G: 0.1859 Convergence: 0.4643 k= 0.005938\n",
      "[0/25][50/12663] Loss_D: 0.4514 Loss_G: 0.2008 Convergence: 0.4724 k= 0.006462\n",
      "[0/25][60/12663] Loss_D: 0.3892 Loss_G: 0.2266 Convergence: 0.4215 k= 0.006372\n",
      "[0/25][70/12663] Loss_D: 0.4043 Loss_G: 0.3506 Convergence: 0.5511 k= 0.005957\n",
      "[0/25][80/12663] Loss_D: 0.3280 Loss_G: 0.1493 Convergence: 0.3365 k= 0.005277\n",
      "[0/25][90/12663] Loss_D: 0.3017 Loss_G: 0.2128 Convergence: 0.3591 k= 0.005011\n",
      "[0/25][100/12663] Loss_D: 0.3108 Loss_G: 0.2065 Convergence: 0.3590 k= 0.004695\n",
      "[0/25][110/12663] Loss_D: 0.3145 Loss_G: 0.1187 Convergence: 0.3431 k= 0.004744\n",
      "[0/25][120/12663] Loss_D: 0.2767 Loss_G: 0.1214 Convergence: 0.2712 k= 0.004955\n",
      "[0/25][130/12663] Loss_D: 0.2477 Loss_G: 0.0871 Convergence: 0.2659 k= 0.005108\n",
      "[0/25][140/12663] Loss_D: 0.2630 Loss_G: 0.1069 Convergence: 0.2864 k= 0.005383\n",
      "[0/25][150/12663] Loss_D: 0.2624 Loss_G: 0.1600 Convergence: 0.2924 k= 0.005456\n",
      "[0/25][160/12663] Loss_D: 0.2825 Loss_G: 0.0873 Convergence: 0.3329 k= 0.005732\n",
      "[0/25][170/12663] Loss_D: 0.2555 Loss_G: 0.1308 Convergence: 0.2615 k= 0.005895\n",
      "[0/25][180/12663] Loss_D: 0.2810 Loss_G: 0.1292 Convergence: 0.2921 k= 0.006078\n",
      "[0/25][190/12663] Loss_D: 0.2619 Loss_G: 0.1143 Convergence: 0.2756 k= 0.006298\n",
      "[0/25][200/12663] Loss_D: 0.2540 Loss_G: 0.1224 Convergence: 0.2625 k= 0.006590\n",
      "[0/25][210/12663] Loss_D: 0.2622 Loss_G: 0.1708 Convergence: 0.3079 k= 0.006571\n",
      "[0/25][220/12663] Loss_D: 0.2480 Loss_G: 0.1116 Convergence: 0.2623 k= 0.006629\n",
      "[0/25][230/12663] Loss_D: 0.2441 Loss_G: 0.0917 Convergence: 0.2699 k= 0.006701\n",
      "[0/25][240/12663] Loss_D: 0.2667 Loss_G: 0.0861 Convergence: 0.3088 k= 0.006907\n",
      "[0/25][250/12663] Loss_D: 0.2287 Loss_G: 0.2159 Convergence: 0.3435 k= 0.006892\n",
      "[0/25][260/12663] Loss_D: 0.2696 Loss_G: 0.1254 Convergence: 0.2494 k= 0.006767\n",
      "[0/25][270/12663] Loss_D: 0.2441 Loss_G: 0.0889 Convergence: 0.2709 k= 0.006808\n",
      "[0/25][280/12663] Loss_D: 0.2470 Loss_G: 0.1393 Convergence: 0.2645 k= 0.006915\n",
      "[0/25][290/12663] Loss_D: 0.2543 Loss_G: 0.1248 Convergence: 0.2675 k= 0.006936\n",
      "[0/25][300/12663] Loss_D: 0.2323 Loss_G: 0.1073 Convergence: 0.2491 k= 0.007037\n",
      "[0/25][310/12663] Loss_D: 0.2285 Loss_G: 0.0937 Convergence: 0.2460 k= 0.007283\n",
      "[0/25][320/12663] Loss_D: 0.2285 Loss_G: 0.1151 Convergence: 0.2330 k= 0.007385\n",
      "[0/25][330/12663] Loss_D: 0.2437 Loss_G: 0.0884 Convergence: 0.2738 k= 0.007560\n",
      "[0/25][340/12663] Loss_D: 0.2360 Loss_G: 0.0968 Convergence: 0.2571 k= 0.007743\n",
      "[0/25][350/12663] Loss_D: 0.2236 Loss_G: 0.0941 Convergence: 0.2410 k= 0.007918\n",
      "[0/25][360/12663] Loss_D: 0.2240 Loss_G: 0.0930 Convergence: 0.2419 k= 0.008126\n",
      "[0/25][370/12663] Loss_D: 0.2283 Loss_G: 0.0910 Convergence: 0.2507 k= 0.008355\n",
      "[0/25][380/12663] Loss_D: 0.2352 Loss_G: 0.0752 Convergence: 0.2760 k= 0.008631\n",
      "[0/25][390/12663] Loss_D: 0.2365 Loss_G: 0.1946 Convergence: 0.3176 k= 0.008591\n",
      "[0/25][400/12663] Loss_D: 0.2283 Loss_G: 0.1021 Convergence: 0.2411 k= 0.008576\n",
      "[0/25][410/12663] Loss_D: 0.2285 Loss_G: 0.0983 Convergence: 0.2248 k= 0.008606\n",
      "[0/25][420/12663] Loss_D: 0.2206 Loss_G: 0.1324 Convergence: 0.2430 k= 0.008592\n",
      "[0/25][430/12663] Loss_D: 0.2225 Loss_G: 0.0849 Convergence: 0.2459 k= 0.008817\n",
      "[0/25][440/12663] Loss_D: 0.1998 Loss_G: 0.0926 Convergence: 0.2057 k= 0.008870\n",
      "[0/25][450/12663] Loss_D: 0.2302 Loss_G: 0.0914 Convergence: 0.2554 k= 0.009011\n",
      "[0/25][460/12663] Loss_D: 0.2139 Loss_G: 0.1066 Convergence: 0.2157 k= 0.009071\n",
      "[0/25][470/12663] Loss_D: 0.2350 Loss_G: 0.0950 Convergence: 0.2547 k= 0.009155\n",
      "[0/25][480/12663] Loss_D: 0.2068 Loss_G: 0.0948 Convergence: 0.2178 k= 0.009260\n",
      "[0/25][490/12663] Loss_D: 0.1991 Loss_G: 0.0962 Convergence: 0.2036 k= 0.009390\n",
      "[0/25][500/12663] Loss_D: 0.2097 Loss_G: 0.1056 Convergence: 0.2109 k= 0.009472\n",
      "[0/25][510/12663] Loss_D: 0.1918 Loss_G: 0.1269 Convergence: 0.2235 k= 0.009369\n",
      "[0/25][520/12663] Loss_D: 0.2194 Loss_G: 0.1775 Convergence: 0.2916 k= 0.009304\n",
      "[0/25][530/12663] Loss_D: 0.2227 Loss_G: 0.1003 Convergence: 0.2390 k= 0.009264\n",
      "[0/25][540/12663] Loss_D: 0.1995 Loss_G: 0.0861 Convergence: 0.2120 k= 0.009272\n",
      "[0/25][550/12663] Loss_D: 0.2369 Loss_G: 0.1372 Convergence: 0.2551 k= 0.009186\n",
      "[0/25][560/12663] Loss_D: 0.2228 Loss_G: 0.1201 Convergence: 0.2312 k= 0.009092\n",
      "[0/25][570/12663] Loss_D: 0.1995 Loss_G: 0.0965 Convergence: 0.2053 k= 0.009022\n",
      "[0/25][580/12663] Loss_D: 0.1940 Loss_G: 0.1150 Convergence: 0.2124 k= 0.008978\n",
      "[0/25][590/12663] Loss_D: 0.2177 Loss_G: 0.1253 Convergence: 0.2340 k= 0.008960\n",
      "[0/25][600/12663] Loss_D: 0.2121 Loss_G: 0.1396 Convergence: 0.2456 k= 0.008854\n",
      "[0/25][610/12663] Loss_D: 0.2003 Loss_G: 0.0825 Convergence: 0.2176 k= 0.008918\n",
      "[0/25][620/12663] Loss_D: 0.2122 Loss_G: 0.1128 Convergence: 0.2189 k= 0.008982\n",
      "[0/25][630/12663] Loss_D: 0.1930 Loss_G: 0.1507 Convergence: 0.2463 k= 0.008804\n",
      "[0/25][640/12663] Loss_D: 0.2344 Loss_G: 0.1100 Convergence: 0.2335 k= 0.008691\n",
      "[0/25][650/12663] Loss_D: 0.1987 Loss_G: 0.1127 Convergence: 0.2120 k= 0.008612\n",
      "[0/25][660/12663] Loss_D: 0.2063 Loss_G: 0.1196 Convergence: 0.2220 k= 0.008420\n",
      "[0/25][670/12663] Loss_D: 0.2181 Loss_G: 0.1024 Convergence: 0.2247 k= 0.008289\n",
      "[0/25][680/12663] Loss_D: 0.1816 Loss_G: 0.0938 Convergence: 0.1835 k= 0.008253\n",
      "[0/25][690/12663] Loss_D: 0.2116 Loss_G: 0.1126 Convergence: 0.2178 k= 0.008287\n",
      "[0/25][700/12663] Loss_D: 0.1987 Loss_G: 0.1247 Convergence: 0.2241 k= 0.008244\n",
      "[0/25][710/12663] Loss_D: 0.2010 Loss_G: 0.1463 Convergence: 0.2470 k= 0.008130\n",
      "[0/25][720/12663] Loss_D: 0.1592 Loss_G: 0.1370 Convergence: 0.2164 k= 0.007959\n",
      "[0/25][730/12663] Loss_D: 0.1867 Loss_G: 0.0902 Convergence: 0.1907 k= 0.007962\n",
      "[0/25][740/12663] Loss_D: 0.2146 Loss_G: 0.1735 Convergence: 0.2843 k= 0.007920\n",
      "[0/25][750/12663] Loss_D: 0.1972 Loss_G: 0.1159 Convergence: 0.2118 k= 0.007737\n",
      "[0/25][760/12663] Loss_D: 0.2009 Loss_G: 0.1023 Convergence: 0.2012 k= 0.007841\n",
      "[0/25][770/12663] Loss_D: 0.1869 Loss_G: 0.0846 Convergence: 0.1900 k= 0.007907\n",
      "[0/25][780/12663] Loss_D: 0.1979 Loss_G: 0.0896 Convergence: 0.2107 k= 0.007997\n",
      "[0/25][790/12663] Loss_D: 0.1827 Loss_G: 0.0952 Convergence: 0.1896 k= 0.008061\n",
      "[0/25][800/12663] Loss_D: 0.1773 Loss_G: 0.0884 Convergence: 0.1776 k= 0.008109\n",
      "[0/25][810/12663] Loss_D: 0.1768 Loss_G: 0.0809 Convergence: 0.1915 k= 0.008159\n",
      "[0/25][820/12663] Loss_D: 0.1882 Loss_G: 0.0750 Convergence: 0.2137 k= 0.008184\n",
      "[0/25][830/12663] Loss_D: 0.2039 Loss_G: 0.0866 Convergence: 0.2227 k= 0.008258\n",
      "[0/25][840/12663] Loss_D: 0.2014 Loss_G: 0.0896 Convergence: 0.2171 k= 0.008241\n",
      "[0/25][850/12663] Loss_D: 0.1940 Loss_G: 0.0879 Convergence: 0.2145 k= 0.008326\n",
      "[0/25][860/12663] Loss_D: 0.2014 Loss_G: 0.0985 Convergence: 0.1983 k= 0.008290\n",
      "[0/25][870/12663] Loss_D: 0.1956 Loss_G: 0.0985 Convergence: 0.1955 k= 0.008279\n",
      "[0/25][880/12663] Loss_D: 0.1932 Loss_G: 0.1169 Convergence: 0.2110 k= 0.008225\n",
      "[0/25][890/12663] Loss_D: 0.1919 Loss_G: 0.1179 Convergence: 0.2141 k= 0.008049\n",
      "[0/25][900/12663] Loss_D: 0.1943 Loss_G: 0.1310 Convergence: 0.2285 k= 0.007764\n",
      "[0/25][910/12663] Loss_D: 0.1789 Loss_G: 0.1364 Convergence: 0.2261 k= 0.007424\n",
      "[0/25][920/12663] Loss_D: 0.1873 Loss_G: 0.1326 Convergence: 0.2248 k= 0.007097\n",
      "[0/25][930/12663] Loss_D: 0.1874 Loss_G: 0.1291 Convergence: 0.2217 k= 0.006680\n",
      "[0/25][940/12663] Loss_D: 0.1905 Loss_G: 0.1320 Convergence: 0.2255 k= 0.006295\n",
      "[0/25][950/12663] Loss_D: 0.1786 Loss_G: 0.1409 Convergence: 0.2309 k= 0.005906\n",
      "[0/25][960/12663] Loss_D: 0.1829 Loss_G: 0.1417 Convergence: 0.2325 k= 0.005361\n",
      "[0/25][970/12663] Loss_D: 0.1971 Loss_G: 0.1367 Convergence: 0.2352 k= 0.004982\n",
      "[0/25][980/12663] Loss_D: 0.2058 Loss_G: 0.1269 Convergence: 0.2276 k= 0.004610\n",
      "[0/25][990/12663] Loss_D: 0.2055 Loss_G: 0.0952 Convergence: 0.2193 k= 0.004347\n",
      "[0/25][1000/12663] Loss_D: 0.1770 Loss_G: 0.1003 Convergence: 0.1869 k= 0.003961\n",
      "[0/25][1010/12663] Loss_D: 0.2015 Loss_G: 0.1091 Convergence: 0.2100 k= 0.003803\n",
      "[0/25][1020/12663] Loss_D: 0.2105 Loss_G: 0.1276 Convergence: 0.2336 k= 0.003518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][1030/12663] Loss_D: 0.1697 Loss_G: 0.1103 Convergence: 0.1970 k= 0.003198\n",
      "[0/25][1040/12663] Loss_D: 0.2085 Loss_G: 0.0915 Convergence: 0.2183 k= 0.003130\n",
      "[0/25][1050/12663] Loss_D: 0.1926 Loss_G: 0.0827 Convergence: 0.2046 k= 0.003143\n",
      "[0/25][1060/12663] Loss_D: 0.1788 Loss_G: 0.0979 Convergence: 0.1867 k= 0.003200\n",
      "[0/25][1070/12663] Loss_D: 0.1799 Loss_G: 0.1001 Convergence: 0.1893 k= 0.003245\n",
      "[0/25][1080/12663] Loss_D: 0.1687 Loss_G: 0.0983 Convergence: 0.1829 k= 0.003368\n",
      "[0/25][1090/12663] Loss_D: 0.1755 Loss_G: 0.0701 Convergence: 0.1855 k= 0.003542\n",
      "[0/25][1100/12663] Loss_D: 0.1773 Loss_G: 0.0942 Convergence: 0.1831 k= 0.003571\n",
      "[0/25][1110/12663] Loss_D: 0.1768 Loss_G: 0.0846 Convergence: 0.1754 k= 0.003593\n",
      "[0/25][1120/12663] Loss_D: 0.1882 Loss_G: 0.0778 Convergence: 0.2028 k= 0.003711\n",
      "[0/25][1130/12663] Loss_D: 0.1692 Loss_G: 0.0762 Convergence: 0.1762 k= 0.003794\n",
      "[0/25][1140/12663] Loss_D: 0.1631 Loss_G: 0.0780 Convergence: 0.1634 k= 0.003790\n",
      "[0/25][1150/12663] Loss_D: 0.1939 Loss_G: 0.1019 Convergence: 0.1980 k= 0.003787\n",
      "[0/25][1160/12663] Loss_D: 0.1775 Loss_G: 0.0758 Convergence: 0.1872 k= 0.003870\n",
      "[0/25][1170/12663] Loss_D: 0.1743 Loss_G: 0.0767 Convergence: 0.1836 k= 0.003901\n",
      "[0/25][1180/12663] Loss_D: 0.1941 Loss_G: 0.1085 Convergence: 0.2063 k= 0.003954\n",
      "[0/25][1190/12663] Loss_D: 0.1786 Loss_G: 0.0697 Convergence: 0.1944 k= 0.003939\n",
      "[0/25][1200/12663] Loss_D: 0.1892 Loss_G: 0.0920 Convergence: 0.1934 k= 0.003926\n",
      "[0/25][1210/12663] Loss_D: 0.1816 Loss_G: 0.0771 Convergence: 0.1923 k= 0.003939\n",
      "[0/25][1220/12663] Loss_D: 0.1849 Loss_G: 0.0754 Convergence: 0.2027 k= 0.003872\n",
      "[0/25][1230/12663] Loss_D: 0.1859 Loss_G: 0.0962 Convergence: 0.1867 k= 0.003889\n",
      "[0/25][1240/12663] Loss_D: 0.1842 Loss_G: 0.0820 Convergence: 0.1944 k= 0.004014\n",
      "[0/25][1250/12663] Loss_D: 0.1768 Loss_G: 0.0819 Convergence: 0.1819 k= 0.004068\n",
      "[0/25][1260/12663] Loss_D: 0.1773 Loss_G: 0.1077 Convergence: 0.1971 k= 0.004040\n",
      "[0/25][1270/12663] Loss_D: 0.1871 Loss_G: 0.0699 Convergence: 0.2155 k= 0.003960\n",
      "[0/25][1280/12663] Loss_D: 0.1884 Loss_G: 0.0859 Convergence: 0.1964 k= 0.004017\n",
      "[0/25][1290/12663] Loss_D: 0.1989 Loss_G: 0.0792 Convergence: 0.2189 k= 0.004165\n",
      "[0/25][1300/12663] Loss_D: 0.1648 Loss_G: 0.0782 Convergence: 0.1704 k= 0.004325\n",
      "[0/25][1310/12663] Loss_D: 0.1750 Loss_G: 0.0702 Convergence: 0.1902 k= 0.004471\n",
      "[0/25][1320/12663] Loss_D: 0.1789 Loss_G: 0.0855 Convergence: 0.1803 k= 0.004555\n",
      "[0/25][1330/12663] Loss_D: 0.1604 Loss_G: 0.0666 Convergence: 0.1718 k= 0.004627\n",
      "[0/25][1340/12663] Loss_D: 0.1624 Loss_G: 0.0795 Convergence: 0.1627 k= 0.004771\n",
      "[0/25][1350/12663] Loss_D: 0.1755 Loss_G: 0.0593 Convergence: 0.2018 k= 0.004913\n",
      "[0/25][1360/12663] Loss_D: 0.1770 Loss_G: 0.0896 Convergence: 0.1763 k= 0.004948\n",
      "[0/25][1370/12663] Loss_D: 0.1689 Loss_G: 0.0615 Convergence: 0.1914 k= 0.005129\n",
      "[0/25][1380/12663] Loss_D: 0.1752 Loss_G: 0.0830 Convergence: 0.1777 k= 0.005196\n",
      "[0/25][1390/12663] Loss_D: 0.1835 Loss_G: 0.0735 Convergence: 0.1992 k= 0.005285\n",
      "[0/25][1400/12663] Loss_D: 0.1862 Loss_G: 0.0806 Convergence: 0.1978 k= 0.005412\n",
      "[0/25][1410/12663] Loss_D: 0.1776 Loss_G: 0.0788 Convergence: 0.1851 k= 0.005483\n",
      "[0/25][1420/12663] Loss_D: 0.1830 Loss_G: 0.1134 Convergence: 0.2061 k= 0.005543\n",
      "[0/25][1430/12663] Loss_D: 0.1615 Loss_G: 0.0678 Convergence: 0.1799 k= 0.005574\n",
      "[0/25][1440/12663] Loss_D: 0.1650 Loss_G: 0.0894 Convergence: 0.1714 k= 0.005711\n",
      "[0/25][1450/12663] Loss_D: 0.1817 Loss_G: 0.1068 Convergence: 0.1960 k= 0.005758\n",
      "[0/25][1460/12663] Loss_D: 0.1790 Loss_G: 0.0692 Convergence: 0.1993 k= 0.005804\n",
      "[0/25][1470/12663] Loss_D: 0.1527 Loss_G: 0.1167 Convergence: 0.1935 k= 0.005822\n",
      "[0/25][1480/12663] Loss_D: 0.1578 Loss_G: 0.0752 Convergence: 0.1591 k= 0.005809\n",
      "[0/25][1490/12663] Loss_D: 0.1652 Loss_G: 0.0703 Convergence: 0.1766 k= 0.005852\n",
      "[0/25][1500/12663] Loss_D: 0.1637 Loss_G: 0.0978 Convergence: 0.1791 k= 0.005844\n",
      "[0/25][1510/12663] Loss_D: 0.1664 Loss_G: 0.0714 Convergence: 0.1799 k= 0.005931\n",
      "[0/25][1520/12663] Loss_D: 0.1794 Loss_G: 0.0763 Convergence: 0.1914 k= 0.006065\n",
      "[0/25][1530/12663] Loss_D: 0.1932 Loss_G: 0.0721 Convergence: 0.2170 k= 0.006224\n",
      "[0/25][1540/12663] Loss_D: 0.1721 Loss_G: 0.0777 Convergence: 0.1787 k= 0.006281\n",
      "[0/25][1550/12663] Loss_D: 0.1729 Loss_G: 0.1457 Convergence: 0.2329 k= 0.006214\n",
      "[0/25][1560/12663] Loss_D: 0.1721 Loss_G: 0.0606 Convergence: 0.2058 k= 0.006070\n",
      "[0/25][1570/12663] Loss_D: 0.1692 Loss_G: 0.0733 Convergence: 0.1721 k= 0.005956\n",
      "[0/25][1580/12663] Loss_D: 0.1770 Loss_G: 0.0805 Convergence: 0.1822 k= 0.005926\n",
      "[0/25][1590/12663] Loss_D: 0.1614 Loss_G: 0.0799 Convergence: 0.1627 k= 0.005991\n",
      "[0/25][1600/12663] Loss_D: 0.1870 Loss_G: 0.0974 Convergence: 0.1904 k= 0.005878\n",
      "[0/25][1610/12663] Loss_D: 0.1675 Loss_G: 0.0930 Convergence: 0.1767 k= 0.005719\n",
      "[0/25][1620/12663] Loss_D: 0.1824 Loss_G: 0.0729 Convergence: 0.2004 k= 0.005709\n",
      "[0/25][1630/12663] Loss_D: 0.1694 Loss_G: 0.0971 Convergence: 0.1811 k= 0.005635\n",
      "[0/25][1640/12663] Loss_D: 0.1714 Loss_G: 0.0901 Convergence: 0.1756 k= 0.005584\n",
      "[0/25][1650/12663] Loss_D: 0.1696 Loss_G: 0.0990 Convergence: 0.1837 k= 0.005518\n",
      "[0/25][1660/12663] Loss_D: 0.1699 Loss_G: 0.0946 Convergence: 0.1799 k= 0.005583\n",
      "[0/25][1670/12663] Loss_D: 0.1573 Loss_G: 0.0756 Convergence: 0.1581 k= 0.005638\n",
      "[0/25][1680/12663] Loss_D: 0.1584 Loss_G: 0.0627 Convergence: 0.1718 k= 0.005818\n",
      "[0/25][1690/12663] Loss_D: 0.1661 Loss_G: 0.0686 Convergence: 0.1851 k= 0.006070\n",
      "[0/25][1700/12663] Loss_D: 0.1658 Loss_G: 0.0689 Convergence: 0.1808 k= 0.006321\n",
      "[0/25][1710/12663] Loss_D: 0.1620 Loss_G: 0.0745 Convergence: 0.1679 k= 0.006540\n",
      "[0/25][1720/12663] Loss_D: 0.2006 Loss_G: 0.0551 Convergence: 0.2501 k= 0.006666\n",
      "[0/25][1730/12663] Loss_D: 0.1821 Loss_G: 0.0587 Convergence: 0.2092 k= 0.006744\n",
      "[0/25][1740/12663] Loss_D: 0.1630 Loss_G: 0.0609 Convergence: 0.1822 k= 0.006949\n",
      "[0/25][1750/12663] Loss_D: 0.1872 Loss_G: 0.0611 Convergence: 0.2200 k= 0.007242\n",
      "[0/25][1760/12663] Loss_D: 0.1717 Loss_G: 0.0640 Convergence: 0.1921 k= 0.007439\n",
      "[0/25][1770/12663] Loss_D: 0.1504 Loss_G: 0.0662 Convergence: 0.1585 k= 0.007647\n",
      "[0/25][1780/12663] Loss_D: 0.1627 Loss_G: 0.0718 Convergence: 0.1727 k= 0.007796\n",
      "[0/25][1790/12663] Loss_D: 0.1848 Loss_G: 0.0601 Convergence: 0.2167 k= 0.008053\n",
      "[0/25][1800/12663] Loss_D: 0.1712 Loss_G: 0.0795 Convergence: 0.1793 k= 0.008230\n",
      "[0/25][1810/12663] Loss_D: 0.1570 Loss_G: 0.0792 Convergence: 0.1659 k= 0.008374\n",
      "[0/25][1820/12663] Loss_D: 0.1859 Loss_G: 0.0713 Convergence: 0.2022 k= 0.008549\n",
      "[0/25][1830/12663] Loss_D: 0.1867 Loss_G: 0.0702 Convergence: 0.2079 k= 0.008813\n",
      "[0/25][1840/12663] Loss_D: 0.1795 Loss_G: 0.0535 Convergence: 0.2154 k= 0.009051\n",
      "[0/25][1850/12663] Loss_D: 0.1613 Loss_G: 0.0608 Convergence: 0.1798 k= 0.009321\n",
      "[0/25][1860/12663] Loss_D: 0.1747 Loss_G: 0.0615 Convergence: 0.1981 k= 0.009571\n",
      "[0/25][1870/12663] Loss_D: 0.1855 Loss_G: 0.0621 Convergence: 0.2198 k= 0.009780\n",
      "[0/25][1880/12663] Loss_D: 0.1548 Loss_G: 0.0606 Convergence: 0.1737 k= 0.009984\n",
      "[0/25][1890/12663] Loss_D: 0.1670 Loss_G: 0.0695 Convergence: 0.1811 k= 0.010227\n",
      "[0/25][1900/12663] Loss_D: 0.1687 Loss_G: 0.0549 Convergence: 0.1911 k= 0.010412\n",
      "[0/25][1910/12663] Loss_D: 0.1866 Loss_G: 0.0559 Convergence: 0.2085 k= 0.010632\n",
      "[0/25][1920/12663] Loss_D: 0.1768 Loss_G: 0.0772 Convergence: 0.1910 k= 0.010881\n",
      "[0/25][1930/12663] Loss_D: 0.1466 Loss_G: 0.0934 Convergence: 0.1682 k= 0.011051\n",
      "[0/25][1940/12663] Loss_D: 0.1664 Loss_G: 0.0691 Convergence: 0.1807 k= 0.011169\n",
      "[0/25][1950/12663] Loss_D: 0.1587 Loss_G: 0.0581 Convergence: 0.1782 k= 0.011307\n",
      "[0/25][1960/12663] Loss_D: 0.1722 Loss_G: 0.0604 Convergence: 0.1951 k= 0.011532\n",
      "[0/25][1970/12663] Loss_D: 0.1680 Loss_G: 0.0733 Convergence: 0.1803 k= 0.011730\n",
      "[0/25][1980/12663] Loss_D: 0.1677 Loss_G: 0.0693 Convergence: 0.1815 k= 0.011934\n",
      "[0/25][1990/12663] Loss_D: 0.1589 Loss_G: 0.0586 Convergence: 0.1774 k= 0.012171\n",
      "[0/25][2000/12663] Loss_D: 0.1634 Loss_G: 0.0763 Convergence: 0.1694 k= 0.012316\n",
      "[0/25][2010/12663] Loss_D: 0.1696 Loss_G: 0.0680 Convergence: 0.1929 k= 0.012340\n",
      "[0/25][2020/12663] Loss_D: 0.1686 Loss_G: 0.1031 Convergence: 0.1867 k= 0.012358\n",
      "[0/25][2030/12663] Loss_D: 0.1648 Loss_G: 0.0633 Convergence: 0.1804 k= 0.012493\n",
      "[0/25][2040/12663] Loss_D: 0.1443 Loss_G: 0.0563 Convergence: 0.1562 k= 0.012702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][2050/12663] Loss_D: 0.1677 Loss_G: 0.0591 Convergence: 0.1924 k= 0.012937\n",
      "[0/25][2060/12663] Loss_D: 0.1816 Loss_G: 0.0537 Convergence: 0.2178 k= 0.013205\n",
      "[0/25][2070/12663] Loss_D: 0.1503 Loss_G: 0.0638 Convergence: 0.1573 k= 0.013454\n",
      "[0/25][2080/12663] Loss_D: 0.1672 Loss_G: 0.0550 Convergence: 0.1976 k= 0.013689\n",
      "[0/25][2090/12663] Loss_D: 0.1783 Loss_G: 0.0613 Convergence: 0.2062 k= 0.013905\n",
      "[0/25][2100/12663] Loss_D: 0.1681 Loss_G: 0.0600 Convergence: 0.1960 k= 0.014123\n",
      "[0/25][2110/12663] Loss_D: 0.1567 Loss_G: 0.0612 Convergence: 0.1733 k= 0.014311\n",
      "[0/25][2120/12663] Loss_D: 0.1694 Loss_G: 0.0634 Convergence: 0.1909 k= 0.014524\n",
      "[0/25][2130/12663] Loss_D: 0.1649 Loss_G: 0.0663 Convergence: 0.1815 k= 0.014708\n",
      "[0/25][2140/12663] Loss_D: 0.1861 Loss_G: 0.0620 Convergence: 0.2181 k= 0.014895\n",
      "[0/25][2150/12663] Loss_D: 0.1597 Loss_G: 0.0592 Convergence: 0.1832 k= 0.015073\n",
      "[0/25][2160/12663] Loss_D: 0.1680 Loss_G: 0.0791 Convergence: 0.1713 k= 0.015223\n",
      "[0/25][2170/12663] Loss_D: 0.1592 Loss_G: 0.0719 Convergence: 0.1724 k= 0.015332\n",
      "[0/25][2180/12663] Loss_D: 0.1819 Loss_G: 0.0666 Convergence: 0.1993 k= 0.015486\n",
      "[0/25][2190/12663] Loss_D: 0.1563 Loss_G: 0.0793 Convergence: 0.1563 k= 0.015630\n",
      "[0/25][2200/12663] Loss_D: 0.1666 Loss_G: 0.0634 Convergence: 0.1863 k= 0.015804\n",
      "[0/25][2210/12663] Loss_D: 0.1665 Loss_G: 0.0693 Convergence: 0.1793 k= 0.016030\n",
      "[0/25][2220/12663] Loss_D: 0.1716 Loss_G: 0.0633 Convergence: 0.1897 k= 0.016184\n",
      "[0/25][2230/12663] Loss_D: 0.1643 Loss_G: 0.0690 Convergence: 0.1792 k= 0.016380\n",
      "[0/25][2240/12663] Loss_D: 0.1663 Loss_G: 0.0619 Convergence: 0.1890 k= 0.016536\n",
      "[0/25][2250/12663] Loss_D: 0.1537 Loss_G: 0.0654 Convergence: 0.1633 k= 0.016711\n",
      "[0/25][2260/12663] Loss_D: 0.1559 Loss_G: 0.0658 Convergence: 0.1666 k= 0.016872\n",
      "[0/25][2270/12663] Loss_D: 0.1726 Loss_G: 0.0589 Convergence: 0.1996 k= 0.017056\n",
      "[0/25][2280/12663] Loss_D: 0.1898 Loss_G: 0.0718 Convergence: 0.2132 k= 0.017252\n",
      "[0/25][2290/12663] Loss_D: 0.1474 Loss_G: 0.0696 Convergence: 0.1518 k= 0.017399\n",
      "[0/25][2300/12663] Loss_D: 0.1647 Loss_G: 0.0801 Convergence: 0.1680 k= 0.017456\n",
      "[0/25][2310/12663] Loss_D: 0.1648 Loss_G: 0.0610 Convergence: 0.1825 k= 0.017603\n",
      "[0/25][2320/12663] Loss_D: 0.2025 Loss_G: 0.0649 Convergence: 0.2380 k= 0.017731\n",
      "[0/25][2330/12663] Loss_D: 0.1722 Loss_G: 0.0740 Convergence: 0.1845 k= 0.017881\n",
      "[0/25][2340/12663] Loss_D: 0.1578 Loss_G: 0.0841 Convergence: 0.1642 k= 0.017974\n",
      "[0/25][2350/12663] Loss_D: 0.1870 Loss_G: 0.0854 Convergence: 0.1949 k= 0.017898\n",
      "[0/25][2360/12663] Loss_D: 0.1593 Loss_G: 0.0643 Convergence: 0.1694 k= 0.017959\n",
      "[0/25][2370/12663] Loss_D: 0.1811 Loss_G: 0.0722 Convergence: 0.1981 k= 0.018057\n",
      "[0/25][2380/12663] Loss_D: 0.1624 Loss_G: 0.0630 Convergence: 0.1798 k= 0.018166\n",
      "[0/25][2390/12663] Loss_D: 0.1800 Loss_G: 0.0687 Convergence: 0.2060 k= 0.018290\n",
      "[0/25][2400/12663] Loss_D: 0.1678 Loss_G: 0.0619 Convergence: 0.1901 k= 0.018431\n",
      "[0/25][2410/12663] Loss_D: 0.1584 Loss_G: 0.0656 Convergence: 0.1700 k= 0.018565\n",
      "[0/25][2420/12663] Loss_D: 0.1570 Loss_G: 0.0584 Convergence: 0.1721 k= 0.018686\n",
      "[0/25][2430/12663] Loss_D: 0.1653 Loss_G: 0.0620 Convergence: 0.1811 k= 0.018777\n",
      "[0/25][2440/12663] Loss_D: 0.1662 Loss_G: 0.0701 Convergence: 0.1728 k= 0.018929\n",
      "[0/25][2450/12663] Loss_D: 0.1533 Loss_G: 0.0646 Convergence: 0.1657 k= 0.019072\n",
      "[0/25][2460/12663] Loss_D: 0.1483 Loss_G: 0.0746 Convergence: 0.1486 k= 0.019229\n",
      "[0/25][2470/12663] Loss_D: 0.1595 Loss_G: 0.0685 Convergence: 0.1727 k= 0.019396\n",
      "[0/25][2480/12663] Loss_D: 0.1653 Loss_G: 0.0693 Convergence: 0.1781 k= 0.019519\n",
      "[0/25][2490/12663] Loss_D: 0.1571 Loss_G: 0.0688 Convergence: 0.1676 k= 0.019654\n",
      "[0/25][2500/12663] Loss_D: 0.1652 Loss_G: 0.0693 Convergence: 0.1794 k= 0.019757\n",
      "[0/25][2510/12663] Loss_D: 0.1738 Loss_G: 0.0646 Convergence: 0.1955 k= 0.019910\n",
      "[0/25][2520/12663] Loss_D: 0.1556 Loss_G: 0.0653 Convergence: 0.1672 k= 0.020062\n",
      "[0/25][2530/12663] Loss_D: 0.1574 Loss_G: 0.0598 Convergence: 0.1771 k= 0.020200\n",
      "[0/25][2540/12663] Loss_D: 0.1589 Loss_G: 0.0602 Convergence: 0.1780 k= 0.020403\n",
      "[0/25][2550/12663] Loss_D: 0.1641 Loss_G: 0.0888 Convergence: 0.1714 k= 0.020547\n",
      "[0/25][2560/12663] Loss_D: 0.1359 Loss_G: 0.0604 Convergence: 0.1484 k= 0.020685\n",
      "[0/25][2570/12663] Loss_D: 0.1692 Loss_G: 0.0644 Convergence: 0.1880 k= 0.020875\n",
      "[0/25][2580/12663] Loss_D: 0.1455 Loss_G: 0.0669 Convergence: 0.1485 k= 0.021009\n",
      "[0/25][2590/12663] Loss_D: 0.1421 Loss_G: 0.0654 Convergence: 0.1442 k= 0.021145\n",
      "[0/25][2600/12663] Loss_D: 0.1767 Loss_G: 0.0739 Convergence: 0.1882 k= 0.021285\n",
      "[0/25][2610/12663] Loss_D: 0.1702 Loss_G: 0.0830 Convergence: 0.1721 k= 0.021383\n",
      "[0/25][2620/12663] Loss_D: 0.1559 Loss_G: 0.0667 Convergence: 0.1645 k= 0.021474\n",
      "[0/25][2630/12663] Loss_D: 0.1519 Loss_G: 0.0595 Convergence: 0.1690 k= 0.021627\n",
      "[0/25][2640/12663] Loss_D: 0.1585 Loss_G: 0.0686 Convergence: 0.1729 k= 0.021827\n",
      "[0/25][2650/12663] Loss_D: 0.1744 Loss_G: 0.0733 Convergence: 0.1815 k= 0.021980\n",
      "[0/25][2660/12663] Loss_D: 0.1687 Loss_G: 0.0738 Convergence: 0.1787 k= 0.022067\n",
      "[0/25][2670/12663] Loss_D: 0.1491 Loss_G: 0.0622 Convergence: 0.1631 k= 0.022231\n",
      "[0/25][2680/12663] Loss_D: 0.1570 Loss_G: 0.0849 Convergence: 0.1628 k= 0.022341\n",
      "[0/25][2690/12663] Loss_D: 0.1776 Loss_G: 0.0693 Convergence: 0.1979 k= 0.022437\n",
      "[0/25][2700/12663] Loss_D: 0.1637 Loss_G: 0.0630 Convergence: 0.1794 k= 0.022612\n",
      "[0/25][2710/12663] Loss_D: 0.1510 Loss_G: 0.0759 Convergence: 0.1514 k= 0.022752\n",
      "[0/25][2720/12663] Loss_D: 0.1648 Loss_G: 0.0752 Convergence: 0.1729 k= 0.022826\n",
      "[0/25][2730/12663] Loss_D: 0.1542 Loss_G: 0.0678 Convergence: 0.1640 k= 0.022967\n",
      "[0/25][2740/12663] Loss_D: 0.1543 Loss_G: 0.0617 Convergence: 0.1711 k= 0.023115\n",
      "[0/25][2750/12663] Loss_D: 0.1725 Loss_G: 0.0644 Convergence: 0.1945 k= 0.023281\n",
      "[0/25][2760/12663] Loss_D: 0.1574 Loss_G: 0.0581 Convergence: 0.1806 k= 0.023405\n",
      "[0/25][2770/12663] Loss_D: 0.1530 Loss_G: 0.0624 Convergence: 0.1662 k= 0.023551\n",
      "[0/25][2780/12663] Loss_D: 0.1623 Loss_G: 0.0618 Convergence: 0.1829 k= 0.023688\n",
      "[0/25][2790/12663] Loss_D: 0.1703 Loss_G: 0.0694 Convergence: 0.1878 k= 0.023877\n",
      "[0/25][2800/12663] Loss_D: 0.1579 Loss_G: 0.0605 Convergence: 0.1759 k= 0.024005\n",
      "[0/25][2810/12663] Loss_D: 0.1625 Loss_G: 0.0763 Convergence: 0.1797 k= 0.024135\n",
      "[0/25][2820/12663] Loss_D: 0.1671 Loss_G: 0.0980 Convergence: 0.1831 k= 0.024297\n",
      "[0/25][2830/12663] Loss_D: 0.1518 Loss_G: 0.0623 Convergence: 0.1615 k= 0.024450\n",
      "[0/25][2840/12663] Loss_D: 0.1490 Loss_G: 0.0806 Convergence: 0.1561 k= 0.024560\n",
      "[0/25][2850/12663] Loss_D: 0.1709 Loss_G: 0.0691 Convergence: 0.1902 k= 0.024699\n",
      "[0/25][2860/12663] Loss_D: 0.1742 Loss_G: 0.0685 Convergence: 0.1949 k= 0.024849\n",
      "[0/25][2870/12663] Loss_D: 0.1590 Loss_G: 0.0663 Convergence: 0.1742 k= 0.025026\n",
      "[0/25][2880/12663] Loss_D: 0.1574 Loss_G: 0.0721 Convergence: 0.1608 k= 0.025126\n",
      "[0/25][2890/12663] Loss_D: 0.1597 Loss_G: 0.0665 Convergence: 0.1738 k= 0.025192\n",
      "[0/25][2900/12663] Loss_D: 0.1784 Loss_G: 0.0542 Convergence: 0.2135 k= 0.025406\n",
      "[0/25][2910/12663] Loss_D: 0.1610 Loss_G: 0.0749 Convergence: 0.1682 k= 0.025483\n",
      "[0/25][2920/12663] Loss_D: 0.1546 Loss_G: 0.0842 Convergence: 0.1631 k= 0.025537\n",
      "[0/25][2930/12663] Loss_D: 0.1637 Loss_G: 0.0678 Convergence: 0.1814 k= 0.025628\n",
      "[0/25][2940/12663] Loss_D: 0.1577 Loss_G: 0.0703 Convergence: 0.1603 k= 0.025793\n",
      "[0/25][2950/12663] Loss_D: 0.1593 Loss_G: 0.0718 Convergence: 0.1676 k= 0.025924\n",
      "[0/25][2960/12663] Loss_D: 0.1596 Loss_G: 0.0730 Convergence: 0.1663 k= 0.026023\n",
      "[0/25][2970/12663] Loss_D: 0.1738 Loss_G: 0.0637 Convergence: 0.1977 k= 0.026141\n",
      "[0/25][2980/12663] Loss_D: 0.1602 Loss_G: 0.0596 Convergence: 0.1818 k= 0.026277\n",
      "[0/25][2990/12663] Loss_D: 0.1564 Loss_G: 0.0638 Convergence: 0.1672 k= 0.026405\n",
      "[0/25][3000/12663] Loss_D: 0.1641 Loss_G: 0.0675 Convergence: 0.1789 k= 0.026563\n",
      "[0/25][3010/12663] Loss_D: 0.1504 Loss_G: 0.0633 Convergence: 0.1602 k= 0.026724\n",
      "[0/25][3020/12663] Loss_D: 0.1634 Loss_G: 0.0645 Convergence: 0.1799 k= 0.026934\n",
      "[0/25][3030/12663] Loss_D: 0.1642 Loss_G: 0.0656 Convergence: 0.1830 k= 0.027077\n",
      "[0/25][3040/12663] Loss_D: 0.1638 Loss_G: 0.0598 Convergence: 0.1810 k= 0.027178\n",
      "[0/25][3050/12663] Loss_D: 0.1527 Loss_G: 0.0627 Convergence: 0.1659 k= 0.027352\n",
      "[0/25][3060/12663] Loss_D: 0.1681 Loss_G: 0.0628 Convergence: 0.1905 k= 0.027517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][3070/12663] Loss_D: 0.1478 Loss_G: 0.0686 Convergence: 0.1559 k= 0.027628\n",
      "[0/25][3080/12663] Loss_D: 0.1649 Loss_G: 0.0601 Convergence: 0.1861 k= 0.027762\n",
      "[0/25][3090/12663] Loss_D: 0.1611 Loss_G: 0.0774 Convergence: 0.1710 k= 0.027925\n",
      "[0/25][3100/12663] Loss_D: 0.1363 Loss_G: 0.1070 Convergence: 0.1783 k= 0.027857\n",
      "[0/25][3110/12663] Loss_D: 0.1469 Loss_G: 0.0833 Convergence: 0.1578 k= 0.027876\n",
      "[0/25][3120/12663] Loss_D: 0.1500 Loss_G: 0.0606 Convergence: 0.1633 k= 0.028056\n",
      "[0/25][3130/12663] Loss_D: 0.1603 Loss_G: 0.0722 Convergence: 0.1687 k= 0.028254\n",
      "[0/25][3140/12663] Loss_D: 0.1479 Loss_G: 0.0585 Convergence: 0.1669 k= 0.028405\n",
      "[0/25][3150/12663] Loss_D: 0.1560 Loss_G: 0.0581 Convergence: 0.1776 k= 0.028545\n",
      "[0/25][3160/12663] Loss_D: 0.1537 Loss_G: 0.0712 Convergence: 0.1610 k= 0.028660\n",
      "[0/25][3170/12663] Loss_D: 0.1674 Loss_G: 0.0519 Convergence: 0.2010 k= 0.028756\n",
      "[0/25][3180/12663] Loss_D: 0.1471 Loss_G: 0.0632 Convergence: 0.1576 k= 0.028850\n",
      "[0/25][3190/12663] Loss_D: 0.1633 Loss_G: 0.0640 Convergence: 0.1792 k= 0.029016\n",
      "[0/25][3200/12663] Loss_D: 0.1620 Loss_G: 0.0665 Convergence: 0.1792 k= 0.029168\n",
      "[0/25][3210/12663] Loss_D: 0.1574 Loss_G: 0.0630 Convergence: 0.1757 k= 0.029315\n",
      "[0/25][3220/12663] Loss_D: 0.1477 Loss_G: 0.0544 Convergence: 0.1698 k= 0.029459\n",
      "[0/25][3230/12663] Loss_D: 0.1533 Loss_G: 0.0547 Convergence: 0.1826 k= 0.029619\n",
      "[0/25][3240/12663] Loss_D: 0.1450 Loss_G: 0.0534 Convergence: 0.1611 k= 0.029773\n",
      "[0/25][3250/12663] Loss_D: 0.1675 Loss_G: 0.0574 Convergence: 0.1790 k= 0.029881\n",
      "[0/25][3260/12663] Loss_D: 0.1731 Loss_G: 0.0690 Convergence: 0.1928 k= 0.029985\n",
      "[0/25][3270/12663] Loss_D: 0.1563 Loss_G: 0.0591 Convergence: 0.1774 k= 0.030140\n",
      "[0/25][3280/12663] Loss_D: 0.1598 Loss_G: 0.0627 Convergence: 0.1788 k= 0.030375\n",
      "[0/25][3290/12663] Loss_D: 0.1494 Loss_G: 0.0647 Convergence: 0.1618 k= 0.030586\n",
      "[0/25][3300/12663] Loss_D: 0.1709 Loss_G: 0.1002 Convergence: 0.1895 k= 0.030725\n",
      "[0/25][3310/12663] Loss_D: 0.1442 Loss_G: 0.0784 Convergence: 0.1528 k= 0.030862\n",
      "[0/25][3320/12663] Loss_D: 0.1495 Loss_G: 0.0554 Convergence: 0.1694 k= 0.031128\n",
      "[0/25][3330/12663] Loss_D: 0.1596 Loss_G: 0.0556 Convergence: 0.1845 k= 0.031374\n",
      "[0/25][3340/12663] Loss_D: 0.1700 Loss_G: 0.0627 Convergence: 0.1951 k= 0.031597\n",
      "[0/25][3350/12663] Loss_D: 0.1402 Loss_G: 0.0535 Convergence: 0.1558 k= 0.031838\n",
      "[0/25][3360/12663] Loss_D: 0.1648 Loss_G: 0.0610 Convergence: 0.1891 k= 0.032038\n",
      "[0/25][3370/12663] Loss_D: 0.1542 Loss_G: 0.0555 Convergence: 0.1758 k= 0.032263\n",
      "[0/25][3380/12663] Loss_D: 0.1479 Loss_G: 0.0582 Convergence: 0.1643 k= 0.032479\n",
      "[0/25][3390/12663] Loss_D: 0.1607 Loss_G: 0.0600 Convergence: 0.1835 k= 0.032693\n",
      "[0/25][3400/12663] Loss_D: 0.1629 Loss_G: 0.0736 Convergence: 0.1741 k= 0.032866\n",
      "[0/25][3410/12663] Loss_D: 0.1623 Loss_G: 0.0630 Convergence: 0.1803 k= 0.033057\n",
      "[0/25][3420/12663] Loss_D: 0.1474 Loss_G: 0.0634 Convergence: 0.1466 k= 0.033187\n",
      "[0/25][3430/12663] Loss_D: 0.1689 Loss_G: 0.0625 Convergence: 0.1899 k= 0.033350\n",
      "[0/25][3440/12663] Loss_D: 0.1558 Loss_G: 0.0667 Convergence: 0.1703 k= 0.033505\n",
      "[0/25][3450/12663] Loss_D: 0.1613 Loss_G: 0.0584 Convergence: 0.1826 k= 0.033702\n",
      "[0/25][3460/12663] Loss_D: 0.1759 Loss_G: 0.0529 Convergence: 0.2161 k= 0.033968\n",
      "[0/25][3470/12663] Loss_D: 0.1441 Loss_G: 0.0508 Convergence: 0.1618 k= 0.034155\n",
      "[0/25][3480/12663] Loss_D: 0.1632 Loss_G: 0.0620 Convergence: 0.1819 k= 0.034287\n",
      "[0/25][3490/12663] Loss_D: 0.1641 Loss_G: 0.0462 Convergence: 0.2017 k= 0.034498\n",
      "[0/25][3500/12663] Loss_D: 0.1507 Loss_G: 0.0551 Convergence: 0.1695 k= 0.034685\n",
      "[0/25][3510/12663] Loss_D: 0.1691 Loss_G: 0.0553 Convergence: 0.2009 k= 0.034871\n",
      "[0/25][3520/12663] Loss_D: 0.1288 Loss_G: 0.0524 Convergence: 0.1380 k= 0.035085\n",
      "[0/25][3530/12663] Loss_D: 0.1735 Loss_G: 0.0527 Convergence: 0.2105 k= 0.035317\n",
      "[0/25][3540/12663] Loss_D: 0.1450 Loss_G: 0.0624 Convergence: 0.1585 k= 0.035381\n",
      "[0/25][3550/12663] Loss_D: 0.1598 Loss_G: 0.0609 Convergence: 0.1843 k= 0.035505\n",
      "[0/25][3560/12663] Loss_D: 0.1602 Loss_G: 0.0553 Convergence: 0.1861 k= 0.035715\n",
      "[0/25][3570/12663] Loss_D: 0.1634 Loss_G: 0.0870 Convergence: 0.1694 k= 0.035887\n",
      "[0/25][3580/12663] Loss_D: 0.1653 Loss_G: 0.0692 Convergence: 0.1800 k= 0.035987\n",
      "[0/25][3590/12663] Loss_D: 0.1487 Loss_G: 0.0864 Convergence: 0.1628 k= 0.036127\n",
      "[0/25][3600/12663] Loss_D: 0.1432 Loss_G: 0.0535 Convergence: 0.1601 k= 0.036326\n",
      "[0/25][3610/12663] Loss_D: 0.1689 Loss_G: 0.0545 Convergence: 0.1984 k= 0.036569\n",
      "[0/25][3620/12663] Loss_D: 0.1648 Loss_G: 0.0600 Convergence: 0.1883 k= 0.036797\n",
      "[0/25][3630/12663] Loss_D: 0.1582 Loss_G: 0.0653 Convergence: 0.1723 k= 0.037045\n"
     ]
    }
   ],
   "source": [
    "data_loss_D = []\n",
    "data_loss_G = []\n",
    "data_loss_k = []\n",
    "for epoch in range(opt.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # prepare real\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "\n",
    "        # train with real\n",
    "        output = netD(input)\n",
    "        errD_real = criterion_L1(output, input)  # score on real\n",
    "        errD_real.backward()  # backward on score on real\n",
    "        L_x = errD_real.data[0]  # score fore supervision\n",
    "        \n",
    "        # generate fake\n",
    "        #noise.data.resize_(batch_size, nz, 1, 1)\n",
    "        noise.data.normal_(0, 1)\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        # train with fake\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion_L1(output, fake.detach())  # score on fake\n",
    "        errD_fake_use = - k * errD_fake\n",
    "        errD_fake_use.backward()  # backward on score on fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #D_G_z1 = errD_fake.data.mean()  # score fore supervision <- generated when calc D loss\n",
    "        errD = errD_real + errD_fake_use  # score fore supervision\n",
    "\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # generate fake\n",
    "        noise.data.resize_(batch_size, nz, 1, 1)\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        # NOT reuse generated fake samples\n",
    "        output = netD(fake)\n",
    "        errG = (output - fake).abs().mean()  # L1\n",
    "        errG.backward()\n",
    "        \n",
    "        L_G = errG.data[0] # score fore supervision <- generated when calc G loss\n",
    "\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # K STEP\n",
    "        #left -> real\n",
    "        output = netD(input)\n",
    "        errD_real_k = criterion_L1(output, input)  # score on real\n",
    "        #right -> fake\n",
    "        fake = netG(noise)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake_k = criterion_L1(output, fake.detach())  # score on fake\n",
    "        \n",
    "        #Convergence Measure\n",
    "        cm = errD_real_k.data[0] + abs(gamma * errD_real_k.data[0] - L_G)\n",
    "        \n",
    "        k += 0.001 * (gamma * errD_real_k.data[0] - errD_fake_k.data[0])\n",
    "        k = max(min(k, 1), 0)\n",
    "        ############################\n",
    "        # (3) Report & 100 Batch checkpoint\n",
    "        ############################\n",
    "        data_loss_G.append(errG.data[0])\n",
    "        data_loss_D.append(errD.data[0])\n",
    "        data_loss_k.append(k)\n",
    "        if i % 2000 == 0:\n",
    "            optimizerD = optim.Adam(netD.parameters(), lr=opt.lr*lr_decay)\n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=opt.lr*lr_decay)\n",
    "            opt.lr = opt.lr * lr_decay\n",
    "        if i % 10 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Convergence: %.4f k= %.6f'\n",
    "                    % (epoch, opt.niter, i, len(dataloader),\n",
    "                     errD.data[0], errG.data[0], cm, k))\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                              '%s/%d_fake_samples_epoch_%03d.png' % (opt.outf, i, epoch))\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
