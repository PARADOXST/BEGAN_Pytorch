{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from glob import glob\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#params\n",
    "noise_dim = 64\n",
    "n_epochs = 5\n",
    "batch_size = 16\n",
    "imageH = 64\n",
    "imageW = 64\n",
    "global_kernel_num = 128\n",
    "learning_rate = 0.00008\n",
    "k_init = 0\n",
    "lam = 0.001\n",
    "gamma = 0.5\n",
    "if_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "111111111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/CelebA/64_64'):\n",
    "    os.mkdir('./data/CelebA/64_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list_ori = glob('./data/CelebA/splits/train/*')\n",
    "for i in file_list_ori:\n",
    "    img = Image.open(i)\n",
    "    img = img.crop((25, 50, 25+128, 50+128))\n",
    "    img = img.resize((64, 64))\n",
    "    img.save('./data/CelebA/64_64/' + i.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = glob('./data/CelebA/64_64/*')\n",
    "mini_batch = len(file_list) / batch_size\n",
    "def get_permutation():\n",
    "    return np.random.permutation(len(file_list))\n",
    "def get_samples(index, i):\n",
    "    data = torch.FloatTensor(batch_size, 3, imageH, imageW)\n",
    "    j = 0\n",
    "    for i in range(batch_size*i, batch_size*(i+1)):\n",
    "        data[j] = transforms.ToTensor()(Image.open(file_list[index[i]]))\n",
    "        j += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator\n",
    "class G_CNN(nn.Module):\n",
    "    def __init__(self, embedding_size, kernel_num):\n",
    "        super(G_CNN, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        FC_start_size = 8 * 8 * self.kernel_num\n",
    "        self.start = nn.Linear(embedding_size, FC_start_size)\n",
    "        #all layers\n",
    "        self.main_arch = nn.Sequential(\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #last layer\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        in_data = self.start(in_data)\n",
    "        in_data = in_data.view(-1, self.kernel_num, 8, 8)\n",
    "        output = self.main_arch(in_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class D_CNN(nn.Module):\n",
    "    def __init__(self, embedding_size, base_kernel_num):\n",
    "        super(D_CNN, self).__init__()\n",
    "        self.base_kernel_num = base_kernel_num\n",
    "        \n",
    "        #encoder w/o flatten and FC\n",
    "        self.encoder_wo_FC = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=4*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        FC_mid1_size = 8 * 8 * 4 * self.base_kernel_num\n",
    "        FC_mid2_size = 8 * 8 * self.base_kernel_num\n",
    "        self.FC1 = nn.Linear(FC_mid1_size, embedding_size)\n",
    "        self.FC2 = nn.Linear(embedding_size, FC_mid2_size)\n",
    "        self.decoder_wo_FC = nn.Sequential(\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #last layer\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        in_data = self.encoder_wo_FC(in_data)\n",
    "        \n",
    "        in_data = in_data.view(-1, 4*self.base_kernel_num * 8 * 8);\n",
    "        in_data = self.FC1(in_data)\n",
    "        in_data = self.FC2(in_data)\n",
    "        in_data = in_data.view(-1, self.base_kernel_num, 8, 8)\n",
    "        \n",
    "        output = self.decoder_wo_FC(in_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = (torch.FloatTensor(batch_size, noise_dim))\n",
    "input = (torch.FloatTensor(batch_size, 3, imageH, imageW))\n",
    "GNet = G_CNN(embedding_size=noise_dim, kernel_num=global_kernel_num)\n",
    "DNet = D_CNN(embedding_size=noise_dim, base_kernel_num=global_kernel_num)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#move to gpu\n",
    "if if_cuda:\n",
    "    GNet.cuda()\n",
    "    DNet.cuda()\n",
    "    noise = noise.cuda()\n",
    "    input = input.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimG = optim.Adam(GNet.parameters(), lr=learning_rate)\n",
    "optimD = optim.Adam(DNet.parameters(), lr=learning_rate)\n",
    "noise = Variable(noise)\n",
    "input = Variable(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lossG_rec = []\n",
    "lossD_rec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(n_epochs):\n",
    "    k = k_init\n",
    "    for i in range(n_epochs):\n",
    "        index = get_permutation()\n",
    "        for j in range(int(mini_batch)):\n",
    "            #Discriminator\n",
    "            #real\n",
    "            DNet.zero_grad()\n",
    "            #input.data.copy_(data)\n",
    "            input.data.copy_(get_samples(index, j))\n",
    "            output_real = DNet(input)\n",
    "            D_real_loss = criterion(output_real, input)\n",
    "            \n",
    "            \n",
    "            #fake\n",
    "            noise.data.normal_(0, 1)\n",
    "            fake = GNet(noise)\n",
    "            #detach from Generator maybe save computing grad from G\n",
    "            output_fake = DNet(fake)\n",
    "            D_fake_loss = criterion(output_fake, fake.detach())\n",
    "            \n",
    "            #update\n",
    "            D_loss = D_real_loss - k * D_fake_loss\n",
    "            D_loss.backward(retain_variables=True)\n",
    "            optimD.step()\n",
    "            \n",
    "            #generator\n",
    "            DNet.zero_grad()\n",
    "            GNet.zero_grad()\n",
    "#             fake = GNet(noise)\n",
    "#             G_out = DNet(fake)\n",
    "#             G_loss = criterion(G_out, fake.detach()) #target can should make require_grad false\n",
    "            G_loss = D_fake_loss\n",
    "            G_loss.backward(retain_variables=True)\n",
    "            optimG.step()\n",
    "            if j == 0:\n",
    "                for s in range(batch_size):\n",
    "                    img_sample = transforms.ToPILImage()(fake[s].data.cpu())\n",
    "                    img_sample.save('./model/'+str(i) + '_' + str(j) + '_' + str(s) + '.jpg')\n",
    "            #update k\n",
    "            k += lam * (gamma * D_real_loss.data.cpu()[0] - D_fake_loss.data.cpu()[0])\n",
    "            k = max(min(1, k), 0)\n",
    "            if j % 20 == 0:\n",
    "                print 'Epoch %d' %i, 'Iter %d'%j, '/', int(mini_batch)\n",
    "                print 'Epoch [%d/%d] Loss of G: %.4f \\tLoss of D: %.4f \\tk: %.4f' % (i, n_epochs, G_loss.data[0], D_loss.data[0], k)\n",
    "            lossD_rec.append(D_loss.data[0])\n",
    "            lossG_rec.append(G_loss.data[0])\n",
    "        # do checkpointing\n",
    "        torch.save(GNet.state_dict(), './model/netG_epoch_%d.pth' % (i))\n",
    "        torch.save(DNet.state_dict(), './model/netD_epoch_%d.pth' % (i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iter 0 / 10173\n",
      "Epoch [0/5] Loss of G: 0.0210 \tLoss of D: 0.4257 \tk: 0.0002\n",
      "Epoch 0 Iter 20 / 10173\n",
      "Epoch [0/5] Loss of G: 0.6551 \tLoss of D: 0.2053 \tk: 0.0000\n",
      "Epoch 0 Iter 40 / 10173\n",
      "Epoch [0/5] Loss of G: 9.6285 \tLoss of D: 0.2069 \tk: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-048d3988ea7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DLoss.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossD_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GLoss.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossG_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9d076022a2ae>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#             G_loss = criterion(G_out, fake.detach()) #target can should make require_grad false\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_fake_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(n_epochs)\n",
    "np.save(np.array('DLoss.npy', lossD_rec))\n",
    "np.save(np.array('GLoss.npy', lossG_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNet(Variable(get_samples(get_permutation(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
