{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--lr_decay'], dest='lr_decay', nargs=None, const=None, default=0.95, type=<type 'float'>, choices=None, help='learning rate decay rate', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=True, help='cifar10 | lsun | imagenet | folder | lfw ')\n",
    "parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='input batch size')\n",
    "parser.add_argument('--image_size', type=int, default=64, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=64, help='size of the latent z vector')\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--base_dir', default='./train4/', help='directory to save image/model/data')\n",
    "parser.add_argument('--k_init', type=float, default=0.0, help='initial value of k')\n",
    "parser.add_argument('--gamma', type=float, default=0.4, help='diversity ratio')\n",
    "parser.add_argument('--numOfKernel', type=int, default=64, help='Number of Kernels in G\\'s Conv')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95, help='learning rate decay rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_dir='./train4/', batch_size=16, beta1=0.5, cuda=True, dataroot='/home/ubuntu/data/work/', dataset='folder', gamma=0.4, image_size=64, k_init=0.0, lr=0.0001, lr_decay=0.95, netD='', netG='', niter=25, numOfKernel=64, nz=64, workers=2)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args(['--dataset', 'folder', '--dataroot', '/home/ubuntu/data/work/', '--cuda', '--niter', '25'])\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(opt.base_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if opt.manualSeed is None:\n",
    "#     opt.manualSeed = random.randint(1, 10000)\n",
    "# print(\"Random Seed: \", opt.manualSeed)\n",
    "# random.seed(opt.manualSeed)\n",
    "# torch.manual_seed(opt.manualSeed)\n",
    "# if opt.cuda:\n",
    "#     torch.cuda.manual_seed_all(opt.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = glob('/home/ubuntu/data/work/img_align_celeba/*')\n",
    "mini_batch = len(file_list) / opt.batch_size\n",
    "def get_permutation():\n",
    "    return np.random.permutation(len(file_list))\n",
    "def get_samples(index, i):\n",
    "    data = torch.FloatTensor(opt.batch_size, 3, opt.image_size, opt.image_size)\n",
    "    j = 0\n",
    "    for i in range(opt.batch_size*i, opt.batch_size*(i+1)):\n",
    "        data[j] = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(transforms.ToTensor()(Image.open(file_list[index[i]])))\n",
    "        j += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "nz = int(opt.nz)\n",
    "k = opt.k_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def G_weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.002)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def D_weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class G_CNN(nn.Module):\n",
    "    def __init__(self, embedding_size, kernel_num):\n",
    "        super(G_CNN, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        FC_start_size = 8 * 8 * self.kernel_num\n",
    "        self.start = nn.Linear(embedding_size, FC_start_size)\n",
    "        #all layers\n",
    "        self.main_arch = nn.Sequential(\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #last layer\n",
    "            #nn.Conv2d(in_channels=kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        in_data = self.start(in_data.view(-1, 64))\n",
    "        in_data = in_data.view(-1, self.kernel_num, 8, 8)\n",
    "        output = self.main_arch(in_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netG = G_CNN(embedding_size=nz, kernel_num=opt.numOfKernel)\n",
    "netG.apply(G_weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class D_CNN(nn.Module):\n",
    "    def __init__(self, embedding_size, base_kernel_num):\n",
    "        super(D_CNN, self).__init__()\n",
    "        self.base_kernel_num = base_kernel_num\n",
    "        \n",
    "        #encoder w/o flatten and FC\n",
    "        self.encoder_wo_FC = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=2*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=2*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=3*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=3*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            #downsampling\n",
    "            nn.Conv2d(in_channels=4*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=4*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=4*base_kernel_num, out_channels=4*base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "        FC_mid1_size = 8 * 8 * 4 * self.base_kernel_num\n",
    "        FC_mid2_size = 8 * 8 * self.base_kernel_num\n",
    "        self.FC1 = nn.Linear(FC_mid1_size, embedding_size)\n",
    "        self.FC2 = nn.Linear(embedding_size, FC_mid2_size)\n",
    "        self.decoder_wo_FC = nn.Sequential(\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #repeated units\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            #last layer\n",
    "            #nn.Conv2d(in_channels=base_kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=base_kernel_num, kernel_size=3, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(in_channels=base_kernel_num, out_channels=3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        in_data = self.encoder_wo_FC(in_data)\n",
    "        \n",
    "        in_data = in_data.view(-1, 4*self.base_kernel_num * 8 * 8);\n",
    "        in_data = self.FC1(in_data)\n",
    "        in_data = self.FC2(in_data)\n",
    "        in_data = in_data.view(-1, self.base_kernel_num, 8, 8)\n",
    "        \n",
    "        output = self.decoder_wo_FC(in_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD = D_CNN(embedding_size=nz, base_kernel_num=opt.numOfKernel)\n",
    "netD.apply(D_weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "input = torch.FloatTensor(opt.batch_size, 3, opt.image_size, opt.image_size)\n",
    "noise = torch.FloatTensor(opt.batch_size, nz)\n",
    "fixed_noise = torch.FloatTensor(opt.batch_size, nz, 1, 1).uniform_(-1, 1)\n",
    "\n",
    "if opt.cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion_L1.cuda()\n",
    "    input = input.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "input = Variable(input)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "np.save(opt.base_dir+'fixed_noise.npy', fixed_noise.data.cpu().numpy())\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/9765] Loss_D: 0.4689 Loss_G: 0.0114 Convergence: 0.6427 k= 0.000176 lr = 0.0001000\n",
      "[0/25][10/9765] Loss_D: 0.4100 Loss_G: 0.1382 Convergence: 0.4176 k= 0.001508 lr = 0.0000950\n",
      "[0/25][20/9765] Loss_D: 0.3795 Loss_G: 0.2455 Convergence: 0.4699 k= 0.002028 lr = 0.0000950\n",
      "[0/25][30/9765] Loss_D: 0.3667 Loss_G: 0.1746 Convergence: 0.3875 k= 0.001169 lr = 0.0000950\n",
      "[0/25][40/9765] Loss_D: 0.3381 Loss_G: 0.2744 Convergence: 0.4683 k= 0.000000 lr = 0.0000950\n",
      "[0/25][50/9765] Loss_D: 0.4019 Loss_G: 0.1036 Convergence: 0.4464 k= 0.000446 lr = 0.0000950\n",
      "[0/25][60/9765] Loss_D: 0.2950 Loss_G: 0.0845 Convergence: 0.3132 k= 0.000592 lr = 0.0000950\n",
      "[0/25][70/9765] Loss_D: 0.2792 Loss_G: 0.0988 Convergence: 0.2864 k= 0.000784 lr = 0.0000950\n",
      "[0/25][80/9765] Loss_D: 0.2839 Loss_G: 0.0796 Convergence: 0.3078 k= 0.001002 lr = 0.0000950\n",
      "[0/25][90/9765] Loss_D: 0.2670 Loss_G: 0.0970 Convergence: 0.2593 k= 0.001218 lr = 0.0000950\n",
      "[0/25][100/9765] Loss_D: 0.2889 Loss_G: 0.0928 Convergence: 0.2929 k= 0.001354 lr = 0.0000950\n",
      "[0/25][110/9765] Loss_D: 0.2554 Loss_G: 0.0721 Convergence: 0.2794 k= 0.001577 lr = 0.0000950\n",
      "[0/25][120/9765] Loss_D: 0.2378 Loss_G: 0.0745 Convergence: 0.2510 k= 0.001801 lr = 0.0000950\n",
      "[0/25][130/9765] Loss_D: 0.2465 Loss_G: 0.0795 Convergence: 0.2625 k= 0.001905 lr = 0.0000950\n",
      "[0/25][140/9765] Loss_D: 0.2363 Loss_G: 0.1013 Convergence: 0.2412 k= 0.002015 lr = 0.0000950\n",
      "[0/25][150/9765] Loss_D: 0.2362 Loss_G: 0.1514 Convergence: 0.2915 k= 0.002024 lr = 0.0000950\n",
      "[0/25][160/9765] Loss_D: 0.2933 Loss_G: 0.0897 Convergence: 0.3028 k= 0.002160 lr = 0.0000950\n",
      "[0/25][170/9765] Loss_D: 0.2687 Loss_G: 0.1715 Convergence: 0.3302 k= 0.002231 lr = 0.0000950\n",
      "[0/25][180/9765] Loss_D: 0.2589 Loss_G: 0.0965 Convergence: 0.2411 k= 0.002301 lr = 0.0000950\n",
      "[0/25][190/9765] Loss_D: 0.2316 Loss_G: 0.1012 Convergence: 0.2417 k= 0.002301 lr = 0.0000950\n",
      "[0/25][200/9765] Loss_D: 0.2444 Loss_G: 0.1202 Convergence: 0.2613 k= 0.002378 lr = 0.0000950\n",
      "[0/25][210/9765] Loss_D: 0.2195 Loss_G: 0.1356 Convergence: 0.2667 k= 0.002421 lr = 0.0000950\n",
      "[0/25][220/9765] Loss_D: 0.2680 Loss_G: 0.2247 Convergence: 0.3830 k= 0.002466 lr = 0.0000950\n",
      "[0/25][230/9765] Loss_D: 0.2187 Loss_G: 0.0731 Convergence: 0.2289 k= 0.002501 lr = 0.0000950\n",
      "[0/25][240/9765] Loss_D: 0.2250 Loss_G: 0.0832 Convergence: 0.2130 k= 0.002517 lr = 0.0000950\n",
      "[0/25][250/9765] Loss_D: 0.2258 Loss_G: 0.1439 Convergence: 0.2784 k= 0.002589 lr = 0.0000950\n",
      "[0/25][260/9765] Loss_D: 0.2249 Loss_G: 0.0767 Convergence: 0.2310 k= 0.002632 lr = 0.0000950\n",
      "[0/25][270/9765] Loss_D: 0.2294 Loss_G: 0.0924 Convergence: 0.2295 k= 0.002669 lr = 0.0000950\n",
      "[0/25][280/9765] Loss_D: 0.2249 Loss_G: 0.1012 Convergence: 0.2340 k= 0.002680 lr = 0.0000950\n",
      "[0/25][290/9765] Loss_D: 0.2228 Loss_G: 0.0720 Convergence: 0.2159 k= 0.002652 lr = 0.0000950\n",
      "[0/25][300/9765] Loss_D: 0.2126 Loss_G: 0.0900 Convergence: 0.2161 k= 0.002680 lr = 0.0000950\n",
      "[0/25][310/9765] Loss_D: 0.2164 Loss_G: 0.1068 Convergence: 0.2311 k= 0.002598 lr = 0.0000950\n",
      "[0/25][320/9765] Loss_D: 0.2240 Loss_G: 0.1409 Convergence: 0.2740 k= 0.002420 lr = 0.0000950\n",
      "[0/25][330/9765] Loss_D: 0.1929 Loss_G: 0.0881 Convergence: 0.2031 k= 0.002387 lr = 0.0000950\n",
      "[0/25][340/9765] Loss_D: 0.2009 Loss_G: 0.0855 Convergence: 0.2051 k= 0.002416 lr = 0.0000950\n",
      "[0/25][350/9765] Loss_D: 0.1976 Loss_G: 0.0808 Convergence: 0.1966 k= 0.002297 lr = 0.0000950\n",
      "[0/25][360/9765] Loss_D: 0.2083 Loss_G: 0.0877 Convergence: 0.2121 k= 0.002260 lr = 0.0000950\n",
      "[0/25][370/9765] Loss_D: 0.1935 Loss_G: 0.1272 Convergence: 0.2447 k= 0.002291 lr = 0.0000950\n",
      "[0/25][380/9765] Loss_D: 0.1942 Loss_G: 0.0894 Convergence: 0.2038 k= 0.002238 lr = 0.0000950\n",
      "[0/25][390/9765] Loss_D: 0.2021 Loss_G: 0.1688 Convergence: 0.2957 k= 0.002180 lr = 0.0000950\n",
      "[0/25][400/9765] Loss_D: 0.2067 Loss_G: 0.0705 Convergence: 0.2176 k= 0.002145 lr = 0.0000950\n",
      "[0/25][410/9765] Loss_D: 0.2074 Loss_G: 0.0755 Convergence: 0.2051 k= 0.002164 lr = 0.0000950\n",
      "[0/25][420/9765] Loss_D: 0.2223 Loss_G: 0.0884 Convergence: 0.2131 k= 0.001944 lr = 0.0000950\n",
      "[0/25][430/9765] Loss_D: 0.1936 Loss_G: 0.0804 Convergence: 0.1934 k= 0.001963 lr = 0.0000950\n",
      "[0/25][440/9765] Loss_D: 0.2070 Loss_G: 0.1152 Convergence: 0.2399 k= 0.001892 lr = 0.0000950\n",
      "[0/25][450/9765] Loss_D: 0.1839 Loss_G: 0.0722 Convergence: 0.1797 k= 0.001928 lr = 0.0000950\n",
      "[0/25][460/9765] Loss_D: 0.1953 Loss_G: 0.1091 Convergence: 0.2231 k= 0.001869 lr = 0.0000950\n",
      "[0/25][470/9765] Loss_D: 0.2075 Loss_G: 0.0936 Convergence: 0.2067 k= 0.001600 lr = 0.0000950\n",
      "[0/25][480/9765] Loss_D: 0.1836 Loss_G: 0.0794 Convergence: 0.1889 k= 0.001545 lr = 0.0000950\n",
      "[0/25][490/9765] Loss_D: 0.1947 Loss_G: 0.0859 Convergence: 0.2022 k= 0.001430 lr = 0.0000950\n",
      "[0/25][500/9765] Loss_D: 0.1916 Loss_G: 0.0980 Convergence: 0.2094 k= 0.001379 lr = 0.0000950\n",
      "[0/25][510/9765] Loss_D: 0.1930 Loss_G: 0.0871 Convergence: 0.1999 k= 0.001232 lr = 0.0000950\n",
      "[0/25][520/9765] Loss_D: 0.1967 Loss_G: 0.1088 Convergence: 0.2248 k= 0.001109 lr = 0.0000950\n",
      "[0/25][530/9765] Loss_D: 0.2125 Loss_G: 0.2282 Convergence: 0.3638 k= 0.000877 lr = 0.0000950\n",
      "[0/25][540/9765] Loss_D: 0.1933 Loss_G: 0.1235 Convergence: 0.2428 k= 0.000692 lr = 0.0000950\n",
      "[0/25][550/9765] Loss_D: 0.1970 Loss_G: 0.0791 Convergence: 0.1949 k= 0.000600 lr = 0.0000950\n",
      "[0/25][560/9765] Loss_D: 0.1936 Loss_G: 0.0822 Convergence: 0.1954 k= 0.000494 lr = 0.0000950\n",
      "[0/25][570/9765] Loss_D: 0.1872 Loss_G: 0.0767 Convergence: 0.1837 k= 0.000367 lr = 0.0000950\n",
      "[0/25][580/9765] Loss_D: 0.1734 Loss_G: 0.0883 Convergence: 0.1942 k= 0.000286 lr = 0.0000950\n",
      "[0/25][590/9765] Loss_D: 0.1965 Loss_G: 0.1417 Convergence: 0.2632 k= 0.000112 lr = 0.0000950\n",
      "[0/25][600/9765] Loss_D: 0.1920 Loss_G: 0.1403 Convergence: 0.2511 k= 0.000010 lr = 0.0000950\n",
      "[0/25][610/9765] Loss_D: 0.1892 Loss_G: 0.0638 Convergence: 0.1907 k= 0.000029 lr = 0.0000950\n",
      "[0/25][620/9765] Loss_D: 0.2041 Loss_G: 0.0632 Convergence: 0.2107 k= 0.000023 lr = 0.0000950\n",
      "[0/25][630/9765] Loss_D: 0.1854 Loss_G: 0.0856 Convergence: 0.2043 k= 0.000000 lr = 0.0000950\n",
      "[0/25][640/9765] Loss_D: 0.1905 Loss_G: 0.0761 Convergence: 0.1881 k= 0.000000 lr = 0.0000950\n",
      "[0/25][650/9765] Loss_D: 0.1690 Loss_G: 0.0892 Convergence: 0.1920 k= 0.000000 lr = 0.0000950\n",
      "[0/25][660/9765] Loss_D: 0.1946 Loss_G: 0.0786 Convergence: 0.1899 k= 0.000000 lr = 0.0000950\n",
      "[0/25][670/9765] Loss_D: 0.1774 Loss_G: 0.1265 Convergence: 0.2362 k= 0.000000 lr = 0.0000950\n",
      "[0/25][680/9765] Loss_D: 0.2084 Loss_G: 0.0803 Convergence: 0.2058 k= 0.000006 lr = 0.0000950\n",
      "[0/25][690/9765] Loss_D: 0.1728 Loss_G: 0.0849 Convergence: 0.1867 k= 0.000000 lr = 0.0000950\n",
      "[0/25][700/9765] Loss_D: 0.1899 Loss_G: 0.0701 Convergence: 0.1890 k= 0.000006 lr = 0.0000950\n",
      "[0/25][710/9765] Loss_D: 0.1677 Loss_G: 0.1781 Convergence: 0.2904 k= 0.000000 lr = 0.0000950\n",
      "[0/25][720/9765] Loss_D: 0.1791 Loss_G: 0.0636 Convergence: 0.1848 k= 0.000012 lr = 0.0000950\n",
      "[0/25][730/9765] Loss_D: 0.1893 Loss_G: 0.0506 Convergence: 0.2123 k= 0.000063 lr = 0.0000950\n",
      "[0/25][740/9765] Loss_D: 0.1985 Loss_G: 0.1062 Convergence: 0.2207 k= 0.000000 lr = 0.0000950\n",
      "[0/25][750/9765] Loss_D: 0.1794 Loss_G: 0.0849 Convergence: 0.1897 k= 0.000000 lr = 0.0000950\n",
      "[0/25][760/9765] Loss_D: 0.1846 Loss_G: 0.0676 Convergence: 0.1844 k= 0.000009 lr = 0.0000950\n",
      "[0/25][770/9765] Loss_D: 0.1919 Loss_G: 0.1829 Convergence: 0.2993 k= 0.000000 lr = 0.0000950\n",
      "[0/25][780/9765] Loss_D: 0.1791 Loss_G: 0.0776 Convergence: 0.1823 k= 0.000000 lr = 0.0000950\n",
      "[0/25][790/9765] Loss_D: 0.2063 Loss_G: 0.0775 Convergence: 0.1857 k= 0.000000 lr = 0.0000950\n",
      "[0/25][800/9765] Loss_D: 0.1783 Loss_G: 0.0602 Convergence: 0.1803 k= 0.000013 lr = 0.0000950\n",
      "[0/25][810/9765] Loss_D: 0.1869 Loss_G: 0.0644 Convergence: 0.1877 k= 0.000010 lr = 0.0000950\n",
      "[0/25][820/9765] Loss_D: 0.1690 Loss_G: 0.0637 Convergence: 0.1665 k= 0.000035 lr = 0.0000950\n",
      "[0/25][830/9765] Loss_D: 0.1792 Loss_G: 0.0698 Convergence: 0.1733 k= 0.000008 lr = 0.0000950\n",
      "[0/25][840/9765] Loss_D: 0.1719 Loss_G: 0.0859 Convergence: 0.1892 k= 0.000017 lr = 0.0000950\n",
      "[0/25][850/9765] Loss_D: 0.1812 Loss_G: 0.1159 Convergence: 0.2216 k= 0.000000 lr = 0.0000950\n",
      "[0/25][860/9765] Loss_D: 0.1842 Loss_G: 0.0625 Convergence: 0.1888 k= 0.000016 lr = 0.0000950\n",
      "[0/25][870/9765] Loss_D: 0.1611 Loss_G: 0.0581 Convergence: 0.1648 k= 0.000023 lr = 0.0000950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][880/9765] Loss_D: 0.1662 Loss_G: 0.0822 Convergence: 0.1816 k= 0.000004 lr = 0.0000950\n",
      "[0/25][890/9765] Loss_D: 0.1843 Loss_G: 0.1251 Convergence: 0.2302 k= 0.000000 lr = 0.0000950\n",
      "[0/25][900/9765] Loss_D: 0.1905 Loss_G: 0.1057 Convergence: 0.2138 k= 0.000000 lr = 0.0000950\n",
      "[0/25][910/9765] Loss_D: 0.1874 Loss_G: 0.0683 Convergence: 0.1804 k= 0.000017 lr = 0.0000950\n",
      "[0/25][920/9765] Loss_D: 0.1834 Loss_G: 0.2145 Convergence: 0.3413 k= 0.000000 lr = 0.0000950\n",
      "[0/25][930/9765] Loss_D: 0.1985 Loss_G: 0.0451 Convergence: 0.2237 k= 0.000075 lr = 0.0000950\n",
      "[0/25][940/9765] Loss_D: 0.1593 Loss_G: 0.0687 Convergence: 0.1651 k= 0.000218 lr = 0.0000950\n",
      "[0/25][950/9765] Loss_D: 0.1875 Loss_G: 0.0736 Convergence: 0.1848 k= 0.000215 lr = 0.0000950\n",
      "[0/25][960/9765] Loss_D: 0.1656 Loss_G: 0.0378 Convergence: 0.1886 k= 0.000369 lr = 0.0000950\n",
      "[0/25][970/9765] Loss_D: 0.1605 Loss_G: 0.0313 Convergence: 0.1954 k= 0.000673 lr = 0.0000950\n",
      "[0/25][980/9765] Loss_D: 0.1560 Loss_G: 0.0330 Convergence: 0.1800 k= 0.000956 lr = 0.0000950\n",
      "[0/25][990/9765] Loss_D: 0.1918 Loss_G: 0.0761 Convergence: 0.1860 k= 0.001239 lr = 0.0000950\n",
      "[0/25][1000/9765] Loss_D: 0.1737 Loss_G: 0.0392 Convergence: 0.1974 k= 0.001496 lr = 0.0000950\n",
      "[0/25][1010/9765] Loss_D: 0.1608 Loss_G: 0.0534 Convergence: 0.1705 k= 0.001766 lr = 0.0000950\n",
      "[0/25][1020/9765] Loss_D: 0.1831 Loss_G: 0.0433 Convergence: 0.2062 k= 0.001907 lr = 0.0000950\n",
      "[0/25][1030/9765] Loss_D: 0.1654 Loss_G: 0.0678 Convergence: 0.1759 k= 0.002108 lr = 0.0000950\n",
      "[0/25][1040/9765] Loss_D: 0.1751 Loss_G: 0.0461 Convergence: 0.1939 k= 0.002344 lr = 0.0000950\n",
      "[0/25][1050/9765] Loss_D: 0.1572 Loss_G: 0.0390 Convergence: 0.1801 k= 0.002561 lr = 0.0000950\n",
      "[0/25][1060/9765] Loss_D: 0.1722 Loss_G: 0.0409 Convergence: 0.2014 k= 0.002776 lr = 0.0000950\n",
      "[0/25][1070/9765] Loss_D: 0.1744 Loss_G: 0.0758 Convergence: 0.1798 k= 0.002882 lr = 0.0000950\n",
      "[0/25][1080/9765] Loss_D: 0.1674 Loss_G: 0.0386 Convergence: 0.1939 k= 0.002886 lr = 0.0000950\n",
      "[0/25][1090/9765] Loss_D: 0.1640 Loss_G: 0.0546 Convergence: 0.1703 k= 0.003083 lr = 0.0000950\n",
      "[0/25][1100/9765] Loss_D: 0.1632 Loss_G: 0.0592 Convergence: 0.1645 k= 0.003241 lr = 0.0000950\n",
      "[0/25][1110/9765] Loss_D: 0.1562 Loss_G: 0.0479 Convergence: 0.1879 k= 0.003403 lr = 0.0000950\n",
      "[0/25][1120/9765] Loss_D: 0.1780 Loss_G: 0.0456 Convergence: 0.2107 k= 0.003509 lr = 0.0000950\n",
      "[0/25][1130/9765] Loss_D: 0.1695 Loss_G: 0.0471 Convergence: 0.1878 k= 0.003538 lr = 0.0000950\n",
      "[0/25][1140/9765] Loss_D: 0.1692 Loss_G: 0.0506 Convergence: 0.2090 k= 0.003702 lr = 0.0000950\n",
      "[0/25][1150/9765] Loss_D: 0.1772 Loss_G: 0.0551 Convergence: 0.1865 k= 0.003853 lr = 0.0000950\n",
      "[0/25][1160/9765] Loss_D: 0.1716 Loss_G: 0.0737 Convergence: 0.1770 k= 0.003993 lr = 0.0000950\n",
      "[0/25][1170/9765] Loss_D: 0.1595 Loss_G: 0.0696 Convergence: 0.1701 k= 0.004083 lr = 0.0000950\n",
      "[0/25][1180/9765] Loss_D: 0.1714 Loss_G: 0.0550 Convergence: 0.1848 k= 0.004109 lr = 0.0000950\n",
      "[0/25][1190/9765] Loss_D: 0.1664 Loss_G: 0.0991 Convergence: 0.1988 k= 0.004073 lr = 0.0000950\n",
      "[0/25][1200/9765] Loss_D: 0.1675 Loss_G: 0.1186 Convergence: 0.2122 k= 0.003962 lr = 0.0000950\n",
      "[0/25][1210/9765] Loss_D: 0.1617 Loss_G: 0.0524 Convergence: 0.1750 k= 0.003902 lr = 0.0000950\n",
      "[0/25][1220/9765] Loss_D: 0.1552 Loss_G: 0.1051 Convergence: 0.2063 k= 0.003804 lr = 0.0000950\n",
      "[0/25][1230/9765] Loss_D: 0.1616 Loss_G: 0.0573 Convergence: 0.1644 k= 0.003840 lr = 0.0000950\n",
      "[0/25][1240/9765] Loss_D: 0.1715 Loss_G: 0.0689 Convergence: 0.1807 k= 0.003834 lr = 0.0000950\n",
      "[0/25][1250/9765] Loss_D: 0.1756 Loss_G: 0.0590 Convergence: 0.1797 k= 0.003908 lr = 0.0000950\n",
      "[0/25][1260/9765] Loss_D: 0.1674 Loss_G: 0.0588 Convergence: 0.1716 k= 0.003723 lr = 0.0000950\n",
      "[0/25][1270/9765] Loss_D: 0.1632 Loss_G: 0.0740 Convergence: 0.1687 k= 0.003660 lr = 0.0000950\n",
      "[0/25][1280/9765] Loss_D: 0.1579 Loss_G: 0.0691 Convergence: 0.1627 k= 0.003631 lr = 0.0000950\n",
      "[0/25][1290/9765] Loss_D: 0.1627 Loss_G: 0.0722 Convergence: 0.1693 k= 0.003585 lr = 0.0000950\n",
      "[0/25][1300/9765] Loss_D: 0.1708 Loss_G: 0.0616 Convergence: 0.1668 k= 0.003554 lr = 0.0000950\n",
      "[0/25][1310/9765] Loss_D: 0.1640 Loss_G: 0.0744 Convergence: 0.1710 k= 0.003506 lr = 0.0000950\n",
      "[0/25][1320/9765] Loss_D: 0.1659 Loss_G: 0.0796 Convergence: 0.1774 k= 0.003315 lr = 0.0000950\n",
      "[0/25][1330/9765] Loss_D: 0.1564 Loss_G: 0.0579 Convergence: 0.1605 k= 0.003308 lr = 0.0000950\n",
      "[0/25][1340/9765] Loss_D: 0.1503 Loss_G: 0.0714 Convergence: 0.1604 k= 0.003212 lr = 0.0000950\n",
      "[0/25][1350/9765] Loss_D: 0.1781 Loss_G: 0.0711 Convergence: 0.1770 k= 0.003115 lr = 0.0000950\n",
      "[0/25][1360/9765] Loss_D: 0.1671 Loss_G: 0.0917 Convergence: 0.1881 k= 0.002902 lr = 0.0000950\n",
      "[0/25][1370/9765] Loss_D: 0.1680 Loss_G: 0.0690 Convergence: 0.1752 k= 0.002847 lr = 0.0000950\n",
      "[0/25][1380/9765] Loss_D: 0.1475 Loss_G: 0.0755 Convergence: 0.1621 k= 0.002643 lr = 0.0000950\n",
      "[0/25][1390/9765] Loss_D: 0.1660 Loss_G: 0.0817 Convergence: 0.1790 k= 0.002583 lr = 0.0000950\n",
      "[0/25][1400/9765] Loss_D: 0.1852 Loss_G: 0.0697 Convergence: 0.1657 k= 0.002560 lr = 0.0000950\n",
      "[0/25][1410/9765] Loss_D: 0.1509 Loss_G: 0.0748 Convergence: 0.1642 k= 0.002497 lr = 0.0000950\n",
      "[0/25][1420/9765] Loss_D: 0.1726 Loss_G: 0.0449 Convergence: 0.1881 k= 0.002561 lr = 0.0000950\n",
      "[0/25][1430/9765] Loss_D: 0.1755 Loss_G: 0.0650 Convergence: 0.1740 k= 0.002604 lr = 0.0000950\n",
      "[0/25][1440/9765] Loss_D: 0.1570 Loss_G: 0.0692 Convergence: 0.1650 k= 0.002551 lr = 0.0000950\n",
      "[0/25][1450/9765] Loss_D: 0.1510 Loss_G: 0.0632 Convergence: 0.1523 k= 0.002504 lr = 0.0000950\n",
      "[0/25][1460/9765] Loss_D: 0.1691 Loss_G: 0.1120 Convergence: 0.2134 k= 0.002541 lr = 0.0000950\n",
      "[0/25][1470/9765] Loss_D: 0.1694 Loss_G: 0.1074 Convergence: 0.2028 k= 0.002596 lr = 0.0000950\n",
      "[0/25][1480/9765] Loss_D: 0.1648 Loss_G: 0.0722 Convergence: 0.1670 k= 0.002550 lr = 0.0000950\n",
      "[0/25][1490/9765] Loss_D: 0.1743 Loss_G: 0.0858 Convergence: 0.1806 k= 0.002499 lr = 0.0000950\n",
      "[0/25][1500/9765] Loss_D: 0.1750 Loss_G: 0.0502 Convergence: 0.1888 k= 0.002495 lr = 0.0000950\n",
      "[0/25][1510/9765] Loss_D: 0.1578 Loss_G: 0.0609 Convergence: 0.1563 k= 0.002503 lr = 0.0000950\n",
      "[0/25][1520/9765] Loss_D: 0.1605 Loss_G: 0.0912 Convergence: 0.1886 k= 0.002440 lr = 0.0000950\n",
      "[0/25][1530/9765] Loss_D: 0.1524 Loss_G: 0.0616 Convergence: 0.1519 k= 0.002397 lr = 0.0000950\n",
      "[0/25][1540/9765] Loss_D: 0.1832 Loss_G: 0.0464 Convergence: 0.2065 k= 0.002407 lr = 0.0000950\n",
      "[0/25][1550/9765] Loss_D: 0.1441 Loss_G: 0.0618 Convergence: 0.1473 k= 0.002378 lr = 0.0000950\n",
      "[0/25][1560/9765] Loss_D: 0.1628 Loss_G: 0.0727 Convergence: 0.1804 k= 0.002423 lr = 0.0000950\n",
      "[0/25][1570/9765] Loss_D: 0.1486 Loss_G: 0.0725 Convergence: 0.1611 k= 0.002281 lr = 0.0000950\n",
      "[0/25][1580/9765] Loss_D: 0.1590 Loss_G: 0.0673 Convergence: 0.1644 k= 0.002340 lr = 0.0000950\n",
      "[0/25][1590/9765] Loss_D: 0.1616 Loss_G: 0.0717 Convergence: 0.1654 k= 0.002373 lr = 0.0000950\n",
      "[0/25][1600/9765] Loss_D: 0.1540 Loss_G: 0.0411 Convergence: 0.1673 k= 0.002274 lr = 0.0000950\n",
      "[0/25][1610/9765] Loss_D: 0.1575 Loss_G: 0.0598 Convergence: 0.1536 k= 0.002281 lr = 0.0000950\n",
      "[0/25][1620/9765] Loss_D: 0.1770 Loss_G: 0.0832 Convergence: 0.1861 k= 0.002306 lr = 0.0000950\n",
      "[0/25][1630/9765] Loss_D: 0.1641 Loss_G: 0.1228 Convergence: 0.2220 k= 0.002228 lr = 0.0000950\n",
      "[0/25][1640/9765] Loss_D: 0.1468 Loss_G: 0.0537 Convergence: 0.1594 k= 0.002259 lr = 0.0000950\n",
      "[0/25][1650/9765] Loss_D: 0.1627 Loss_G: 0.0604 Convergence: 0.1615 k= 0.002192 lr = 0.0000950\n",
      "[0/25][1660/9765] Loss_D: 0.1525 Loss_G: 0.0555 Convergence: 0.1542 k= 0.002150 lr = 0.0000950\n",
      "[0/25][1670/9765] Loss_D: 0.1387 Loss_G: 0.0823 Convergence: 0.1638 k= 0.002099 lr = 0.0000950\n",
      "[0/25][1680/9765] Loss_D: 0.1521 Loss_G: 0.0698 Convergence: 0.1604 k= 0.002076 lr = 0.0000950\n",
      "[0/25][1690/9765] Loss_D: 0.1569 Loss_G: 0.0681 Convergence: 0.1592 k= 0.002037 lr = 0.0000950\n",
      "[0/25][1700/9765] Loss_D: 0.1680 Loss_G: 0.0678 Convergence: 0.1664 k= 0.002007 lr = 0.0000950\n",
      "[0/25][1710/9765] Loss_D: 0.1542 Loss_G: 0.0559 Convergence: 0.1609 k= 0.001984 lr = 0.0000950\n",
      "[0/25][1720/9765] Loss_D: 0.1612 Loss_G: 0.0988 Convergence: 0.1928 k= 0.001940 lr = 0.0000950\n",
      "[0/25][1730/9765] Loss_D: 0.1546 Loss_G: 0.0864 Convergence: 0.1781 k= 0.001779 lr = 0.0000950\n",
      "[0/25][1740/9765] Loss_D: 0.1605 Loss_G: 0.0629 Convergence: 0.1604 k= 0.001781 lr = 0.0000950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][1750/9765] Loss_D: 0.1594 Loss_G: 0.0584 Convergence: 0.1742 k= 0.001700 lr = 0.0000950\n",
      "[0/25][1760/9765] Loss_D: 0.1489 Loss_G: 0.0597 Convergence: 0.1482 k= 0.001645 lr = 0.0000950\n",
      "[0/25][1770/9765] Loss_D: 0.1595 Loss_G: 0.0623 Convergence: 0.1579 k= 0.001602 lr = 0.0000950\n",
      "[0/25][1780/9765] Loss_D: 0.1796 Loss_G: 0.0624 Convergence: 0.1722 k= 0.001670 lr = 0.0000950\n",
      "[0/25][1790/9765] Loss_D: 0.1594 Loss_G: 0.0566 Convergence: 0.1639 k= 0.001599 lr = 0.0000950\n",
      "[0/25][1800/9765] Loss_D: 0.1749 Loss_G: 0.0647 Convergence: 0.1723 k= 0.001566 lr = 0.0000950\n",
      "[0/25][1810/9765] Loss_D: 0.1666 Loss_G: 0.0544 Convergence: 0.1665 k= 0.001548 lr = 0.0000950\n",
      "[0/25][1820/9765] Loss_D: 0.1648 Loss_G: 0.0699 Convergence: 0.1669 k= 0.001488 lr = 0.0000950\n",
      "[0/25][1830/9765] Loss_D: 0.1566 Loss_G: 0.0588 Convergence: 0.1626 k= 0.001387 lr = 0.0000950\n",
      "[0/25][1840/9765] Loss_D: 0.1637 Loss_G: 0.0732 Convergence: 0.1663 k= 0.001360 lr = 0.0000950\n",
      "[0/25][1850/9765] Loss_D: 0.1532 Loss_G: 0.0681 Convergence: 0.1631 k= 0.001401 lr = 0.0000950\n",
      "[0/25][1860/9765] Loss_D: 0.1440 Loss_G: 0.0757 Convergence: 0.1634 k= 0.001221 lr = 0.0000950\n",
      "[0/25][1870/9765] Loss_D: 0.1695 Loss_G: 0.0734 Convergence: 0.1781 k= 0.001180 lr = 0.0000950\n",
      "[0/25][1880/9765] Loss_D: 0.1507 Loss_G: 0.0617 Convergence: 0.1495 k= 0.001125 lr = 0.0000950\n",
      "[0/25][1890/9765] Loss_D: 0.1587 Loss_G: 0.0775 Convergence: 0.1720 k= 0.001105 lr = 0.0000950\n",
      "[0/25][1900/9765] Loss_D: 0.1589 Loss_G: 0.0926 Convergence: 0.1856 k= 0.001014 lr = 0.0000950\n",
      "[0/25][1910/9765] Loss_D: 0.1511 Loss_G: 0.0660 Convergence: 0.1679 k= 0.000967 lr = 0.0000950\n",
      "[0/25][1920/9765] Loss_D: 0.1613 Loss_G: 0.0801 Convergence: 0.1846 k= 0.000825 lr = 0.0000950\n",
      "[0/25][1930/9765] Loss_D: 0.1762 Loss_G: 0.0597 Convergence: 0.1811 k= 0.000770 lr = 0.0000950\n",
      "[0/25][1940/9765] Loss_D: 0.1700 Loss_G: 0.0735 Convergence: 0.1734 k= 0.000731 lr = 0.0000950\n",
      "[0/25][1950/9765] Loss_D: 0.1455 Loss_G: 0.0643 Convergence: 0.1526 k= 0.000728 lr = 0.0000950\n",
      "[0/25][1960/9765] Loss_D: 0.1626 Loss_G: 0.0532 Convergence: 0.1714 k= 0.000744 lr = 0.0000950\n",
      "[0/25][1970/9765] Loss_D: 0.1654 Loss_G: 0.1249 Convergence: 0.2187 k= 0.000663 lr = 0.0000950\n",
      "[0/25][1980/9765] Loss_D: 0.1716 Loss_G: 0.0545 Convergence: 0.1833 k= 0.000651 lr = 0.0000950\n",
      "[0/25][1990/9765] Loss_D: 0.1645 Loss_G: 0.0429 Convergence: 0.1862 k= 0.000754 lr = 0.0000950\n",
      "[0/25][2000/9765] Loss_D: 0.1522 Loss_G: 0.0575 Convergence: 0.1504 k= 0.000808 lr = 0.0000950\n",
      "[0/25][2010/9765] Loss_D: 0.1437 Loss_G: 0.0602 Convergence: 0.1476 k= 0.000913 lr = 0.0000902\n",
      "[0/25][2020/9765] Loss_D: 0.1643 Loss_G: 0.0535 Convergence: 0.1715 k= 0.000935 lr = 0.0000902\n",
      "[0/25][2030/9765] Loss_D: 0.1596 Loss_G: 0.0454 Convergence: 0.1719 k= 0.000999 lr = 0.0000902\n",
      "[0/25][2040/9765] Loss_D: 0.1543 Loss_G: 0.0555 Convergence: 0.1766 k= 0.001077 lr = 0.0000902\n",
      "[0/25][2050/9765] Loss_D: 0.1421 Loss_G: 0.0612 Convergence: 0.1456 k= 0.001151 lr = 0.0000902\n",
      "[0/25][2060/9765] Loss_D: 0.1509 Loss_G: 0.0559 Convergence: 0.1521 k= 0.001226 lr = 0.0000902\n",
      "[0/25][2070/9765] Loss_D: 0.1475 Loss_G: 0.0547 Convergence: 0.1452 k= 0.001299 lr = 0.0000902\n",
      "[0/25][2080/9765] Loss_D: 0.1493 Loss_G: 0.0604 Convergence: 0.1476 k= 0.001319 lr = 0.0000902\n",
      "[0/25][2090/9765] Loss_D: 0.1559 Loss_G: 0.0478 Convergence: 0.1614 k= 0.001306 lr = 0.0000902\n",
      "[0/25][2100/9765] Loss_D: 0.1561 Loss_G: 0.0430 Convergence: 0.1701 k= 0.001442 lr = 0.0000902\n",
      "[0/25][2110/9765] Loss_D: 0.1607 Loss_G: 0.0523 Convergence: 0.1821 k= 0.001605 lr = 0.0000902\n",
      "[0/25][2120/9765] Loss_D: 0.1570 Loss_G: 0.0429 Convergence: 0.1716 k= 0.001763 lr = 0.0000902\n",
      "[0/25][2130/9765] Loss_D: 0.1438 Loss_G: 0.0437 Convergence: 0.1563 k= 0.001910 lr = 0.0000902\n",
      "[0/25][2140/9765] Loss_D: 0.1523 Loss_G: 0.0618 Convergence: 0.1501 k= 0.002004 lr = 0.0000902\n",
      "[0/25][2150/9765] Loss_D: 0.1617 Loss_G: 0.0491 Convergence: 0.1787 k= 0.002079 lr = 0.0000902\n",
      "[0/25][2160/9765] Loss_D: 0.1456 Loss_G: 0.0437 Convergence: 0.1554 k= 0.002148 lr = 0.0000902\n",
      "[0/25][2170/9765] Loss_D: 0.1541 Loss_G: 0.0541 Convergence: 0.1567 k= 0.002244 lr = 0.0000902\n",
      "[0/25][2180/9765] Loss_D: 0.1560 Loss_G: 0.0429 Convergence: 0.1731 k= 0.002334 lr = 0.0000902\n",
      "[0/25][2190/9765] Loss_D: 0.1429 Loss_G: 0.0693 Convergence: 0.1542 k= 0.002438 lr = 0.0000902\n",
      "[0/25][2200/9765] Loss_D: 0.1498 Loss_G: 0.0388 Convergence: 0.1674 k= 0.002601 lr = 0.0000902\n",
      "[0/25][2210/9765] Loss_D: 0.1591 Loss_G: 0.0394 Convergence: 0.1909 k= 0.002687 lr = 0.0000902\n",
      "[0/25][2220/9765] Loss_D: 0.1600 Loss_G: 0.0478 Convergence: 0.1729 k= 0.002862 lr = 0.0000902\n",
      "[0/25][2230/9765] Loss_D: 0.1595 Loss_G: 0.0481 Convergence: 0.1664 k= 0.002936 lr = 0.0000902\n",
      "[0/25][2240/9765] Loss_D: 0.1647 Loss_G: 0.0692 Convergence: 0.1662 k= 0.003028 lr = 0.0000902\n",
      "[0/25][2250/9765] Loss_D: 0.1484 Loss_G: 0.0396 Convergence: 0.1702 k= 0.003056 lr = 0.0000902\n",
      "[0/25][2260/9765] Loss_D: 0.1469 Loss_G: 0.0390 Convergence: 0.1595 k= 0.003159 lr = 0.0000902\n",
      "[0/25][2270/9765] Loss_D: 0.1394 Loss_G: 0.0480 Convergence: 0.1427 k= 0.003217 lr = 0.0000902\n",
      "[0/25][2280/9765] Loss_D: 0.1477 Loss_G: 0.0699 Convergence: 0.1574 k= 0.003246 lr = 0.0000902\n",
      "[0/25][2290/9765] Loss_D: 0.1746 Loss_G: 0.0434 Convergence: 0.1620 k= 0.003357 lr = 0.0000902\n",
      "[0/25][2300/9765] Loss_D: 0.1557 Loss_G: 0.0552 Convergence: 0.1553 k= 0.003389 lr = 0.0000902\n",
      "[0/25][2310/9765] Loss_D: 0.1398 Loss_G: 0.0544 Convergence: 0.1418 k= 0.003464 lr = 0.0000902\n",
      "[0/25][2320/9765] Loss_D: 0.1436 Loss_G: 0.0494 Convergence: 0.1513 k= 0.003492 lr = 0.0000902\n",
      "[0/25][2330/9765] Loss_D: 0.1537 Loss_G: 0.0631 Convergence: 0.1504 k= 0.003556 lr = 0.0000902\n",
      "[0/25][2340/9765] Loss_D: 0.1447 Loss_G: 0.0458 Convergence: 0.1555 k= 0.003661 lr = 0.0000902\n",
      "[0/25][2350/9765] Loss_D: 0.1572 Loss_G: 0.0473 Convergence: 0.1665 k= 0.003673 lr = 0.0000902\n",
      "[0/25][2360/9765] Loss_D: 0.1485 Loss_G: 0.0393 Convergence: 0.1621 k= 0.003818 lr = 0.0000902\n",
      "[0/25][2370/9765] Loss_D: 0.1428 Loss_G: 0.0449 Convergence: 0.1499 k= 0.003953 lr = 0.0000902\n",
      "[0/25][2380/9765] Loss_D: 0.1505 Loss_G: 0.0767 Convergence: 0.1638 k= 0.003968 lr = 0.0000902\n",
      "[0/25][2390/9765] Loss_D: 0.1603 Loss_G: 0.0577 Convergence: 0.1638 k= 0.003938 lr = 0.0000902\n",
      "[0/25][2400/9765] Loss_D: 0.1461 Loss_G: 0.0635 Convergence: 0.1484 k= 0.003984 lr = 0.0000902\n",
      "[0/25][2410/9765] Loss_D: 0.1476 Loss_G: 0.0481 Convergence: 0.1532 k= 0.004111 lr = 0.0000902\n",
      "[0/25][2420/9765] Loss_D: 0.1477 Loss_G: 0.0454 Convergence: 0.1526 k= 0.004141 lr = 0.0000902\n",
      "[0/25][2430/9765] Loss_D: 0.1470 Loss_G: 0.0651 Convergence: 0.1501 k= 0.004215 lr = 0.0000902\n",
      "[0/25][2440/9765] Loss_D: 0.1642 Loss_G: 0.0866 Convergence: 0.1753 k= 0.004177 lr = 0.0000902\n",
      "[0/25][2450/9765] Loss_D: 0.1375 Loss_G: 0.0631 Convergence: 0.1446 k= 0.004190 lr = 0.0000902\n",
      "[0/25][2460/9765] Loss_D: 0.1473 Loss_G: 0.0533 Convergence: 0.1474 k= 0.004270 lr = 0.0000902\n",
      "[0/25][2470/9765] Loss_D: 0.1350 Loss_G: 0.0563 Convergence: 0.1355 k= 0.004350 lr = 0.0000902\n",
      "[0/25][2480/9765] Loss_D: 0.1330 Loss_G: 0.0463 Convergence: 0.1497 k= 0.004411 lr = 0.0000902\n",
      "[0/25][2490/9765] Loss_D: 0.1419 Loss_G: 0.0566 Convergence: 0.1389 k= 0.004440 lr = 0.0000902\n",
      "[0/25][2500/9765] Loss_D: 0.1495 Loss_G: 0.0557 Convergence: 0.1484 k= 0.004494 lr = 0.0000902\n",
      "[0/25][2510/9765] Loss_D: 0.1347 Loss_G: 0.0492 Convergence: 0.1337 k= 0.004576 lr = 0.0000902\n",
      "[0/25][2520/9765] Loss_D: 0.1433 Loss_G: 0.0642 Convergence: 0.1498 k= 0.004625 lr = 0.0000902\n",
      "[0/25][2530/9765] Loss_D: 0.1402 Loss_G: 0.0516 Convergence: 0.1391 k= 0.004703 lr = 0.0000902\n",
      "[0/25][2540/9765] Loss_D: 0.1326 Loss_G: 0.0614 Convergence: 0.1401 k= 0.004751 lr = 0.0000902\n",
      "[0/25][2550/9765] Loss_D: 0.1384 Loss_G: 0.0498 Convergence: 0.1451 k= 0.004798 lr = 0.0000902\n",
      "[0/25][2560/9765] Loss_D: 0.1420 Loss_G: 0.0501 Convergence: 0.1522 k= 0.004832 lr = 0.0000902\n",
      "[0/25][2570/9765] Loss_D: 0.1410 Loss_G: 0.1099 Convergence: 0.1937 k= 0.004805 lr = 0.0000902\n",
      "[0/25][2580/9765] Loss_D: 0.1569 Loss_G: 0.0665 Convergence: 0.1581 k= 0.004829 lr = 0.0000902\n",
      "[0/25][2590/9765] Loss_D: 0.1520 Loss_G: 0.0460 Convergence: 0.1532 k= 0.004935 lr = 0.0000902\n",
      "[0/25][2600/9765] Loss_D: 0.1469 Loss_G: 0.0519 Convergence: 0.1466 k= 0.005004 lr = 0.0000902\n",
      "[0/25][2610/9765] Loss_D: 0.1381 Loss_G: 0.1117 Convergence: 0.1924 k= 0.004866 lr = 0.0000902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][2620/9765] Loss_D: 0.1443 Loss_G: 0.0553 Convergence: 0.1435 k= 0.004853 lr = 0.0000902\n",
      "[0/25][2630/9765] Loss_D: 0.1574 Loss_G: 0.0541 Convergence: 0.1580 k= 0.004874 lr = 0.0000902\n",
      "[0/25][2640/9765] Loss_D: 0.1448 Loss_G: 0.0504 Convergence: 0.1434 k= 0.004845 lr = 0.0000902\n",
      "[0/25][2650/9765] Loss_D: 0.1308 Loss_G: 0.0529 Convergence: 0.1297 k= 0.004933 lr = 0.0000902\n",
      "[0/25][2660/9765] Loss_D: 0.1224 Loss_G: 0.0592 Convergence: 0.1307 k= 0.004884 lr = 0.0000902\n",
      "[0/25][2670/9765] Loss_D: 0.1433 Loss_G: 0.0590 Convergence: 0.1428 k= 0.004933 lr = 0.0000902\n",
      "[0/25][2680/9765] Loss_D: 0.1517 Loss_G: 0.0519 Convergence: 0.1506 k= 0.004947 lr = 0.0000902\n",
      "[0/25][2690/9765] Loss_D: 0.1452 Loss_G: 0.0534 Convergence: 0.1463 k= 0.004935 lr = 0.0000902\n",
      "[0/25][2700/9765] Loss_D: 0.1315 Loss_G: 0.0554 Convergence: 0.1334 k= 0.004956 lr = 0.0000902\n",
      "[0/25][2710/9765] Loss_D: 0.1392 Loss_G: 0.0621 Convergence: 0.1511 k= 0.004956 lr = 0.0000902\n",
      "[0/25][2720/9765] Loss_D: 0.1366 Loss_G: 0.0492 Convergence: 0.1364 k= 0.004997 lr = 0.0000902\n",
      "[0/25][2730/9765] Loss_D: 0.1551 Loss_G: 0.0660 Convergence: 0.1605 k= 0.004970 lr = 0.0000902\n",
      "[0/25][2740/9765] Loss_D: 0.1484 Loss_G: 0.0642 Convergence: 0.1574 k= 0.004993 lr = 0.0000902\n",
      "[0/25][2750/9765] Loss_D: 0.1346 Loss_G: 0.0494 Convergence: 0.1371 k= 0.004999 lr = 0.0000902\n",
      "[0/25][2760/9765] Loss_D: 0.1409 Loss_G: 0.0531 Convergence: 0.1409 k= 0.005026 lr = 0.0000902\n",
      "[0/25][2770/9765] Loss_D: 0.1362 Loss_G: 0.0572 Convergence: 0.1369 k= 0.004990 lr = 0.0000902\n",
      "[0/25][2780/9765] Loss_D: 0.1414 Loss_G: 0.0675 Convergence: 0.1505 k= 0.005037 lr = 0.0000902\n",
      "[0/25][2790/9765] Loss_D: 0.1384 Loss_G: 0.0521 Convergence: 0.1418 k= 0.005059 lr = 0.0000902\n",
      "[0/25][2800/9765] Loss_D: 0.1408 Loss_G: 0.0453 Convergence: 0.1427 k= 0.005093 lr = 0.0000902\n",
      "[0/25][2810/9765] Loss_D: 0.1312 Loss_G: 0.0426 Convergence: 0.1374 k= 0.005130 lr = 0.0000902\n",
      "[0/25][2820/9765] Loss_D: 0.1397 Loss_G: 0.0525 Convergence: 0.1442 k= 0.005221 lr = 0.0000902\n",
      "[0/25][2830/9765] Loss_D: 0.1351 Loss_G: 0.0468 Convergence: 0.1395 k= 0.005252 lr = 0.0000902\n",
      "[0/25][2840/9765] Loss_D: 0.1630 Loss_G: 0.0678 Convergence: 0.1640 k= 0.005307 lr = 0.0000902\n",
      "[0/25][2850/9765] Loss_D: 0.1555 Loss_G: 0.0502 Convergence: 0.1527 k= 0.005363 lr = 0.0000902\n",
      "[0/25][2860/9765] Loss_D: 0.1457 Loss_G: 0.0582 Convergence: 0.1479 k= 0.005438 lr = 0.0000902\n",
      "[0/25][2870/9765] Loss_D: 0.1352 Loss_G: 0.0508 Convergence: 0.1381 k= 0.005505 lr = 0.0000902\n",
      "[0/25][2880/9765] Loss_D: 0.1520 Loss_G: 0.0569 Convergence: 0.1485 k= 0.005600 lr = 0.0000902\n",
      "[0/25][2890/9765] Loss_D: 0.1349 Loss_G: 0.0714 Convergence: 0.1531 k= 0.005635 lr = 0.0000902\n",
      "[0/25][2900/9765] Loss_D: 0.1381 Loss_G: 0.0429 Convergence: 0.1420 k= 0.005710 lr = 0.0000902\n",
      "[0/25][2910/9765] Loss_D: 0.1454 Loss_G: 0.0531 Convergence: 0.1457 k= 0.005798 lr = 0.0000902\n",
      "[0/25][2920/9765] Loss_D: 0.1412 Loss_G: 0.0526 Convergence: 0.1486 k= 0.005839 lr = 0.0000902\n",
      "[0/25][2930/9765] Loss_D: 0.1457 Loss_G: 0.0627 Convergence: 0.1527 k= 0.005853 lr = 0.0000902\n",
      "[0/25][2940/9765] Loss_D: 0.1458 Loss_G: 0.0490 Convergence: 0.1469 k= 0.005869 lr = 0.0000902\n",
      "[0/25][2950/9765] Loss_D: 0.1319 Loss_G: 0.0470 Convergence: 0.1335 k= 0.005925 lr = 0.0000902\n",
      "[0/25][2960/9765] Loss_D: 0.1356 Loss_G: 0.0496 Convergence: 0.1356 k= 0.005986 lr = 0.0000902\n",
      "[0/25][2970/9765] Loss_D: 0.1402 Loss_G: 0.0468 Convergence: 0.1448 k= 0.006085 lr = 0.0000902\n",
      "[0/25][2980/9765] Loss_D: 0.1320 Loss_G: 0.0436 Convergence: 0.1365 k= 0.006133 lr = 0.0000902\n",
      "[0/25][2990/9765] Loss_D: 0.1440 Loss_G: 0.0693 Convergence: 0.1536 k= 0.006173 lr = 0.0000902\n",
      "[0/25][3000/9765] Loss_D: 0.1380 Loss_G: 0.0507 Convergence: 0.1372 k= 0.006247 lr = 0.0000902\n",
      "[0/25][3010/9765] Loss_D: 0.1367 Loss_G: 0.0451 Convergence: 0.1424 k= 0.006276 lr = 0.0000902\n",
      "[0/25][3020/9765] Loss_D: 0.1425 Loss_G: 0.0504 Convergence: 0.1452 k= 0.006371 lr = 0.0000902\n",
      "[0/25][3030/9765] Loss_D: 0.1323 Loss_G: 0.0472 Convergence: 0.1328 k= 0.006476 lr = 0.0000902\n",
      "[0/25][3040/9765] Loss_D: 0.1443 Loss_G: 0.0576 Convergence: 0.1482 k= 0.006528 lr = 0.0000902\n",
      "[0/25][3050/9765] Loss_D: 0.1409 Loss_G: 0.0568 Convergence: 0.1400 k= 0.006560 lr = 0.0000902\n",
      "[0/25][3060/9765] Loss_D: 0.1354 Loss_G: 0.0522 Convergence: 0.1370 k= 0.006525 lr = 0.0000902\n",
      "[0/25][3070/9765] Loss_D: 0.1345 Loss_G: 0.0483 Convergence: 0.1381 k= 0.006543 lr = 0.0000902\n",
      "[0/25][3080/9765] Loss_D: 0.1433 Loss_G: 0.0549 Convergence: 0.1444 k= 0.006611 lr = 0.0000902\n",
      "[0/25][3090/9765] Loss_D: 0.1460 Loss_G: 0.0504 Convergence: 0.1439 k= 0.006680 lr = 0.0000902\n",
      "[0/25][3100/9765] Loss_D: 0.1418 Loss_G: 0.0612 Convergence: 0.1451 k= 0.006713 lr = 0.0000902\n",
      "[0/25][3110/9765] Loss_D: 0.1337 Loss_G: 0.0437 Convergence: 0.1406 k= 0.006765 lr = 0.0000902\n",
      "[0/25][3120/9765] Loss_D: 0.1554 Loss_G: 0.0703 Convergence: 0.1626 k= 0.006827 lr = 0.0000902\n",
      "[0/25][3130/9765] Loss_D: 0.1460 Loss_G: 0.0508 Convergence: 0.1527 k= 0.006891 lr = 0.0000902\n",
      "[0/25][3140/9765] Loss_D: 0.1329 Loss_G: 0.0443 Convergence: 0.1363 k= 0.006945 lr = 0.0000902\n",
      "[0/25][3150/9765] Loss_D: 0.1394 Loss_G: 0.0415 Convergence: 0.1490 k= 0.006992 lr = 0.0000902\n",
      "[0/25][3160/9765] Loss_D: 0.1383 Loss_G: 0.0504 Convergence: 0.1399 k= 0.007061 lr = 0.0000902\n",
      "[0/25][3170/9765] Loss_D: 0.1296 Loss_G: 0.0445 Convergence: 0.1343 k= 0.007122 lr = 0.0000902\n",
      "[0/25][3180/9765] Loss_D: 0.1295 Loss_G: 0.0627 Convergence: 0.1410 k= 0.007141 lr = 0.0000902\n",
      "[0/25][3190/9765] Loss_D: 0.1327 Loss_G: 0.0493 Convergence: 0.1346 k= 0.007220 lr = 0.0000902\n",
      "[0/25][3200/9765] Loss_D: 0.1370 Loss_G: 0.0468 Convergence: 0.1424 k= 0.007290 lr = 0.0000902\n",
      "[0/25][3210/9765] Loss_D: 0.1355 Loss_G: 0.0585 Convergence: 0.1389 k= 0.007283 lr = 0.0000902\n",
      "[0/25][3220/9765] Loss_D: 0.1540 Loss_G: 0.0595 Convergence: 0.1417 k= 0.007394 lr = 0.0000902\n",
      "[0/25][3230/9765] Loss_D: 0.1362 Loss_G: 0.0742 Convergence: 0.1560 k= 0.007340 lr = 0.0000902\n",
      "[0/25][3240/9765] Loss_D: 0.1471 Loss_G: 0.0581 Convergence: 0.1479 k= 0.007405 lr = 0.0000902\n",
      "[0/25][3250/9765] Loss_D: 0.1437 Loss_G: 0.0391 Convergence: 0.1521 k= 0.007459 lr = 0.0000902\n",
      "[0/25][3260/9765] Loss_D: 0.1301 Loss_G: 0.0468 Convergence: 0.1313 k= 0.007580 lr = 0.0000902\n",
      "[0/25][3270/9765] Loss_D: 0.1392 Loss_G: 0.0474 Convergence: 0.1498 k= 0.007633 lr = 0.0000902\n",
      "[0/25][3280/9765] Loss_D: 0.1301 Loss_G: 0.0436 Convergence: 0.1385 k= 0.007682 lr = 0.0000902\n",
      "[0/25][3290/9765] Loss_D: 0.1498 Loss_G: 0.0518 Convergence: 0.1519 k= 0.007736 lr = 0.0000902\n",
      "[0/25][3300/9765] Loss_D: 0.1266 Loss_G: 0.0513 Convergence: 0.1252 k= 0.007759 lr = 0.0000902\n",
      "[0/25][3310/9765] Loss_D: 0.1337 Loss_G: 0.0512 Convergence: 0.1360 k= 0.007834 lr = 0.0000902\n",
      "[0/25][3320/9765] Loss_D: 0.1400 Loss_G: 0.0559 Convergence: 0.1431 k= 0.007889 lr = 0.0000902\n",
      "[0/25][3330/9765] Loss_D: 0.1283 Loss_G: 0.0409 Convergence: 0.1489 k= 0.007973 lr = 0.0000902\n",
      "[0/25][3340/9765] Loss_D: 0.1438 Loss_G: 0.0414 Convergence: 0.1630 k= 0.008045 lr = 0.0000902\n",
      "[0/25][3350/9765] Loss_D: 0.1329 Loss_G: 0.0456 Convergence: 0.1365 k= 0.008107 lr = 0.0000902\n",
      "[0/25][3360/9765] Loss_D: 0.1347 Loss_G: 0.0657 Convergence: 0.1472 k= 0.008138 lr = 0.0000902\n",
      "[0/25][3370/9765] Loss_D: 0.1381 Loss_G: 0.0669 Convergence: 0.1499 k= 0.008140 lr = 0.0000902\n",
      "[0/25][3380/9765] Loss_D: 0.1482 Loss_G: 0.0402 Convergence: 0.1718 k= 0.008185 lr = 0.0000902\n",
      "[0/25][3390/9765] Loss_D: 0.1397 Loss_G: 0.0508 Convergence: 0.1398 k= 0.008242 lr = 0.0000902\n",
      "[0/25][3400/9765] Loss_D: 0.1334 Loss_G: 0.0555 Convergence: 0.1334 k= 0.008244 lr = 0.0000902\n",
      "[0/25][3410/9765] Loss_D: 0.1389 Loss_G: 0.0649 Convergence: 0.1477 k= 0.008237 lr = 0.0000902\n",
      "[0/25][3420/9765] Loss_D: 0.1303 Loss_G: 0.0457 Convergence: 0.1324 k= 0.008281 lr = 0.0000902\n",
      "[0/25][3430/9765] Loss_D: 0.1384 Loss_G: 0.0502 Convergence: 0.1415 k= 0.008380 lr = 0.0000902\n",
      "[0/25][3440/9765] Loss_D: 0.1328 Loss_G: 0.0624 Convergence: 0.1431 k= 0.008451 lr = 0.0000902\n",
      "[0/25][3450/9765] Loss_D: 0.1390 Loss_G: 0.0493 Convergence: 0.1523 k= 0.008530 lr = 0.0000902\n",
      "[0/25][3460/9765] Loss_D: 0.1412 Loss_G: 0.0458 Convergence: 0.1416 k= 0.008541 lr = 0.0000902\n",
      "[0/25][3470/9765] Loss_D: 0.1382 Loss_G: 0.0409 Convergence: 0.1481 k= 0.008600 lr = 0.0000902\n",
      "[0/25][3480/9765] Loss_D: 0.1371 Loss_G: 0.0487 Convergence: 0.1398 k= 0.008660 lr = 0.0000902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][3490/9765] Loss_D: 0.1278 Loss_G: 0.0465 Convergence: 0.1331 k= 0.008734 lr = 0.0000902\n",
      "[0/25][3500/9765] Loss_D: 0.1341 Loss_G: 0.0456 Convergence: 0.1395 k= 0.008768 lr = 0.0000902\n",
      "[0/25][3510/9765] Loss_D: 0.1304 Loss_G: 0.0452 Convergence: 0.1351 k= 0.008790 lr = 0.0000902\n",
      "[0/25][3520/9765] Loss_D: 0.1266 Loss_G: 0.0445 Convergence: 0.1373 k= 0.008816 lr = 0.0000902\n",
      "[0/25][3530/9765] Loss_D: 0.1338 Loss_G: 0.0523 Convergence: 0.1307 k= 0.008848 lr = 0.0000902\n",
      "[0/25][3540/9765] Loss_D: 0.1387 Loss_G: 0.0631 Convergence: 0.1446 k= 0.008920 lr = 0.0000902\n",
      "[0/25][3550/9765] Loss_D: 0.1291 Loss_G: 0.0430 Convergence: 0.1335 k= 0.009004 lr = 0.0000902\n",
      "[0/25][3560/9765] Loss_D: 0.1513 Loss_G: 0.0681 Convergence: 0.1623 k= 0.009069 lr = 0.0000902\n",
      "[0/25][3570/9765] Loss_D: 0.1374 Loss_G: 0.0536 Convergence: 0.1436 k= 0.009101 lr = 0.0000902\n",
      "[0/25][3580/9765] Loss_D: 0.1343 Loss_G: 0.0502 Convergence: 0.1334 k= 0.009078 lr = 0.0000902\n",
      "[0/25][3590/9765] Loss_D: 0.1495 Loss_G: 0.0456 Convergence: 0.1705 k= 0.009141 lr = 0.0000902\n",
      "[0/25][3600/9765] Loss_D: 0.1300 Loss_G: 0.0691 Convergence: 0.1466 k= 0.009152 lr = 0.0000902\n",
      "[0/25][3610/9765] Loss_D: 0.1324 Loss_G: 0.0612 Convergence: 0.1417 k= 0.009210 lr = 0.0000902\n",
      "[0/25][3620/9765] Loss_D: 0.1390 Loss_G: 0.0763 Convergence: 0.1632 k= 0.009303 lr = 0.0000902\n",
      "[0/25][3630/9765] Loss_D: 0.1339 Loss_G: 0.0451 Convergence: 0.1371 k= 0.009276 lr = 0.0000902\n",
      "[0/25][3640/9765] Loss_D: 0.1376 Loss_G: 0.0428 Convergence: 0.1453 k= 0.009363 lr = 0.0000902\n",
      "[0/25][3650/9765] Loss_D: 0.1329 Loss_G: 0.0523 Convergence: 0.1292 k= 0.009433 lr = 0.0000902\n",
      "[0/25][3660/9765] Loss_D: 0.1393 Loss_G: 0.0512 Convergence: 0.1421 k= 0.009518 lr = 0.0000902\n",
      "[0/25][3670/9765] Loss_D: 0.1314 Loss_G: 0.0548 Convergence: 0.1316 k= 0.009535 lr = 0.0000902\n",
      "[0/25][3680/9765] Loss_D: 0.1275 Loss_G: 0.0518 Convergence: 0.1272 k= 0.009533 lr = 0.0000902\n",
      "[0/25][3690/9765] Loss_D: 0.1337 Loss_G: 0.0434 Convergence: 0.1585 k= 0.009557 lr = 0.0000902\n",
      "[0/25][3700/9765] Loss_D: 0.1270 Loss_G: 0.0586 Convergence: 0.1337 k= 0.009535 lr = 0.0000902\n",
      "[0/25][3710/9765] Loss_D: 0.1397 Loss_G: 0.0963 Convergence: 0.1792 k= 0.009547 lr = 0.0000902\n",
      "[0/25][3720/9765] Loss_D: 0.1244 Loss_G: 0.0475 Convergence: 0.1251 k= 0.009565 lr = 0.0000902\n",
      "[0/25][3730/9765] Loss_D: 0.1320 Loss_G: 0.0577 Convergence: 0.1356 k= 0.009601 lr = 0.0000902\n",
      "[0/25][3740/9765] Loss_D: 0.1445 Loss_G: 0.0536 Convergence: 0.1533 k= 0.009684 lr = 0.0000902\n",
      "[0/25][3750/9765] Loss_D: 0.1390 Loss_G: 0.0543 Convergence: 0.1350 k= 0.009730 lr = 0.0000902\n",
      "[0/25][3760/9765] Loss_D: 0.1176 Loss_G: 0.0486 Convergence: 0.1192 k= 0.009792 lr = 0.0000902\n",
      "[0/25][3770/9765] Loss_D: 0.1356 Loss_G: 0.0454 Convergence: 0.1407 k= 0.009861 lr = 0.0000902\n",
      "[0/25][3780/9765] Loss_D: 0.1391 Loss_G: 0.0496 Convergence: 0.1343 k= 0.009943 lr = 0.0000902\n",
      "[0/25][3790/9765] Loss_D: 0.1372 Loss_G: 0.0659 Convergence: 0.1454 k= 0.009876 lr = 0.0000902\n",
      "[0/25][3800/9765] Loss_D: 0.1226 Loss_G: 0.0577 Convergence: 0.1319 k= 0.009887 lr = 0.0000902\n",
      "[0/25][3810/9765] Loss_D: 0.1363 Loss_G: 0.0430 Convergence: 0.1366 k= 0.009972 lr = 0.0000902\n",
      "[0/25][3820/9765] Loss_D: 0.1332 Loss_G: 0.0490 Convergence: 0.1411 k= 0.010012 lr = 0.0000902\n",
      "[0/25][3830/9765] Loss_D: 0.1312 Loss_G: 0.0433 Convergence: 0.1386 k= 0.010078 lr = 0.0000902\n",
      "[0/25][3840/9765] Loss_D: 0.1330 Loss_G: 0.0559 Convergence: 0.1327 k= 0.010086 lr = 0.0000902\n",
      "[0/25][3850/9765] Loss_D: 0.1438 Loss_G: 0.0456 Convergence: 0.1572 k= 0.010099 lr = 0.0000902\n",
      "[0/25][3860/9765] Loss_D: 0.1342 Loss_G: 0.0463 Convergence: 0.1366 k= 0.010147 lr = 0.0000902\n",
      "[0/25][3870/9765] Loss_D: 0.1352 Loss_G: 0.0595 Convergence: 0.1423 k= 0.010226 lr = 0.0000902\n",
      "[0/25][3880/9765] Loss_D: 0.1456 Loss_G: 0.0493 Convergence: 0.1501 k= 0.010281 lr = 0.0000902\n",
      "[0/25][3890/9765] Loss_D: 0.1437 Loss_G: 0.0392 Convergence: 0.1586 k= 0.010377 lr = 0.0000902\n",
      "[0/25][3900/9765] Loss_D: 0.1226 Loss_G: 0.0401 Convergence: 0.1288 k= 0.010469 lr = 0.0000902\n",
      "[0/25][3910/9765] Loss_D: 0.1278 Loss_G: 0.0668 Convergence: 0.1444 k= 0.010493 lr = 0.0000902\n",
      "[0/25][3920/9765] Loss_D: 0.1305 Loss_G: 0.0469 Convergence: 0.1322 k= 0.010560 lr = 0.0000902\n",
      "[0/25][3930/9765] Loss_D: 0.1333 Loss_G: 0.0493 Convergence: 0.1351 k= 0.010573 lr = 0.0000902\n",
      "[0/25][3940/9765] Loss_D: 0.1248 Loss_G: 0.0370 Convergence: 0.1405 k= 0.010555 lr = 0.0000902\n",
      "[0/25][3950/9765] Loss_D: 0.1273 Loss_G: 0.0382 Convergence: 0.1358 k= 0.010628 lr = 0.0000902\n",
      "[0/25][3960/9765] Loss_D: 0.1436 Loss_G: 0.0450 Convergence: 0.1520 k= 0.010721 lr = 0.0000902\n",
      "[0/25][3970/9765] Loss_D: 0.1445 Loss_G: 0.0510 Convergence: 0.1467 k= 0.010723 lr = 0.0000902\n",
      "[0/25][3980/9765] Loss_D: 0.1341 Loss_G: 0.0544 Convergence: 0.1424 k= 0.010785 lr = 0.0000902\n",
      "[0/25][3990/9765] Loss_D: 0.1412 Loss_G: 0.0466 Convergence: 0.1479 k= 0.010862 lr = 0.0000902\n",
      "[0/25][4000/9765] Loss_D: 0.1312 Loss_G: 0.0418 Convergence: 0.1381 k= 0.010962 lr = 0.0000902\n",
      "[0/25][4010/9765] Loss_D: 0.1313 Loss_G: 0.0495 Convergence: 0.1295 k= 0.011044 lr = 0.0000857\n",
      "[0/25][4020/9765] Loss_D: 0.1281 Loss_G: 0.0472 Convergence: 0.1290 k= 0.011095 lr = 0.0000857\n",
      "[0/25][4030/9765] Loss_D: 0.1345 Loss_G: 0.0509 Convergence: 0.1339 k= 0.011117 lr = 0.0000857\n",
      "[0/25][4040/9765] Loss_D: 0.1415 Loss_G: 0.0549 Convergence: 0.1399 k= 0.011155 lr = 0.0000857\n",
      "[0/25][4050/9765] Loss_D: 0.1273 Loss_G: 0.0440 Convergence: 0.1329 k= 0.011184 lr = 0.0000857\n",
      "[0/25][4060/9765] Loss_D: 0.1315 Loss_G: 0.0393 Convergence: 0.1417 k= 0.011283 lr = 0.0000857\n",
      "[0/25][4070/9765] Loss_D: 0.1315 Loss_G: 0.0510 Convergence: 0.1347 k= 0.011334 lr = 0.0000857\n",
      "[0/25][4080/9765] Loss_D: 0.1362 Loss_G: 0.0465 Convergence: 0.1524 k= 0.011406 lr = 0.0000857\n",
      "[0/25][4090/9765] Loss_D: 0.1340 Loss_G: 0.0537 Convergence: 0.1319 k= 0.011449 lr = 0.0000857\n",
      "[0/25][4100/9765] Loss_D: 0.1434 Loss_G: 0.0756 Convergence: 0.1619 k= 0.011498 lr = 0.0000857\n",
      "[0/25][4110/9765] Loss_D: 0.1247 Loss_G: 0.0551 Convergence: 0.1284 k= 0.011501 lr = 0.0000857\n",
      "[0/25][4120/9765] Loss_D: 0.1261 Loss_G: 0.0471 Convergence: 0.1264 k= 0.011548 lr = 0.0000857\n",
      "[0/25][4130/9765] Loss_D: 0.1324 Loss_G: 0.0405 Convergence: 0.1416 k= 0.011638 lr = 0.0000857\n",
      "[0/25][4140/9765] Loss_D: 0.1337 Loss_G: 0.0589 Convergence: 0.1388 k= 0.011702 lr = 0.0000857\n",
      "[0/25][4150/9765] Loss_D: 0.1499 Loss_G: 0.0651 Convergence: 0.1513 k= 0.011768 lr = 0.0000857\n",
      "[0/25][4160/9765] Loss_D: 0.1266 Loss_G: 0.0574 Convergence: 0.1363 k= 0.011784 lr = 0.0000857\n",
      "[0/25][4170/9765] Loss_D: 0.1373 Loss_G: 0.0444 Convergence: 0.1488 k= 0.011820 lr = 0.0000857\n",
      "[0/25][4180/9765] Loss_D: 0.1399 Loss_G: 0.0389 Convergence: 0.1548 k= 0.011891 lr = 0.0000857\n",
      "[0/25][4190/9765] Loss_D: 0.1349 Loss_G: 0.0367 Convergence: 0.1511 k= 0.012010 lr = 0.0000857\n",
      "[0/25][4200/9765] Loss_D: 0.1344 Loss_G: 0.0377 Convergence: 0.1461 k= 0.012101 lr = 0.0000857\n",
      "[0/25][4210/9765] Loss_D: 0.1356 Loss_G: 0.0641 Convergence: 0.1480 k= 0.012170 lr = 0.0000857\n",
      "[0/25][4220/9765] Loss_D: 0.1255 Loss_G: 0.0485 Convergence: 0.1259 k= 0.012157 lr = 0.0000857\n",
      "[0/25][4230/9765] Loss_D: 0.1161 Loss_G: 0.0369 Convergence: 0.1238 k= 0.012189 lr = 0.0000857\n",
      "[0/25][4240/9765] Loss_D: 0.1239 Loss_G: 0.0369 Convergence: 0.1324 k= 0.012232 lr = 0.0000857\n",
      "[0/25][4250/9765] Loss_D: 0.1376 Loss_G: 0.0501 Convergence: 0.1414 k= 0.012290 lr = 0.0000857\n",
      "[0/25][4260/9765] Loss_D: 0.1323 Loss_G: 0.0797 Convergence: 0.1635 k= 0.012320 lr = 0.0000857\n",
      "[0/25][4270/9765] Loss_D: 0.1296 Loss_G: 0.0482 Convergence: 0.1300 k= 0.012375 lr = 0.0000857\n",
      "[0/25][4280/9765] Loss_D: 0.1339 Loss_G: 0.0518 Convergence: 0.1310 k= 0.012381 lr = 0.0000857\n",
      "[0/25][4290/9765] Loss_D: 0.1216 Loss_G: 0.0435 Convergence: 0.1276 k= 0.012465 lr = 0.0000857\n",
      "[0/25][4300/9765] Loss_D: 0.1275 Loss_G: 0.0401 Convergence: 0.1414 k= 0.012569 lr = 0.0000857\n",
      "[0/25][4310/9765] Loss_D: 0.1257 Loss_G: 0.0481 Convergence: 0.1266 k= 0.012654 lr = 0.0000857\n",
      "[0/25][4320/9765] Loss_D: 0.1413 Loss_G: 0.0484 Convergence: 0.1450 k= 0.012730 lr = 0.0000857\n",
      "[0/25][4330/9765] Loss_D: 0.1305 Loss_G: 0.0370 Convergence: 0.1471 k= 0.012797 lr = 0.0000857\n",
      "[0/25][4340/9765] Loss_D: 0.1306 Loss_G: 0.0419 Convergence: 0.1394 k= 0.012869 lr = 0.0000857\n",
      "[0/25][4350/9765] Loss_D: 0.1390 Loss_G: 0.0424 Convergence: 0.1480 k= 0.012930 lr = 0.0000857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][4360/9765] Loss_D: 0.1567 Loss_G: 0.1431 Convergence: 0.2486 k= 0.013019 lr = 0.0000857\n",
      "[0/25][4370/9765] Loss_D: 0.1328 Loss_G: 0.0522 Convergence: 0.1339 k= 0.013047 lr = 0.0000857\n",
      "[0/25][4380/9765] Loss_D: 0.1329 Loss_G: 0.0579 Convergence: 0.1390 k= 0.013006 lr = 0.0000857\n",
      "[0/25][4390/9765] Loss_D: 0.1298 Loss_G: 0.0595 Convergence: 0.1357 k= 0.012976 lr = 0.0000857\n",
      "[0/25][4400/9765] Loss_D: 0.1377 Loss_G: 0.0445 Convergence: 0.1449 k= 0.013026 lr = 0.0000857\n",
      "[0/25][4410/9765] Loss_D: 0.1248 Loss_G: 0.0439 Convergence: 0.1296 k= 0.013086 lr = 0.0000857\n",
      "[0/25][4420/9765] Loss_D: 0.1210 Loss_G: 0.0401 Convergence: 0.1272 k= 0.013159 lr = 0.0000857\n",
      "[0/25][4430/9765] Loss_D: 0.1336 Loss_G: 0.0595 Convergence: 0.1376 k= 0.013171 lr = 0.0000857\n",
      "[0/25][4440/9765] Loss_D: 0.1294 Loss_G: 0.0368 Convergence: 0.1379 k= 0.013285 lr = 0.0000857\n",
      "[0/25][4450/9765] Loss_D: 0.1297 Loss_G: 0.0532 Convergence: 0.1320 k= 0.013326 lr = 0.0000857\n",
      "[0/25][4460/9765] Loss_D: 0.1277 Loss_G: 0.0465 Convergence: 0.1297 k= 0.013398 lr = 0.0000857\n",
      "[0/25][4470/9765] Loss_D: 0.1438 Loss_G: 0.0421 Convergence: 0.1520 k= 0.013454 lr = 0.0000857\n",
      "[0/25][4480/9765] Loss_D: 0.1352 Loss_G: 0.0401 Convergence: 0.1459 k= 0.013473 lr = 0.0000857\n",
      "[0/25][4490/9765] Loss_D: 0.1362 Loss_G: 0.0445 Convergence: 0.1425 k= 0.013557 lr = 0.0000857\n",
      "[0/25][4500/9765] Loss_D: 0.1197 Loss_G: 0.0435 Convergence: 0.1230 k= 0.013583 lr = 0.0000857\n",
      "[0/25][4510/9765] Loss_D: 0.1374 Loss_G: 0.0475 Convergence: 0.1365 k= 0.013651 lr = 0.0000857\n",
      "[0/25][4520/9765] Loss_D: 0.1304 Loss_G: 0.0386 Convergence: 0.1369 k= 0.013662 lr = 0.0000857\n",
      "[0/25][4530/9765] Loss_D: 0.1307 Loss_G: 0.0422 Convergence: 0.1364 k= 0.013712 lr = 0.0000857\n",
      "[0/25][4540/9765] Loss_D: 0.1367 Loss_G: 0.0520 Convergence: 0.1311 k= 0.013721 lr = 0.0000857\n",
      "[0/25][4550/9765] Loss_D: 0.1365 Loss_G: 0.0547 Convergence: 0.1355 k= 0.013761 lr = 0.0000857\n",
      "[0/25][4560/9765] Loss_D: 0.1343 Loss_G: 0.0507 Convergence: 0.1329 k= 0.013826 lr = 0.0000857\n",
      "[0/25][4570/9765] Loss_D: 0.1347 Loss_G: 0.0565 Convergence: 0.1367 k= 0.013894 lr = 0.0000857\n",
      "[0/25][4580/9765] Loss_D: 0.1353 Loss_G: 0.0409 Convergence: 0.1422 k= 0.013956 lr = 0.0000857\n",
      "[0/25][4590/9765] Loss_D: 0.1343 Loss_G: 0.0791 Convergence: 0.1621 k= 0.013999 lr = 0.0000857\n",
      "[0/25][4600/9765] Loss_D: 0.1447 Loss_G: 0.0737 Convergence: 0.1615 k= 0.013998 lr = 0.0000857\n",
      "[0/25][4610/9765] Loss_D: 0.1411 Loss_G: 0.0557 Convergence: 0.1408 k= 0.013984 lr = 0.0000857\n",
      "[0/25][4620/9765] Loss_D: 0.1373 Loss_G: 0.0589 Convergence: 0.1401 k= 0.014034 lr = 0.0000857\n",
      "[0/25][4630/9765] Loss_D: 0.1257 Loss_G: 0.0367 Convergence: 0.1351 k= 0.014115 lr = 0.0000857\n",
      "[0/25][4640/9765] Loss_D: 0.1235 Loss_G: 0.0547 Convergence: 0.1318 k= 0.014135 lr = 0.0000857\n",
      "[0/25][4650/9765] Loss_D: 0.1359 Loss_G: 0.0503 Convergence: 0.1340 k= 0.014124 lr = 0.0000857\n",
      "[0/25][4660/9765] Loss_D: 0.1261 Loss_G: 0.0469 Convergence: 0.1275 k= 0.014151 lr = 0.0000857\n",
      "[0/25][4670/9765] Loss_D: 0.1310 Loss_G: 0.0454 Convergence: 0.1369 k= 0.014235 lr = 0.0000857\n",
      "[0/25][4680/9765] Loss_D: 0.1423 Loss_G: 0.0526 Convergence: 0.1337 k= 0.014304 lr = 0.0000857\n",
      "[0/25][4690/9765] Loss_D: 0.1301 Loss_G: 0.0531 Convergence: 0.1295 k= 0.014313 lr = 0.0000857\n",
      "[0/25][4700/9765] Loss_D: 0.1400 Loss_G: 0.0509 Convergence: 0.1409 k= 0.014347 lr = 0.0000857\n",
      "[0/25][4710/9765] Loss_D: 0.1244 Loss_G: 0.0527 Convergence: 0.1260 k= 0.014396 lr = 0.0000857\n",
      "[0/25][4720/9765] Loss_D: 0.1258 Loss_G: 0.0510 Convergence: 0.1253 k= 0.014459 lr = 0.0000857\n",
      "[0/25][4730/9765] Loss_D: 0.1326 Loss_G: 0.0410 Convergence: 0.1407 k= 0.014481 lr = 0.0000857\n",
      "[0/25][4740/9765] Loss_D: 0.1421 Loss_G: 0.0591 Convergence: 0.1436 k= 0.014518 lr = 0.0000857\n",
      "[0/25][4750/9765] Loss_D: 0.1273 Loss_G: 0.0419 Convergence: 0.1370 k= 0.014590 lr = 0.0000857\n",
      "[0/25][4760/9765] Loss_D: 0.1325 Loss_G: 0.0446 Convergence: 0.1378 k= 0.014637 lr = 0.0000857\n",
      "[0/25][4770/9765] Loss_D: 0.1416 Loss_G: 0.0418 Convergence: 0.1508 k= 0.014695 lr = 0.0000857\n",
      "[0/25][4780/9765] Loss_D: 0.1347 Loss_G: 0.0538 Convergence: 0.1359 k= 0.014765 lr = 0.0000857\n",
      "[0/25][4790/9765] Loss_D: 0.1318 Loss_G: 0.0443 Convergence: 0.1362 k= 0.014824 lr = 0.0000857\n",
      "[0/25][4800/9765] Loss_D: 0.1201 Loss_G: 0.0530 Convergence: 0.1248 k= 0.014895 lr = 0.0000857\n",
      "[0/25][4810/9765] Loss_D: 0.1275 Loss_G: 0.0642 Convergence: 0.1421 k= 0.014946 lr = 0.0000857\n",
      "[0/25][4820/9765] Loss_D: 0.1276 Loss_G: 0.0449 Convergence: 0.1308 k= 0.014986 lr = 0.0000857\n",
      "[0/25][4830/9765] Loss_D: 0.1215 Loss_G: 0.0499 Convergence: 0.1237 k= 0.015026 lr = 0.0000857\n",
      "[0/25][4840/9765] Loss_D: 0.1293 Loss_G: 0.0530 Convergence: 0.1303 k= 0.015014 lr = 0.0000857\n",
      "[0/25][4850/9765] Loss_D: 0.1343 Loss_G: 0.0496 Convergence: 0.1343 k= 0.015039 lr = 0.0000857\n",
      "[0/25][4860/9765] Loss_D: 0.1244 Loss_G: 0.0429 Convergence: 0.1273 k= 0.015077 lr = 0.0000857\n",
      "[0/25][4870/9765] Loss_D: 0.1185 Loss_G: 0.0538 Convergence: 0.1257 k= 0.015133 lr = 0.0000857\n",
      "[0/25][4880/9765] Loss_D: 0.1263 Loss_G: 0.0472 Convergence: 0.1293 k= 0.015190 lr = 0.0000857\n",
      "[0/25][4890/9765] Loss_D: 0.1326 Loss_G: 0.0539 Convergence: 0.1313 k= 0.015283 lr = 0.0000857\n",
      "[0/25][4900/9765] Loss_D: 0.1294 Loss_G: 0.0402 Convergence: 0.1315 k= 0.015357 lr = 0.0000857\n",
      "[0/25][4910/9765] Loss_D: 0.1244 Loss_G: 0.0680 Convergence: 0.1478 k= 0.015389 lr = 0.0000857\n",
      "[0/25][4920/9765] Loss_D: 0.1241 Loss_G: 0.0650 Convergence: 0.1380 k= 0.015414 lr = 0.0000857\n",
      "[0/25][4930/9765] Loss_D: 0.1341 Loss_G: 0.0456 Convergence: 0.1421 k= 0.015472 lr = 0.0000857\n",
      "[0/25][4940/9765] Loss_D: 0.1259 Loss_G: 0.0382 Convergence: 0.1331 k= 0.015535 lr = 0.0000857\n",
      "[0/25][4950/9765] Loss_D: 0.1279 Loss_G: 0.0392 Convergence: 0.1374 k= 0.015641 lr = 0.0000857\n",
      "[0/25][4960/9765] Loss_D: 0.1239 Loss_G: 0.0524 Convergence: 0.1281 k= 0.015701 lr = 0.0000857\n",
      "[0/25][4970/9765] Loss_D: 0.1178 Loss_G: 0.0474 Convergence: 0.1166 k= 0.015772 lr = 0.0000857\n",
      "[0/25][4980/9765] Loss_D: 0.1257 Loss_G: 0.0466 Convergence: 0.1245 k= 0.015820 lr = 0.0000857\n",
      "[0/25][4990/9765] Loss_D: 0.1309 Loss_G: 0.0378 Convergence: 0.1413 k= 0.015913 lr = 0.0000857\n",
      "[0/25][5000/9765] Loss_D: 0.1212 Loss_G: 0.0739 Convergence: 0.1470 k= 0.015945 lr = 0.0000857\n",
      "[0/25][5010/9765] Loss_D: 0.1223 Loss_G: 0.0393 Convergence: 0.1319 k= 0.016015 lr = 0.0000857\n",
      "[0/25][5020/9765] Loss_D: 0.1295 Loss_G: 0.0353 Convergence: 0.1413 k= 0.016084 lr = 0.0000857\n",
      "[0/25][5030/9765] Loss_D: 0.1220 Loss_G: 0.0521 Convergence: 0.1250 k= 0.016128 lr = 0.0000857\n",
      "[0/25][5040/9765] Loss_D: 0.1447 Loss_G: 0.0487 Convergence: 0.1509 k= 0.016167 lr = 0.0000857\n",
      "[0/25][5050/9765] Loss_D: 0.1233 Loss_G: 0.0646 Convergence: 0.1462 k= 0.016252 lr = 0.0000857\n",
      "[0/25][5060/9765] Loss_D: 0.1258 Loss_G: 0.0492 Convergence: 0.1245 k= 0.016290 lr = 0.0000857\n",
      "[0/25][5070/9765] Loss_D: 0.1171 Loss_G: 0.0745 Convergence: 0.1486 k= 0.016322 lr = 0.0000857\n",
      "[0/25][5080/9765] Loss_D: 0.1290 Loss_G: 0.0384 Convergence: 0.1431 k= 0.016388 lr = 0.0000857\n",
      "[0/25][5090/9765] Loss_D: 0.1431 Loss_G: 0.0594 Convergence: 0.1453 k= 0.016408 lr = 0.0000857\n",
      "[0/25][5100/9765] Loss_D: 0.1382 Loss_G: 0.0541 Convergence: 0.1303 k= 0.016445 lr = 0.0000857\n",
      "[0/25][5110/9765] Loss_D: 0.1249 Loss_G: 0.0450 Convergence: 0.1248 k= 0.016505 lr = 0.0000857\n",
      "[0/25][5120/9765] Loss_D: 0.1285 Loss_G: 0.0358 Convergence: 0.1415 k= 0.016575 lr = 0.0000857\n",
      "[0/25][5130/9765] Loss_D: 0.1379 Loss_G: 0.0558 Convergence: 0.1368 k= 0.016642 lr = 0.0000857\n",
      "[0/25][5140/9765] Loss_D: 0.1281 Loss_G: 0.0490 Convergence: 0.1295 k= 0.016611 lr = 0.0000857\n",
      "[0/25][5150/9765] Loss_D: 0.1271 Loss_G: 0.0406 Convergence: 0.1338 k= 0.016655 lr = 0.0000857\n",
      "[0/25][5160/9765] Loss_D: 0.1308 Loss_G: 0.0639 Convergence: 0.1432 k= 0.016672 lr = 0.0000857\n",
      "[0/25][5170/9765] Loss_D: 0.1267 Loss_G: 0.0416 Convergence: 0.1342 k= 0.016698 lr = 0.0000857\n",
      "[0/25][5180/9765] Loss_D: 0.1267 Loss_G: 0.0496 Convergence: 0.1221 k= 0.016739 lr = 0.0000857\n",
      "[0/25][5190/9765] Loss_D: 0.1228 Loss_G: 0.0393 Convergence: 0.1321 k= 0.016804 lr = 0.0000857\n",
      "[0/25][5200/9765] Loss_D: 0.1269 Loss_G: 0.0477 Convergence: 0.1266 k= 0.016865 lr = 0.0000857\n",
      "[0/25][5210/9765] Loss_D: 0.1280 Loss_G: 0.0509 Convergence: 0.1285 k= 0.016917 lr = 0.0000857\n",
      "[0/25][5220/9765] Loss_D: 0.1450 Loss_G: 0.0647 Convergence: 0.1522 k= 0.016949 lr = 0.0000857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][5230/9765] Loss_D: 0.1225 Loss_G: 0.0506 Convergence: 0.1235 k= 0.017022 lr = 0.0000857\n",
      "[0/25][5240/9765] Loss_D: 0.1210 Loss_G: 0.0758 Convergence: 0.1519 k= 0.017022 lr = 0.0000857\n",
      "[0/25][5250/9765] Loss_D: 0.1354 Loss_G: 0.0517 Convergence: 0.1333 k= 0.017031 lr = 0.0000857\n",
      "[0/25][5260/9765] Loss_D: 0.1258 Loss_G: 0.0621 Convergence: 0.1374 k= 0.017028 lr = 0.0000857\n",
      "[0/25][5270/9765] Loss_D: 0.1264 Loss_G: 0.0458 Convergence: 0.1294 k= 0.017088 lr = 0.0000857\n",
      "[0/25][5280/9765] Loss_D: 0.1255 Loss_G: 0.0499 Convergence: 0.1258 k= 0.017098 lr = 0.0000857\n",
      "[0/25][5290/9765] Loss_D: 0.1322 Loss_G: 0.0481 Convergence: 0.1361 k= 0.017159 lr = 0.0000857\n",
      "[0/25][5300/9765] Loss_D: 0.1285 Loss_G: 0.0369 Convergence: 0.1395 k= 0.017226 lr = 0.0000857\n",
      "[0/25][5310/9765] Loss_D: 0.1306 Loss_G: 0.0490 Convergence: 0.1323 k= 0.017255 lr = 0.0000857\n",
      "[0/25][5320/9765] Loss_D: 0.1282 Loss_G: 0.0431 Convergence: 0.1329 k= 0.017311 lr = 0.0000857\n",
      "[0/25][5330/9765] Loss_D: 0.1266 Loss_G: 0.0569 Convergence: 0.1302 k= 0.017348 lr = 0.0000857\n",
      "[0/25][5340/9765] Loss_D: 0.1319 Loss_G: 0.0469 Convergence: 0.1462 k= 0.017379 lr = 0.0000857\n",
      "[0/25][5350/9765] Loss_D: 0.1275 Loss_G: 0.0492 Convergence: 0.1232 k= 0.017356 lr = 0.0000857\n",
      "[0/25][5360/9765] Loss_D: 0.1275 Loss_G: 0.0440 Convergence: 0.1322 k= 0.017399 lr = 0.0000857\n",
      "[0/25][5370/9765] Loss_D: 0.1279 Loss_G: 0.0521 Convergence: 0.1274 k= 0.017442 lr = 0.0000857\n",
      "[0/25][5380/9765] Loss_D: 0.1188 Loss_G: 0.0562 Convergence: 0.1276 k= 0.017506 lr = 0.0000857\n",
      "[0/25][5390/9765] Loss_D: 0.1336 Loss_G: 0.0477 Convergence: 0.1396 k= 0.017582 lr = 0.0000857\n",
      "[0/25][5400/9765] Loss_D: 0.1256 Loss_G: 0.0427 Convergence: 0.1329 k= 0.017665 lr = 0.0000857\n",
      "[0/25][5410/9765] Loss_D: 0.1311 Loss_G: 0.0587 Convergence: 0.1378 k= 0.017710 lr = 0.0000857\n",
      "[0/25][5420/9765] Loss_D: 0.1195 Loss_G: 0.0401 Convergence: 0.1286 k= 0.017742 lr = 0.0000857\n",
      "[0/25][5430/9765] Loss_D: 0.1180 Loss_G: 0.0487 Convergence: 0.1188 k= 0.017807 lr = 0.0000857\n",
      "[0/25][5440/9765] Loss_D: 0.1272 Loss_G: 0.0527 Convergence: 0.1295 k= 0.017837 lr = 0.0000857\n",
      "[0/25][5450/9765] Loss_D: 0.1244 Loss_G: 0.0424 Convergence: 0.1355 k= 0.017877 lr = 0.0000857\n",
      "[0/25][5460/9765] Loss_D: 0.1356 Loss_G: 0.0482 Convergence: 0.1303 k= 0.017892 lr = 0.0000857\n",
      "[0/25][5470/9765] Loss_D: 0.1228 Loss_G: 0.0467 Convergence: 0.1210 k= 0.017905 lr = 0.0000857\n",
      "[0/25][5480/9765] Loss_D: 0.1513 Loss_G: 0.0570 Convergence: 0.1437 k= 0.017949 lr = 0.0000857\n",
      "[0/25][5490/9765] Loss_D: 0.1277 Loss_G: 0.0441 Convergence: 0.1275 k= 0.017957 lr = 0.0000857\n",
      "[0/25][5500/9765] Loss_D: 0.1351 Loss_G: 0.0519 Convergence: 0.1278 k= 0.018052 lr = 0.0000857\n",
      "[0/25][5510/9765] Loss_D: 0.1245 Loss_G: 0.0408 Convergence: 0.1308 k= 0.018035 lr = 0.0000857\n",
      "[0/25][5520/9765] Loss_D: 0.1336 Loss_G: 0.0419 Convergence: 0.1428 k= 0.018064 lr = 0.0000857\n",
      "[0/25][5530/9765] Loss_D: 0.1226 Loss_G: 0.0426 Convergence: 0.1260 k= 0.018130 lr = 0.0000857\n",
      "[0/25][5540/9765] Loss_D: 0.1223 Loss_G: 0.0401 Convergence: 0.1290 k= 0.018209 lr = 0.0000857\n",
      "[0/25][5550/9765] Loss_D: 0.1297 Loss_G: 0.0457 Convergence: 0.1333 k= 0.018218 lr = 0.0000857\n",
      "[0/25][5560/9765] Loss_D: 0.1268 Loss_G: 0.0453 Convergence: 0.1304 k= 0.018265 lr = 0.0000857\n",
      "[0/25][5570/9765] Loss_D: 0.1298 Loss_G: 0.0430 Convergence: 0.1372 k= 0.018341 lr = 0.0000857\n",
      "[0/25][5580/9765] Loss_D: 0.1168 Loss_G: 0.0430 Convergence: 0.1174 k= 0.018385 lr = 0.0000857\n",
      "[0/25][5590/9765] Loss_D: 0.1235 Loss_G: 0.0459 Convergence: 0.1237 k= 0.018445 lr = 0.0000857\n",
      "[0/25][5600/9765] Loss_D: 0.1244 Loss_G: 0.0436 Convergence: 0.1269 k= 0.018511 lr = 0.0000857\n",
      "[0/25][5610/9765] Loss_D: 0.1259 Loss_G: 0.0502 Convergence: 0.1248 k= 0.018579 lr = 0.0000857\n",
      "[0/25][5620/9765] Loss_D: 0.1266 Loss_G: 0.0442 Convergence: 0.1259 k= 0.018609 lr = 0.0000857\n",
      "[0/25][5630/9765] Loss_D: 0.1310 Loss_G: 0.0435 Convergence: 0.1357 k= 0.018656 lr = 0.0000857\n",
      "[0/25][5640/9765] Loss_D: 0.1278 Loss_G: 0.0466 Convergence: 0.1265 k= 0.018671 lr = 0.0000857\n",
      "[0/25][5650/9765] Loss_D: 0.1321 Loss_G: 0.0446 Convergence: 0.1368 k= 0.018702 lr = 0.0000857\n",
      "[0/25][5660/9765] Loss_D: 0.1280 Loss_G: 0.0503 Convergence: 0.1310 k= 0.018752 lr = 0.0000857\n",
      "[0/25][5670/9765] Loss_D: 0.1338 Loss_G: 0.0549 Convergence: 0.1353 k= 0.018760 lr = 0.0000857\n",
      "[0/25][5680/9765] Loss_D: 0.1161 Loss_G: 0.0367 Convergence: 0.1295 k= 0.018809 lr = 0.0000857\n",
      "[0/25][5690/9765] Loss_D: 0.1353 Loss_G: 0.0547 Convergence: 0.1427 k= 0.018843 lr = 0.0000857\n",
      "[0/25][5700/9765] Loss_D: 0.1244 Loss_G: 0.0472 Convergence: 0.1241 k= 0.018868 lr = 0.0000857\n",
      "[0/25][5710/9765] Loss_D: 0.1216 Loss_G: 0.0362 Convergence: 0.1308 k= 0.018921 lr = 0.0000857\n",
      "[0/25][5720/9765] Loss_D: 0.1378 Loss_G: 0.0761 Convergence: 0.1569 k= 0.018919 lr = 0.0000857\n",
      "[0/25][5730/9765] Loss_D: 0.1242 Loss_G: 0.0609 Convergence: 0.1350 k= 0.018940 lr = 0.0000857\n",
      "[0/25][5740/9765] Loss_D: 0.1332 Loss_G: 0.0386 Convergence: 0.1439 k= 0.018988 lr = 0.0000857\n",
      "[0/25][5750/9765] Loss_D: 0.1363 Loss_G: 0.0546 Convergence: 0.1317 k= 0.019038 lr = 0.0000857\n",
      "[0/25][5760/9765] Loss_D: 0.1143 Loss_G: 0.0534 Convergence: 0.1215 k= 0.019058 lr = 0.0000857\n",
      "[0/25][5770/9765] Loss_D: 0.1320 Loss_G: 0.0574 Convergence: 0.1358 k= 0.019089 lr = 0.0000857\n",
      "[0/25][5780/9765] Loss_D: 0.1217 Loss_G: 0.0519 Convergence: 0.1243 k= 0.019150 lr = 0.0000857\n",
      "[0/25][5790/9765] Loss_D: 0.1372 Loss_G: 0.0412 Convergence: 0.1480 k= 0.019228 lr = 0.0000857\n",
      "[0/25][5800/9765] Loss_D: 0.1228 Loss_G: 0.0561 Convergence: 0.1300 k= 0.019288 lr = 0.0000857\n",
      "[0/25][5810/9765] Loss_D: 0.1290 Loss_G: 0.0769 Convergence: 0.1563 k= 0.019327 lr = 0.0000857\n",
      "[0/25][5820/9765] Loss_D: 0.1273 Loss_G: 0.0488 Convergence: 0.1290 k= 0.019374 lr = 0.0000857\n",
      "[0/25][5830/9765] Loss_D: 0.1239 Loss_G: 0.0534 Convergence: 0.1273 k= 0.019431 lr = 0.0000857\n",
      "[0/25][5840/9765] Loss_D: 0.1140 Loss_G: 0.0389 Convergence: 0.1184 k= 0.019504 lr = 0.0000857\n",
      "[0/25][5850/9765] Loss_D: 0.1347 Loss_G: 0.0465 Convergence: 0.1388 k= 0.019596 lr = 0.0000857\n",
      "[0/25][5860/9765] Loss_D: 0.1294 Loss_G: 0.0489 Convergence: 0.1272 k= 0.019622 lr = 0.0000857\n",
      "[0/25][5870/9765] Loss_D: 0.1306 Loss_G: 0.0526 Convergence: 0.1302 k= 0.019687 lr = 0.0000857\n",
      "[0/25][5880/9765] Loss_D: 0.1329 Loss_G: 0.0469 Convergence: 0.1394 k= 0.019734 lr = 0.0000857\n",
      "[0/25][5890/9765] Loss_D: 0.1260 Loss_G: 0.0422 Convergence: 0.1307 k= 0.019803 lr = 0.0000857\n",
      "[0/25][5900/9765] Loss_D: 0.1286 Loss_G: 0.0458 Convergence: 0.1287 k= 0.019861 lr = 0.0000857\n",
      "[0/25][5910/9765] Loss_D: 0.1250 Loss_G: 0.0492 Convergence: 0.1231 k= 0.019913 lr = 0.0000857\n",
      "[0/25][5920/9765] Loss_D: 0.1310 Loss_G: 0.0789 Convergence: 0.1563 k= 0.019833 lr = 0.0000857\n",
      "[0/25][5930/9765] Loss_D: 0.1220 Loss_G: 0.0423 Convergence: 0.1267 k= 0.019818 lr = 0.0000857\n",
      "[0/25][5940/9765] Loss_D: 0.1278 Loss_G: 0.0498 Convergence: 0.1240 k= 0.019883 lr = 0.0000857\n",
      "[0/25][5950/9765] Loss_D: 0.1256 Loss_G: 0.0376 Convergence: 0.1349 k= 0.019958 lr = 0.0000857\n",
      "[0/25][5960/9765] Loss_D: 0.1196 Loss_G: 0.0422 Convergence: 0.1291 k= 0.019961 lr = 0.0000857\n",
      "[0/25][5970/9765] Loss_D: 0.1427 Loss_G: 0.0497 Convergence: 0.1455 k= 0.020043 lr = 0.0000857\n",
      "[0/25][5980/9765] Loss_D: 0.1218 Loss_G: 0.0432 Convergence: 0.1258 k= 0.020129 lr = 0.0000857\n",
      "[0/25][5990/9765] Loss_D: 0.1233 Loss_G: 0.0386 Convergence: 0.1313 k= 0.020206 lr = 0.0000857\n",
      "[0/25][6000/9765] Loss_D: 0.1241 Loss_G: 0.0377 Convergence: 0.1376 k= 0.020265 lr = 0.0000857\n",
      "[0/25][6010/9765] Loss_D: 0.1158 Loss_G: 0.0442 Convergence: 0.1152 k= 0.020305 lr = 0.0000815\n",
      "[0/25][6020/9765] Loss_D: 0.1311 Loss_G: 0.0496 Convergence: 0.1334 k= 0.020367 lr = 0.0000815\n",
      "[0/25][6030/9765] Loss_D: 0.1299 Loss_G: 0.0420 Convergence: 0.1331 k= 0.020404 lr = 0.0000815\n",
      "[0/25][6040/9765] Loss_D: 0.1346 Loss_G: 0.0458 Convergence: 0.1382 k= 0.020445 lr = 0.0000815\n",
      "[0/25][6050/9765] Loss_D: 0.1218 Loss_G: 0.0479 Convergence: 0.1196 k= 0.020470 lr = 0.0000815\n",
      "[0/25][6060/9765] Loss_D: 0.1257 Loss_G: 0.0463 Convergence: 0.1399 k= 0.020556 lr = 0.0000815\n",
      "[0/25][6070/9765] Loss_D: 0.1188 Loss_G: 0.0489 Convergence: 0.1192 k= 0.020576 lr = 0.0000815\n",
      "[0/25][6080/9765] Loss_D: 0.1181 Loss_G: 0.0382 Convergence: 0.1248 k= 0.020627 lr = 0.0000815\n",
      "[0/25][6090/9765] Loss_D: 0.1334 Loss_G: 0.0422 Convergence: 0.1411 k= 0.020695 lr = 0.0000815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][6100/9765] Loss_D: 0.1298 Loss_G: 0.0373 Convergence: 0.1422 k= 0.020788 lr = 0.0000815\n",
      "[0/25][6110/9765] Loss_D: 0.1214 Loss_G: 0.0411 Convergence: 0.1307 k= 0.020836 lr = 0.0000815\n",
      "[0/25][6120/9765] Loss_D: 0.1288 Loss_G: 0.0487 Convergence: 0.1307 k= 0.020896 lr = 0.0000815\n",
      "[0/25][6130/9765] Loss_D: 0.1254 Loss_G: 0.0566 Convergence: 0.1317 k= 0.020920 lr = 0.0000815\n",
      "[0/25][6140/9765] Loss_D: 0.1289 Loss_G: 0.0536 Convergence: 0.1288 k= 0.020943 lr = 0.0000815\n",
      "[0/25][6150/9765] Loss_D: 0.1346 Loss_G: 0.0506 Convergence: 0.1334 k= 0.020990 lr = 0.0000815\n",
      "[0/25][6160/9765] Loss_D: 0.1263 Loss_G: 0.0457 Convergence: 0.1288 k= 0.021047 lr = 0.0000815\n",
      "[0/25][6170/9765] Loss_D: 0.1291 Loss_G: 0.0573 Convergence: 0.1351 k= 0.021059 lr = 0.0000815\n",
      "[0/25][6180/9765] Loss_D: 0.1338 Loss_G: 0.0447 Convergence: 0.1396 k= 0.021135 lr = 0.0000815\n",
      "[0/25][6190/9765] Loss_D: 0.1225 Loss_G: 0.0327 Convergence: 0.1395 k= 0.021208 lr = 0.0000815\n",
      "[0/25][6200/9765] Loss_D: 0.1133 Loss_G: 0.0485 Convergence: 0.1187 k= 0.021278 lr = 0.0000815\n",
      "[0/25][6210/9765] Loss_D: 0.1216 Loss_G: 0.0505 Convergence: 0.1233 k= 0.021303 lr = 0.0000815\n",
      "[0/25][6220/9765] Loss_D: 0.1249 Loss_G: 0.0483 Convergence: 0.1243 k= 0.021364 lr = 0.0000815\n",
      "[0/25][6230/9765] Loss_D: 0.1205 Loss_G: 0.0459 Convergence: 0.1239 k= 0.021415 lr = 0.0000815\n",
      "[0/25][6240/9765] Loss_D: 0.1182 Loss_G: 0.0389 Convergence: 0.1246 k= 0.021438 lr = 0.0000815\n",
      "[0/25][6250/9765] Loss_D: 0.1249 Loss_G: 0.0471 Convergence: 0.1256 k= 0.021483 lr = 0.0000815\n",
      "[0/25][6260/9765] Loss_D: 0.1149 Loss_G: 0.0409 Convergence: 0.1210 k= 0.021541 lr = 0.0000815\n",
      "[0/25][6270/9765] Loss_D: 0.1285 Loss_G: 0.0443 Convergence: 0.1324 k= 0.021626 lr = 0.0000815\n",
      "[0/25][6280/9765] Loss_D: 0.1278 Loss_G: 0.0443 Convergence: 0.1311 k= 0.021697 lr = 0.0000815\n",
      "[0/25][6290/9765] Loss_D: 0.1284 Loss_G: 0.0593 Convergence: 0.1390 k= 0.021785 lr = 0.0000815\n",
      "[0/25][6300/9765] Loss_D: 0.1170 Loss_G: 0.0437 Convergence: 0.1167 k= 0.021809 lr = 0.0000815\n",
      "[0/25][6310/9765] Loss_D: 0.1191 Loss_G: 0.0517 Convergence: 0.1249 k= 0.021873 lr = 0.0000815\n",
      "[0/25][6320/9765] Loss_D: 0.1319 Loss_G: 0.0471 Convergence: 0.1347 k= 0.021953 lr = 0.0000815\n",
      "[0/25][6330/9765] Loss_D: 0.1302 Loss_G: 0.0744 Convergence: 0.1493 k= 0.021945 lr = 0.0000815\n",
      "[0/25][6340/9765] Loss_D: 0.1391 Loss_G: 0.0666 Convergence: 0.1460 k= 0.021950 lr = 0.0000815\n",
      "[0/25][6350/9765] Loss_D: 0.1305 Loss_G: 0.0480 Convergence: 0.1315 k= 0.021993 lr = 0.0000815\n",
      "[0/25][6360/9765] Loss_D: 0.1255 Loss_G: 0.0484 Convergence: 0.1373 k= 0.022041 lr = 0.0000815\n",
      "[0/25][6370/9765] Loss_D: 0.1201 Loss_G: 0.0403 Convergence: 0.1263 k= 0.022101 lr = 0.0000815\n",
      "[0/25][6380/9765] Loss_D: 0.1117 Loss_G: 0.0457 Convergence: 0.1149 k= 0.022183 lr = 0.0000815\n",
      "[0/25][6390/9765] Loss_D: 0.1296 Loss_G: 0.0521 Convergence: 0.1285 k= 0.022260 lr = 0.0000815\n",
      "[0/25][6400/9765] Loss_D: 0.1170 Loss_G: 0.0463 Convergence: 0.1146 k= 0.022327 lr = 0.0000815\n",
      "[0/25][6410/9765] Loss_D: 0.1218 Loss_G: 0.0427 Convergence: 0.1273 k= 0.022418 lr = 0.0000815\n",
      "[0/25][6420/9765] Loss_D: 0.1204 Loss_G: 0.0439 Convergence: 0.1223 k= 0.022466 lr = 0.0000815\n",
      "[0/25][6430/9765] Loss_D: 0.1262 Loss_G: 0.0396 Convergence: 0.1349 k= 0.022531 lr = 0.0000815\n",
      "[0/25][6440/9765] Loss_D: 0.1226 Loss_G: 0.0616 Convergence: 0.1393 k= 0.022560 lr = 0.0000815\n",
      "[0/25][6450/9765] Loss_D: 0.1290 Loss_G: 0.0477 Convergence: 0.1309 k= 0.022583 lr = 0.0000815\n",
      "[0/25][6460/9765] Loss_D: 0.1199 Loss_G: 0.0596 Convergence: 0.1329 k= 0.022640 lr = 0.0000815\n",
      "[0/25][6470/9765] Loss_D: 0.1238 Loss_G: 0.0401 Convergence: 0.1304 k= 0.022682 lr = 0.0000815\n",
      "[0/25][6480/9765] Loss_D: 0.1250 Loss_G: 0.0496 Convergence: 0.1242 k= 0.022730 lr = 0.0000815\n",
      "[0/25][6490/9765] Loss_D: 0.1224 Loss_G: 0.0399 Convergence: 0.1370 k= 0.022786 lr = 0.0000815\n",
      "[0/25][6500/9765] Loss_D: 0.1232 Loss_G: 0.0514 Convergence: 0.1238 k= 0.022777 lr = 0.0000815\n",
      "[0/25][6510/9765] Loss_D: 0.1236 Loss_G: 0.0482 Convergence: 0.1215 k= 0.022816 lr = 0.0000815\n",
      "[0/25][6520/9765] Loss_D: 0.1234 Loss_G: 0.0600 Convergence: 0.1336 k= 0.022828 lr = 0.0000815\n",
      "[0/25][6530/9765] Loss_D: 0.1201 Loss_G: 0.0448 Convergence: 0.1229 k= 0.022822 lr = 0.0000815\n",
      "[0/25][6540/9765] Loss_D: 0.1290 Loss_G: 0.0433 Convergence: 0.1357 k= 0.022871 lr = 0.0000815\n",
      "[0/25][6550/9765] Loss_D: 0.1291 Loss_G: 0.0605 Convergence: 0.1352 k= 0.022894 lr = 0.0000815\n",
      "[0/25][6560/9765] Loss_D: 0.1269 Loss_G: 0.0418 Convergence: 0.1330 k= 0.022936 lr = 0.0000815\n",
      "[0/25][6570/9765] Loss_D: 0.1231 Loss_G: 0.0504 Convergence: 0.1331 k= 0.022970 lr = 0.0000815\n",
      "[0/25][6580/9765] Loss_D: 0.1173 Loss_G: 0.0413 Convergence: 0.1212 k= 0.023004 lr = 0.0000815\n",
      "[0/25][6590/9765] Loss_D: 0.1267 Loss_G: 0.0410 Convergence: 0.1310 k= 0.023053 lr = 0.0000815\n",
      "[0/25][6600/9765] Loss_D: 0.1154 Loss_G: 0.0424 Convergence: 0.1185 k= 0.023111 lr = 0.0000815\n",
      "[0/25][6610/9765] Loss_D: 0.1275 Loss_G: 0.0582 Convergence: 0.1356 k= 0.023112 lr = 0.0000815\n",
      "[0/25][6620/9765] Loss_D: 0.1215 Loss_G: 0.0450 Convergence: 0.1226 k= 0.023144 lr = 0.0000815\n",
      "[0/25][6630/9765] Loss_D: 0.1280 Loss_G: 0.0520 Convergence: 0.1287 k= 0.023189 lr = 0.0000815\n",
      "[0/25][6640/9765] Loss_D: 0.1283 Loss_G: 0.0466 Convergence: 0.1341 k= 0.023203 lr = 0.0000815\n",
      "[0/25][6650/9765] Loss_D: 0.1204 Loss_G: 0.0598 Convergence: 0.1326 k= 0.023181 lr = 0.0000815\n",
      "[0/25][6660/9765] Loss_D: 0.1301 Loss_G: 0.0490 Convergence: 0.1325 k= 0.023215 lr = 0.0000815\n",
      "[0/25][6670/9765] Loss_D: 0.1158 Loss_G: 0.0454 Convergence: 0.1221 k= 0.023253 lr = 0.0000815\n",
      "[0/25][6680/9765] Loss_D: 0.1263 Loss_G: 0.0446 Convergence: 0.1299 k= 0.023219 lr = 0.0000815\n",
      "[0/25][6690/9765] Loss_D: 0.1249 Loss_G: 0.0462 Convergence: 0.1272 k= 0.023231 lr = 0.0000815\n",
      "[0/25][6700/9765] Loss_D: 0.1207 Loss_G: 0.0428 Convergence: 0.1247 k= 0.023258 lr = 0.0000815\n",
      "[0/25][6710/9765] Loss_D: 0.1377 Loss_G: 0.0417 Convergence: 0.1503 k= 0.023325 lr = 0.0000815\n",
      "[0/25][6720/9765] Loss_D: 0.1186 Loss_G: 0.0473 Convergence: 0.1175 k= 0.023392 lr = 0.0000815\n",
      "[0/25][6730/9765] Loss_D: 0.1140 Loss_G: 0.0466 Convergence: 0.1147 k= 0.023444 lr = 0.0000815\n",
      "[0/25][6740/9765] Loss_D: 0.1210 Loss_G: 0.0486 Convergence: 0.1225 k= 0.023490 lr = 0.0000815\n",
      "[0/25][6750/9765] Loss_D: 0.1254 Loss_G: 0.0558 Convergence: 0.1292 k= 0.023469 lr = 0.0000815\n",
      "[0/25][6760/9765] Loss_D: 0.1281 Loss_G: 0.0584 Convergence: 0.1332 k= 0.023483 lr = 0.0000815\n",
      "[0/25][6770/9765] Loss_D: 0.1268 Loss_G: 0.0400 Convergence: 0.1349 k= 0.023552 lr = 0.0000815\n",
      "[0/25][6780/9765] Loss_D: 0.1298 Loss_G: 0.0400 Convergence: 0.1388 k= 0.023613 lr = 0.0000815\n",
      "[0/25][6790/9765] Loss_D: 0.1338 Loss_G: 0.0407 Convergence: 0.1437 k= 0.023683 lr = 0.0000815\n",
      "[0/25][6800/9765] Loss_D: 0.1284 Loss_G: 0.0504 Convergence: 0.1306 k= 0.023726 lr = 0.0000815\n",
      "[0/25][6810/9765] Loss_D: 0.1193 Loss_G: 0.0539 Convergence: 0.1317 k= 0.023773 lr = 0.0000815\n",
      "[0/25][6820/9765] Loss_D: 0.1272 Loss_G: 0.0414 Convergence: 0.1336 k= 0.023788 lr = 0.0000815\n",
      "[0/25][6830/9765] Loss_D: 0.1217 Loss_G: 0.0474 Convergence: 0.1211 k= 0.023844 lr = 0.0000815\n",
      "[0/25][6840/9765] Loss_D: 0.1224 Loss_G: 0.0407 Convergence: 0.1328 k= 0.023876 lr = 0.0000815\n",
      "[0/25][6850/9765] Loss_D: 0.1184 Loss_G: 0.0418 Convergence: 0.1229 k= 0.023926 lr = 0.0000815\n",
      "[0/25][6860/9765] Loss_D: 0.1270 Loss_G: 0.0443 Convergence: 0.1271 k= 0.023983 lr = 0.0000815\n",
      "[0/25][6870/9765] Loss_D: 0.1219 Loss_G: 0.0413 Convergence: 0.1285 k= 0.024031 lr = 0.0000815\n",
      "[0/25][6880/9765] Loss_D: 0.1359 Loss_G: 0.0567 Convergence: 0.1375 k= 0.024029 lr = 0.0000815\n",
      "[0/25][6890/9765] Loss_D: 0.1243 Loss_G: 0.0416 Convergence: 0.1308 k= 0.024074 lr = 0.0000815\n",
      "[0/25][6900/9765] Loss_D: 0.1298 Loss_G: 0.0492 Convergence: 0.1312 k= 0.024112 lr = 0.0000815\n",
      "[0/25][6910/9765] Loss_D: 0.1181 Loss_G: 0.0479 Convergence: 0.1182 k= 0.024128 lr = 0.0000815\n",
      "[0/25][6920/9765] Loss_D: 0.1161 Loss_G: 0.0536 Convergence: 0.1235 k= 0.024165 lr = 0.0000815\n",
      "[0/25][6930/9765] Loss_D: 0.1245 Loss_G: 0.0439 Convergence: 0.1282 k= 0.024256 lr = 0.0000815\n",
      "[0/25][6940/9765] Loss_D: 0.1219 Loss_G: 0.0411 Convergence: 0.1279 k= 0.024293 lr = 0.0000815\n",
      "[0/25][6950/9765] Loss_D: 0.1225 Loss_G: 0.0390 Convergence: 0.1322 k= 0.024355 lr = 0.0000815\n",
      "[0/25][6960/9765] Loss_D: 0.1341 Loss_G: 0.0376 Convergence: 0.1476 k= 0.024399 lr = 0.0000815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][6970/9765] Loss_D: 0.1199 Loss_G: 0.0501 Convergence: 0.1218 k= 0.024407 lr = 0.0000815\n",
      "[0/25][6980/9765] Loss_D: 0.1169 Loss_G: 0.0512 Convergence: 0.1216 k= 0.024417 lr = 0.0000815\n",
      "[0/25][6990/9765] Loss_D: 0.1147 Loss_G: 0.0434 Convergence: 0.1159 k= 0.024456 lr = 0.0000815\n",
      "[0/25][7000/9765] Loss_D: 0.1172 Loss_G: 0.0535 Convergence: 0.1229 k= 0.024498 lr = 0.0000815\n",
      "[0/25][7010/9765] Loss_D: 0.1184 Loss_G: 0.0740 Convergence: 0.1480 k= 0.024512 lr = 0.0000815\n",
      "[0/25][7020/9765] Loss_D: 0.1107 Loss_G: 0.0629 Convergence: 0.1339 k= 0.024527 lr = 0.0000815\n",
      "[0/25][7030/9765] Loss_D: 0.1284 Loss_G: 0.0407 Convergence: 0.1499 k= 0.024589 lr = 0.0000815\n",
      "[0/25][7040/9765] Loss_D: 0.1195 Loss_G: 0.0501 Convergence: 0.1220 k= 0.024625 lr = 0.0000815\n",
      "[0/25][7050/9765] Loss_D: 0.1330 Loss_G: 0.0384 Convergence: 0.1444 k= 0.024610 lr = 0.0000815\n",
      "[0/25][7060/9765] Loss_D: 0.1217 Loss_G: 0.0409 Convergence: 0.1276 k= 0.024649 lr = 0.0000815\n",
      "[0/25][7070/9765] Loss_D: 0.1196 Loss_G: 0.0456 Convergence: 0.1217 k= 0.024742 lr = 0.0000815\n",
      "[0/25][7080/9765] Loss_D: 0.1138 Loss_G: 0.0536 Convergence: 0.1226 k= 0.024774 lr = 0.0000815\n",
      "[0/25][7090/9765] Loss_D: 0.1303 Loss_G: 0.0491 Convergence: 0.1298 k= 0.024823 lr = 0.0000815\n",
      "[0/25][7100/9765] Loss_D: 0.1162 Loss_G: 0.0402 Convergence: 0.1169 k= 0.024890 lr = 0.0000815\n",
      "[0/25][7110/9765] Loss_D: 0.1314 Loss_G: 0.0521 Convergence: 0.1317 k= 0.024919 lr = 0.0000815\n",
      "[0/25][7120/9765] Loss_D: 0.1478 Loss_G: 0.0519 Convergence: 0.1465 k= 0.024916 lr = 0.0000815\n",
      "[0/25][7130/9765] Loss_D: 0.1213 Loss_G: 0.0419 Convergence: 0.1257 k= 0.024978 lr = 0.0000815\n",
      "[0/25][7140/9765] Loss_D: 0.1251 Loss_G: 0.0404 Convergence: 0.1353 k= 0.025008 lr = 0.0000815\n",
      "[0/25][7150/9765] Loss_D: 0.1328 Loss_G: 0.0463 Convergence: 0.1410 k= 0.025064 lr = 0.0000815\n",
      "[0/25][7160/9765] Loss_D: 0.1239 Loss_G: 0.0438 Convergence: 0.1280 k= 0.025073 lr = 0.0000815\n",
      "[0/25][7170/9765] Loss_D: 0.1235 Loss_G: 0.0551 Convergence: 0.1281 k= 0.025107 lr = 0.0000815\n",
      "[0/25][7180/9765] Loss_D: 0.1233 Loss_G: 0.0519 Convergence: 0.1260 k= 0.025123 lr = 0.0000815\n",
      "[0/25][7190/9765] Loss_D: 0.1245 Loss_G: 0.0412 Convergence: 0.1337 k= 0.025182 lr = 0.0000815\n",
      "[0/25][7200/9765] Loss_D: 0.1295 Loss_G: 0.0615 Convergence: 0.1390 k= 0.025196 lr = 0.0000815\n",
      "[0/25][7210/9765] Loss_D: 0.1251 Loss_G: 0.0460 Convergence: 0.1277 k= 0.025225 lr = 0.0000815\n",
      "[0/25][7220/9765] Loss_D: 0.1322 Loss_G: 0.0436 Convergence: 0.1391 k= 0.025292 lr = 0.0000815\n",
      "[0/25][7230/9765] Loss_D: 0.1212 Loss_G: 0.0456 Convergence: 0.1225 k= 0.025348 lr = 0.0000815\n",
      "[0/25][7240/9765] Loss_D: 0.1196 Loss_G: 0.0407 Convergence: 0.1266 k= 0.025427 lr = 0.0000815\n",
      "[0/25][7250/9765] Loss_D: 0.1305 Loss_G: 0.0513 Convergence: 0.1276 k= 0.025488 lr = 0.0000815\n",
      "[0/25][7260/9765] Loss_D: 0.1332 Loss_G: 0.0476 Convergence: 0.1385 k= 0.025532 lr = 0.0000815\n",
      "[0/25][7270/9765] Loss_D: 0.1284 Loss_G: 0.0423 Convergence: 0.1340 k= 0.025593 lr = 0.0000815\n",
      "[0/25][7280/9765] Loss_D: 0.1203 Loss_G: 0.0495 Convergence: 0.1226 k= 0.025637 lr = 0.0000815\n",
      "[0/25][7290/9765] Loss_D: 0.1276 Loss_G: 0.0597 Convergence: 0.1351 k= 0.025659 lr = 0.0000815\n",
      "[0/25][7300/9765] Loss_D: 0.1172 Loss_G: 0.0439 Convergence: 0.1201 k= 0.025649 lr = 0.0000815\n",
      "[0/25][7310/9765] Loss_D: 0.1236 Loss_G: 0.0477 Convergence: 0.1241 k= 0.025704 lr = 0.0000815\n",
      "[0/25][7320/9765] Loss_D: 0.1210 Loss_G: 0.0427 Convergence: 0.1258 k= 0.025758 lr = 0.0000815\n",
      "[0/25][7330/9765] Loss_D: 0.1121 Loss_G: 0.0386 Convergence: 0.1169 k= 0.025799 lr = 0.0000815\n",
      "[0/25][7340/9765] Loss_D: 0.1146 Loss_G: 0.0535 Convergence: 0.1229 k= 0.025852 lr = 0.0000815\n",
      "[0/25][7350/9765] Loss_D: 0.1226 Loss_G: 0.0468 Convergence: 0.1226 k= 0.025914 lr = 0.0000815\n",
      "[0/25][7360/9765] Loss_D: 0.1148 Loss_G: 0.0389 Convergence: 0.1216 k= 0.025944 lr = 0.0000815\n",
      "[0/25][7370/9765] Loss_D: 0.1244 Loss_G: 0.0378 Convergence: 0.1357 k= 0.025979 lr = 0.0000815\n",
      "[0/25][7380/9765] Loss_D: 0.1195 Loss_G: 0.0437 Convergence: 0.1212 k= 0.025987 lr = 0.0000815\n",
      "[0/25][7390/9765] Loss_D: 0.1292 Loss_G: 0.0524 Convergence: 0.1288 k= 0.026055 lr = 0.0000815\n",
      "[0/25][7400/9765] Loss_D: 0.1161 Loss_G: 0.0439 Convergence: 0.1211 k= 0.026102 lr = 0.0000815\n",
      "[0/25][7410/9765] Loss_D: 0.1226 Loss_G: 0.0465 Convergence: 0.1229 k= 0.026094 lr = 0.0000815\n",
      "[0/25][7420/9765] Loss_D: 0.1284 Loss_G: 0.0424 Convergence: 0.1314 k= 0.026118 lr = 0.0000815\n",
      "[0/25][7430/9765] Loss_D: 0.1215 Loss_G: 0.0582 Convergence: 0.1349 k= 0.026148 lr = 0.0000815\n",
      "[0/25][7440/9765] Loss_D: 0.1296 Loss_G: 0.0524 Convergence: 0.1312 k= 0.026169 lr = 0.0000815\n",
      "[0/25][7450/9765] Loss_D: 0.1134 Loss_G: 0.0380 Convergence: 0.1193 k= 0.026203 lr = 0.0000815\n",
      "[0/25][7460/9765] Loss_D: 0.1191 Loss_G: 0.0617 Convergence: 0.1364 k= 0.026241 lr = 0.0000815\n",
      "[0/25][7470/9765] Loss_D: 0.1278 Loss_G: 0.0430 Convergence: 0.1350 k= 0.026267 lr = 0.0000815\n",
      "[0/25][7480/9765] Loss_D: 0.1194 Loss_G: 0.0595 Convergence: 0.1315 k= 0.026268 lr = 0.0000815\n",
      "[0/25][7490/9765] Loss_D: 0.1176 Loss_G: 0.0536 Convergence: 0.1248 k= 0.026307 lr = 0.0000815\n",
      "[0/25][7500/9765] Loss_D: 0.1332 Loss_G: 0.0421 Convergence: 0.1412 k= 0.026354 lr = 0.0000815\n",
      "[0/25][7510/9765] Loss_D: 0.1329 Loss_G: 0.0540 Convergence: 0.1319 k= 0.026392 lr = 0.0000815\n",
      "[0/25][7520/9765] Loss_D: 0.1215 Loss_G: 0.0421 Convergence: 0.1255 k= 0.026419 lr = 0.0000815\n",
      "[0/25][7530/9765] Loss_D: 0.1268 Loss_G: 0.0422 Convergence: 0.1318 k= 0.026428 lr = 0.0000815\n",
      "[0/25][7540/9765] Loss_D: 0.1299 Loss_G: 0.0551 Convergence: 0.1285 k= 0.026473 lr = 0.0000815\n",
      "[0/25][7550/9765] Loss_D: 0.1282 Loss_G: 0.0511 Convergence: 0.1267 k= 0.026410 lr = 0.0000815\n",
      "[0/25][7560/9765] Loss_D: 0.1229 Loss_G: 0.0501 Convergence: 0.1218 k= 0.026396 lr = 0.0000815\n",
      "[0/25][7570/9765] Loss_D: 0.1224 Loss_G: 0.0548 Convergence: 0.1292 k= 0.026384 lr = 0.0000815\n",
      "[0/25][7580/9765] Loss_D: 0.1241 Loss_G: 0.0453 Convergence: 0.1260 k= 0.026429 lr = 0.0000815\n",
      "[0/25][7590/9765] Loss_D: 0.1150 Loss_G: 0.0533 Convergence: 0.1238 k= 0.026420 lr = 0.0000815\n",
      "[0/25][7600/9765] Loss_D: 0.1309 Loss_G: 0.0391 Convergence: 0.1463 k= 0.026480 lr = 0.0000815\n",
      "[0/25][7610/9765] Loss_D: 0.1251 Loss_G: 0.0457 Convergence: 0.1260 k= 0.026495 lr = 0.0000815\n",
      "[0/25][7620/9765] Loss_D: 0.1192 Loss_G: 0.0493 Convergence: 0.1224 k= 0.026547 lr = 0.0000815\n",
      "[0/25][7630/9765] Loss_D: 0.1334 Loss_G: 0.0575 Convergence: 0.1337 k= 0.026578 lr = 0.0000815\n",
      "[0/25][7640/9765] Loss_D: 0.1175 Loss_G: 0.0580 Convergence: 0.1304 k= 0.026614 lr = 0.0000815\n",
      "[0/25][7650/9765] Loss_D: 0.1230 Loss_G: 0.0550 Convergence: 0.1279 k= 0.026619 lr = 0.0000815\n",
      "[0/25][7660/9765] Loss_D: 0.1371 Loss_G: 0.0440 Convergence: 0.1454 k= 0.026646 lr = 0.0000815\n",
      "[0/25][7670/9765] Loss_D: 0.1289 Loss_G: 0.0441 Convergence: 0.1340 k= 0.026690 lr = 0.0000815\n",
      "[0/25][7680/9765] Loss_D: 0.1256 Loss_G: 0.0445 Convergence: 0.1300 k= 0.026730 lr = 0.0000815\n",
      "[0/25][7690/9765] Loss_D: 0.1260 Loss_G: 0.0398 Convergence: 0.1406 k= 0.026801 lr = 0.0000815\n",
      "[0/25][7700/9765] Loss_D: 0.1228 Loss_G: 0.0448 Convergence: 0.1256 k= 0.026823 lr = 0.0000815\n",
      "[0/25][7710/9765] Loss_D: 0.1173 Loss_G: 0.0402 Convergence: 0.1221 k= 0.026868 lr = 0.0000815\n",
      "[0/25][7720/9765] Loss_D: 0.1211 Loss_G: 0.0482 Convergence: 0.1202 k= 0.026921 lr = 0.0000815\n",
      "[0/25][7730/9765] Loss_D: 0.1166 Loss_G: 0.0446 Convergence: 0.1173 k= 0.026970 lr = 0.0000815\n",
      "[0/25][7740/9765] Loss_D: 0.1160 Loss_G: 0.0455 Convergence: 0.1143 k= 0.027014 lr = 0.0000815\n",
      "[0/25][7750/9765] Loss_D: 0.1231 Loss_G: 0.0557 Convergence: 0.1298 k= 0.027058 lr = 0.0000815\n",
      "[0/25][7760/9765] Loss_D: 0.1250 Loss_G: 0.0425 Convergence: 0.1355 k= 0.027099 lr = 0.0000815\n",
      "[0/25][7770/9765] Loss_D: 0.1283 Loss_G: 0.0544 Convergence: 0.1293 k= 0.027136 lr = 0.0000815\n",
      "[0/25][7780/9765] Loss_D: 0.1260 Loss_G: 0.0445 Convergence: 0.1295 k= 0.027197 lr = 0.0000815\n",
      "[0/25][7790/9765] Loss_D: 0.1170 Loss_G: 0.0529 Convergence: 0.1234 k= 0.027246 lr = 0.0000815\n",
      "[0/25][7800/9765] Loss_D: 0.1163 Loss_G: 0.0449 Convergence: 0.1163 k= 0.027311 lr = 0.0000815\n",
      "[0/25][7810/9765] Loss_D: 0.1243 Loss_G: 0.0433 Convergence: 0.1291 k= 0.027346 lr = 0.0000815\n",
      "[0/25][7820/9765] Loss_D: 0.1141 Loss_G: 0.0488 Convergence: 0.1172 k= 0.027387 lr = 0.0000815\n",
      "[0/25][7830/9765] Loss_D: 0.1273 Loss_G: 0.0446 Convergence: 0.1303 k= 0.027398 lr = 0.0000815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][7840/9765] Loss_D: 0.1111 Loss_G: 0.0473 Convergence: 0.1122 k= 0.027402 lr = 0.0000815\n",
      "[0/25][7850/9765] Loss_D: 0.1293 Loss_G: 0.0535 Convergence: 0.1325 k= 0.027403 lr = 0.0000815\n",
      "[0/25][7860/9765] Loss_D: 0.1253 Loss_G: 0.0430 Convergence: 0.1288 k= 0.027410 lr = 0.0000815\n",
      "[0/25][7870/9765] Loss_D: 0.1281 Loss_G: 0.0495 Convergence: 0.1292 k= 0.027452 lr = 0.0000815\n",
      "[0/25][7880/9765] Loss_D: 0.1196 Loss_G: 0.0491 Convergence: 0.1205 k= 0.027493 lr = 0.0000815\n",
      "[0/25][7890/9765] Loss_D: 0.1136 Loss_G: 0.0411 Convergence: 0.1263 k= 0.027526 lr = 0.0000815\n",
      "[0/25][7900/9765] Loss_D: 0.1136 Loss_G: 0.0474 Convergence: 0.1152 k= 0.027531 lr = 0.0000815\n",
      "[0/25][7910/9765] Loss_D: 0.1190 Loss_G: 0.0429 Convergence: 0.1219 k= 0.027560 lr = 0.0000815\n",
      "[0/25][7920/9765] Loss_D: 0.1216 Loss_G: 0.0520 Convergence: 0.1278 k= 0.027607 lr = 0.0000815\n",
      "[0/25][7930/9765] Loss_D: 0.1229 Loss_G: 0.0503 Convergence: 0.1301 k= 0.027613 lr = 0.0000815\n",
      "[0/25][7940/9765] Loss_D: 0.1244 Loss_G: 0.0566 Convergence: 0.1368 k= 0.027616 lr = 0.0000815\n",
      "[0/25][7950/9765] Loss_D: 0.1203 Loss_G: 0.0479 Convergence: 0.1195 k= 0.027592 lr = 0.0000815\n",
      "[0/25][7960/9765] Loss_D: 0.1160 Loss_G: 0.0452 Convergence: 0.1155 k= 0.027626 lr = 0.0000815\n",
      "[0/25][7970/9765] Loss_D: 0.1174 Loss_G: 0.0435 Convergence: 0.1167 k= 0.027644 lr = 0.0000815\n",
      "[0/25][7980/9765] Loss_D: 0.1296 Loss_G: 0.0441 Convergence: 0.1339 k= 0.027718 lr = 0.0000815\n",
      "[0/25][7990/9765] Loss_D: 0.1258 Loss_G: 0.0414 Convergence: 0.1334 k= 0.027769 lr = 0.0000815\n",
      "[0/25][8000/9765] Loss_D: 0.1108 Loss_G: 0.0473 Convergence: 0.1135 k= 0.027827 lr = 0.0000815\n",
      "[0/25][8010/9765] Loss_D: 0.1152 Loss_G: 0.0403 Convergence: 0.1193 k= 0.027848 lr = 0.0000774\n",
      "[0/25][8020/9765] Loss_D: 0.1225 Loss_G: 0.0435 Convergence: 0.1239 k= 0.027885 lr = 0.0000774\n",
      "[0/25][8030/9765] Loss_D: 0.1269 Loss_G: 0.0425 Convergence: 0.1332 k= 0.027947 lr = 0.0000774\n",
      "[0/25][8040/9765] Loss_D: 0.1178 Loss_G: 0.0409 Convergence: 0.1226 k= 0.028007 lr = 0.0000774\n",
      "[0/25][8050/9765] Loss_D: 0.1206 Loss_G: 0.0486 Convergence: 0.1209 k= 0.028010 lr = 0.0000774\n",
      "[0/25][8060/9765] Loss_D: 0.1131 Loss_G: 0.0440 Convergence: 0.1137 k= 0.028039 lr = 0.0000774\n",
      "[0/25][8070/9765] Loss_D: 0.1180 Loss_G: 0.0413 Convergence: 0.1232 k= 0.028048 lr = 0.0000774\n",
      "[0/25][8080/9765] Loss_D: 0.1304 Loss_G: 0.0597 Convergence: 0.1388 k= 0.028077 lr = 0.0000774\n",
      "[0/25][8090/9765] Loss_D: 0.1340 Loss_G: 0.0453 Convergence: 0.1393 k= 0.028130 lr = 0.0000774\n",
      "[0/25][8100/9765] Loss_D: 0.1229 Loss_G: 0.0416 Convergence: 0.1268 k= 0.028205 lr = 0.0000774\n",
      "[0/25][8110/9765] Loss_D: 0.1177 Loss_G: 0.0433 Convergence: 0.1271 k= 0.028258 lr = 0.0000774\n",
      "[0/25][8120/9765] Loss_D: 0.1322 Loss_G: 0.0661 Convergence: 0.1468 k= 0.028254 lr = 0.0000774\n",
      "[0/25][8130/9765] Loss_D: 0.1137 Loss_G: 0.0412 Convergence: 0.1156 k= 0.028288 lr = 0.0000774\n",
      "[0/25][8140/9765] Loss_D: 0.1202 Loss_G: 0.0508 Convergence: 0.1229 k= 0.028360 lr = 0.0000774\n",
      "[0/25][8150/9765] Loss_D: 0.1208 Loss_G: 0.0578 Convergence: 0.1297 k= 0.028362 lr = 0.0000774\n",
      "[0/25][8160/9765] Loss_D: 0.1098 Loss_G: 0.0381 Convergence: 0.1133 k= 0.028399 lr = 0.0000774\n",
      "[0/25][8170/9765] Loss_D: 0.1081 Loss_G: 0.0394 Convergence: 0.1131 k= 0.028455 lr = 0.0000774\n",
      "[0/25][8180/9765] Loss_D: 0.1198 Loss_G: 0.0375 Convergence: 0.1274 k= 0.028499 lr = 0.0000774\n",
      "[0/25][8190/9765] Loss_D: 0.1350 Loss_G: 0.0433 Convergence: 0.1451 k= 0.028540 lr = 0.0000774\n",
      "[0/25][8200/9765] Loss_D: 0.1194 Loss_G: 0.0405 Convergence: 0.1247 k= 0.028574 lr = 0.0000774\n",
      "[0/25][8210/9765] Loss_D: 0.1198 Loss_G: 0.0381 Convergence: 0.1273 k= 0.028622 lr = 0.0000774\n",
      "[0/25][8220/9765] Loss_D: 0.1176 Loss_G: 0.0575 Convergence: 0.1287 k= 0.028645 lr = 0.0000774\n",
      "[0/25][8230/9765] Loss_D: 0.1348 Loss_G: 0.0517 Convergence: 0.1357 k= 0.028693 lr = 0.0000774\n",
      "[0/25][8240/9765] Loss_D: 0.1155 Loss_G: 0.0525 Convergence: 0.1214 k= 0.028723 lr = 0.0000774\n",
      "[0/25][8250/9765] Loss_D: 0.1229 Loss_G: 0.0495 Convergence: 0.1227 k= 0.028750 lr = 0.0000774\n",
      "[0/25][8260/9765] Loss_D: 0.1123 Loss_G: 0.0442 Convergence: 0.1121 k= 0.028763 lr = 0.0000774\n",
      "[0/25][8270/9765] Loss_D: 0.1176 Loss_G: 0.0491 Convergence: 0.1188 k= 0.028795 lr = 0.0000774\n",
      "[0/25][8280/9765] Loss_D: 0.1288 Loss_G: 0.0552 Convergence: 0.1366 k= 0.028811 lr = 0.0000774\n",
      "[0/25][8290/9765] Loss_D: 0.1236 Loss_G: 0.0450 Convergence: 0.1268 k= 0.028843 lr = 0.0000774\n",
      "[0/25][8300/9765] Loss_D: 0.1186 Loss_G: 0.0608 Convergence: 0.1325 k= 0.028872 lr = 0.0000774\n",
      "[0/25][8310/9765] Loss_D: 0.1324 Loss_G: 0.0472 Convergence: 0.1344 k= 0.028915 lr = 0.0000774\n",
      "[0/25][8320/9765] Loss_D: 0.1224 Loss_G: 0.0571 Convergence: 0.1305 k= 0.028956 lr = 0.0000774\n",
      "[0/25][8330/9765] Loss_D: 0.1319 Loss_G: 0.0556 Convergence: 0.1337 k= 0.028981 lr = 0.0000774\n",
      "[0/25][8340/9765] Loss_D: 0.1183 Loss_G: 0.0540 Convergence: 0.1255 k= 0.028938 lr = 0.0000774\n",
      "[0/25][8350/9765] Loss_D: 0.1308 Loss_G: 0.0473 Convergence: 0.1316 k= 0.028990 lr = 0.0000774\n",
      "[0/25][8360/9765] Loss_D: 0.1180 Loss_G: 0.0444 Convergence: 0.1218 k= 0.029055 lr = 0.0000774\n",
      "[0/25][8370/9765] Loss_D: 0.1242 Loss_G: 0.0465 Convergence: 0.1282 k= 0.029093 lr = 0.0000774\n",
      "[0/25][8380/9765] Loss_D: 0.1159 Loss_G: 0.0406 Convergence: 0.1218 k= 0.029153 lr = 0.0000774\n",
      "[0/25][8390/9765] Loss_D: 0.1251 Loss_G: 0.0424 Convergence: 0.1305 k= 0.029163 lr = 0.0000774\n",
      "[0/25][8400/9765] Loss_D: 0.1240 Loss_G: 0.0427 Convergence: 0.1268 k= 0.029184 lr = 0.0000774\n",
      "[0/25][8410/9765] Loss_D: 0.1212 Loss_G: 0.0418 Convergence: 0.1252 k= 0.029208 lr = 0.0000774\n",
      "[0/25][8420/9765] Loss_D: 0.1181 Loss_G: 0.0415 Convergence: 0.1225 k= 0.029239 lr = 0.0000774\n",
      "[0/25][8430/9765] Loss_D: 0.1133 Loss_G: 0.0482 Convergence: 0.1172 k= 0.029285 lr = 0.0000774\n",
      "[0/25][8440/9765] Loss_D: 0.1259 Loss_G: 0.0415 Convergence: 0.1325 k= 0.029336 lr = 0.0000774\n",
      "[0/25][8450/9765] Loss_D: 0.1215 Loss_G: 0.0427 Convergence: 0.1258 k= 0.029395 lr = 0.0000774\n",
      "[0/25][8460/9765] Loss_D: 0.1278 Loss_G: 0.0467 Convergence: 0.1286 k= 0.029432 lr = 0.0000774\n",
      "[0/25][8470/9765] Loss_D: 0.1249 Loss_G: 0.0509 Convergence: 0.1256 k= 0.029476 lr = 0.0000774\n",
      "[0/25][8480/9765] Loss_D: 0.1250 Loss_G: 0.0428 Convergence: 0.1304 k= 0.029528 lr = 0.0000774\n",
      "[0/25][8490/9765] Loss_D: 0.1231 Loss_G: 0.0455 Convergence: 0.1228 k= 0.029570 lr = 0.0000774\n",
      "[0/25][8500/9765] Loss_D: 0.1167 Loss_G: 0.0475 Convergence: 0.1169 k= 0.029616 lr = 0.0000774\n",
      "[0/25][8510/9765] Loss_D: 0.1158 Loss_G: 0.0495 Convergence: 0.1182 k= 0.029621 lr = 0.0000774\n",
      "[0/25][8520/9765] Loss_D: 0.1190 Loss_G: 0.0384 Convergence: 0.1268 k= 0.029671 lr = 0.0000774\n",
      "[0/25][8530/9765] Loss_D: 0.1194 Loss_G: 0.0420 Convergence: 0.1232 k= 0.029689 lr = 0.0000774\n",
      "[0/25][8540/9765] Loss_D: 0.1266 Loss_G: 0.0553 Convergence: 0.1303 k= 0.029703 lr = 0.0000774\n",
      "[0/25][8550/9765] Loss_D: 0.1177 Loss_G: 0.0550 Convergence: 0.1240 k= 0.029686 lr = 0.0000774\n",
      "[0/25][8560/9765] Loss_D: 0.1178 Loss_G: 0.0470 Convergence: 0.1180 k= 0.029690 lr = 0.0000774\n",
      "[0/25][8570/9765] Loss_D: 0.1218 Loss_G: 0.0432 Convergence: 0.1266 k= 0.029738 lr = 0.0000774\n",
      "[0/25][8580/9765] Loss_D: 0.1160 Loss_G: 0.0414 Convergence: 0.1189 k= 0.029736 lr = 0.0000774\n",
      "[0/25][8590/9765] Loss_D: 0.1251 Loss_G: 0.0369 Convergence: 0.1358 k= 0.029768 lr = 0.0000774\n",
      "[0/25][8600/9765] Loss_D: 0.1221 Loss_G: 0.0472 Convergence: 0.1231 k= 0.029828 lr = 0.0000774\n",
      "[0/25][8610/9765] Loss_D: 0.1158 Loss_G: 0.0510 Convergence: 0.1213 k= 0.029890 lr = 0.0000774\n",
      "[0/25][8620/9765] Loss_D: 0.1232 Loss_G: 0.0444 Convergence: 0.1250 k= 0.029919 lr = 0.0000774\n",
      "[0/25][8630/9765] Loss_D: 0.1191 Loss_G: 0.0472 Convergence: 0.1178 k= 0.029970 lr = 0.0000774\n",
      "[0/25][8640/9765] Loss_D: 0.1183 Loss_G: 0.0446 Convergence: 0.1196 k= 0.030047 lr = 0.0000774\n",
      "[0/25][8650/9765] Loss_D: 0.1197 Loss_G: 0.0468 Convergence: 0.1228 k= 0.030088 lr = 0.0000774\n",
      "[0/25][8660/9765] Loss_D: 0.1188 Loss_G: 0.0388 Convergence: 0.1261 k= 0.030121 lr = 0.0000774\n",
      "[0/25][8670/9765] Loss_D: 0.1219 Loss_G: 0.0432 Convergence: 0.1234 k= 0.030187 lr = 0.0000774\n",
      "[0/25][8680/9765] Loss_D: 0.1201 Loss_G: 0.0419 Convergence: 0.1230 k= 0.030217 lr = 0.0000774\n",
      "[0/25][8690/9765] Loss_D: 0.1221 Loss_G: 0.0556 Convergence: 0.1290 k= 0.030224 lr = 0.0000774\n",
      "[0/25][8700/9765] Loss_D: 0.1144 Loss_G: 0.0523 Convergence: 0.1214 k= 0.030246 lr = 0.0000774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][8710/9765] Loss_D: 0.1263 Loss_G: 0.0461 Convergence: 0.1254 k= 0.030277 lr = 0.0000774\n",
      "[0/25][8720/9765] Loss_D: 0.1198 Loss_G: 0.0566 Convergence: 0.1300 k= 0.030259 lr = 0.0000774\n",
      "[0/25][8730/9765] Loss_D: 0.1177 Loss_G: 0.0387 Convergence: 0.1245 k= 0.030306 lr = 0.0000774\n",
      "[0/25][8740/9765] Loss_D: 0.1120 Loss_G: 0.0450 Convergence: 0.1111 k= 0.030354 lr = 0.0000774\n",
      "[0/25][8750/9765] Loss_D: 0.1086 Loss_G: 0.0536 Convergence: 0.1182 k= 0.030331 lr = 0.0000774\n",
      "[0/25][8760/9765] Loss_D: 0.1172 Loss_G: 0.0413 Convergence: 0.1222 k= 0.030371 lr = 0.0000774\n",
      "[0/25][8770/9765] Loss_D: 0.1207 Loss_G: 0.0516 Convergence: 0.1241 k= 0.030404 lr = 0.0000774\n",
      "[0/25][8780/9765] Loss_D: 0.1276 Loss_G: 0.0437 Convergence: 0.1275 k= 0.030430 lr = 0.0000774\n",
      "[0/25][8790/9765] Loss_D: 0.1181 Loss_G: 0.0434 Convergence: 0.1201 k= 0.030453 lr = 0.0000774\n",
      "[0/25][8800/9765] Loss_D: 0.1175 Loss_G: 0.0452 Convergence: 0.1181 k= 0.030466 lr = 0.0000774\n",
      "[0/25][8810/9765] Loss_D: 0.1183 Loss_G: 0.0545 Convergence: 0.1251 k= 0.030459 lr = 0.0000774\n",
      "[0/25][8820/9765] Loss_D: 0.1259 Loss_G: 0.0440 Convergence: 0.1307 k= 0.030511 lr = 0.0000774\n",
      "[0/25][8830/9765] Loss_D: 0.1255 Loss_G: 0.0428 Convergence: 0.1320 k= 0.030573 lr = 0.0000774\n",
      "[0/25][8840/9765] Loss_D: 0.1136 Loss_G: 0.0568 Convergence: 0.1259 k= 0.030601 lr = 0.0000774\n",
      "[0/25][8850/9765] Loss_D: 0.1229 Loss_G: 0.0403 Convergence: 0.1298 k= 0.030660 lr = 0.0000774\n",
      "[0/25][8860/9765] Loss_D: 0.1175 Loss_G: 0.0393 Convergence: 0.1256 k= 0.030725 lr = 0.0000774\n",
      "[0/25][8870/9765] Loss_D: 0.1142 Loss_G: 0.0504 Convergence: 0.1203 k= 0.030761 lr = 0.0000774\n",
      "[0/25][8880/9765] Loss_D: 0.1253 Loss_G: 0.0450 Convergence: 0.1296 k= 0.030790 lr = 0.0000774\n",
      "[0/25][8890/9765] Loss_D: 0.1267 Loss_G: 0.0477 Convergence: 0.1238 k= 0.030816 lr = 0.0000774\n",
      "[0/25][8900/9765] Loss_D: 0.1284 Loss_G: 0.0556 Convergence: 0.1320 k= 0.030829 lr = 0.0000774\n",
      "[0/25][8910/9765] Loss_D: 0.1078 Loss_G: 0.0513 Convergence: 0.1163 k= 0.030828 lr = 0.0000774\n",
      "[0/25][8920/9765] Loss_D: 0.1236 Loss_G: 0.0451 Convergence: 0.1279 k= 0.030855 lr = 0.0000774\n",
      "[0/25][8930/9765] Loss_D: 0.1233 Loss_G: 0.0542 Convergence: 0.1306 k= 0.030851 lr = 0.0000774\n",
      "[0/25][8940/9765] Loss_D: 0.1208 Loss_G: 0.0440 Convergence: 0.1235 k= 0.030865 lr = 0.0000774\n",
      "[0/25][8950/9765] Loss_D: 0.1173 Loss_G: 0.0384 Convergence: 0.1268 k= 0.030918 lr = 0.0000774\n",
      "[0/25][8960/9765] Loss_D: 0.1218 Loss_G: 0.0452 Convergence: 0.1237 k= 0.030965 lr = 0.0000774\n",
      "[0/25][8970/9765] Loss_D: 0.1151 Loss_G: 0.0433 Convergence: 0.1179 k= 0.031013 lr = 0.0000774\n",
      "[0/25][8980/9765] Loss_D: 0.1230 Loss_G: 0.0440 Convergence: 0.1267 k= 0.031061 lr = 0.0000774\n",
      "[0/25][8990/9765] Loss_D: 0.1234 Loss_G: 0.0474 Convergence: 0.1259 k= 0.031083 lr = 0.0000774\n",
      "[0/25][9000/9765] Loss_D: 0.1158 Loss_G: 0.0404 Convergence: 0.1226 k= 0.031128 lr = 0.0000774\n",
      "[0/25][9010/9765] Loss_D: 0.1254 Loss_G: 0.0536 Convergence: 0.1294 k= 0.031149 lr = 0.0000774\n",
      "[0/25][9020/9765] Loss_D: 0.1133 Loss_G: 0.0492 Convergence: 0.1171 k= 0.031160 lr = 0.0000774\n",
      "[0/25][9030/9765] Loss_D: 0.1233 Loss_G: 0.0537 Convergence: 0.1273 k= 0.031186 lr = 0.0000774\n",
      "[0/25][9040/9765] Loss_D: 0.1418 Loss_G: 0.0503 Convergence: 0.1508 k= 0.031217 lr = 0.0000774\n",
      "[0/25][9050/9765] Loss_D: 0.1158 Loss_G: 0.0487 Convergence: 0.1181 k= 0.031223 lr = 0.0000774\n",
      "[0/25][9060/9765] Loss_D: 0.1277 Loss_G: 0.0515 Convergence: 0.1269 k= 0.031258 lr = 0.0000774\n",
      "[0/25][9070/9765] Loss_D: 0.1209 Loss_G: 0.0417 Convergence: 0.1244 k= 0.031274 lr = 0.0000774\n",
      "[0/25][9080/9765] Loss_D: 0.1115 Loss_G: 0.0403 Convergence: 0.1166 k= 0.031342 lr = 0.0000774\n",
      "[0/25][9090/9765] Loss_D: 0.1209 Loss_G: 0.0393 Convergence: 0.1271 k= 0.031381 lr = 0.0000774\n",
      "[0/25][9100/9765] Loss_D: 0.1183 Loss_G: 0.0477 Convergence: 0.1179 k= 0.031379 lr = 0.0000774\n",
      "[0/25][9110/9765] Loss_D: 0.1313 Loss_G: 0.0636 Convergence: 0.1424 k= 0.031374 lr = 0.0000774\n",
      "[0/25][9120/9765] Loss_D: 0.1197 Loss_G: 0.0576 Convergence: 0.1292 k= 0.031368 lr = 0.0000774\n",
      "[0/25][9130/9765] Loss_D: 0.1245 Loss_G: 0.0442 Convergence: 0.1293 k= 0.031400 lr = 0.0000774\n",
      "[0/25][9140/9765] Loss_D: 0.1222 Loss_G: 0.0412 Convergence: 0.1262 k= 0.031427 lr = 0.0000774\n",
      "[0/25][9150/9765] Loss_D: 0.1138 Loss_G: 0.0434 Convergence: 0.1147 k= 0.031457 lr = 0.0000774\n",
      "[0/25][9160/9765] Loss_D: 0.1235 Loss_G: 0.0428 Convergence: 0.1275 k= 0.031481 lr = 0.0000774\n",
      "[0/25][9170/9765] Loss_D: 0.1257 Loss_G: 0.0613 Convergence: 0.1362 k= 0.031501 lr = 0.0000774\n",
      "[0/25][9180/9765] Loss_D: 0.1180 Loss_G: 0.0600 Convergence: 0.1312 k= 0.031485 lr = 0.0000774\n",
      "[0/25][9190/9765] Loss_D: 0.1209 Loss_G: 0.0470 Convergence: 0.1220 k= 0.031474 lr = 0.0000774\n",
      "[0/25][9200/9765] Loss_D: 0.1205 Loss_G: 0.0479 Convergence: 0.1197 k= 0.031463 lr = 0.0000774\n",
      "[0/25][9210/9765] Loss_D: 0.1232 Loss_G: 0.0529 Convergence: 0.1258 k= 0.031465 lr = 0.0000774\n",
      "[0/25][9220/9765] Loss_D: 0.1182 Loss_G: 0.0411 Convergence: 0.1241 k= 0.031492 lr = 0.0000774\n",
      "[0/25][9230/9765] Loss_D: 0.1062 Loss_G: 0.0430 Convergence: 0.1076 k= 0.031521 lr = 0.0000774\n",
      "[0/25][9240/9765] Loss_D: 0.1167 Loss_G: 0.0517 Convergence: 0.1216 k= 0.031550 lr = 0.0000774\n",
      "[0/25][9250/9765] Loss_D: 0.1304 Loss_G: 0.0588 Convergence: 0.1378 k= 0.031541 lr = 0.0000774\n",
      "[0/25][9260/9765] Loss_D: 0.1075 Loss_G: 0.0409 Convergence: 0.1120 k= 0.031553 lr = 0.0000774\n",
      "[0/25][9270/9765] Loss_D: 0.1146 Loss_G: 0.0602 Convergence: 0.1295 k= 0.031568 lr = 0.0000774\n",
      "[0/25][9280/9765] Loss_D: 0.1227 Loss_G: 0.0454 Convergence: 0.1271 k= 0.031592 lr = 0.0000774\n",
      "[0/25][9290/9765] Loss_D: 0.1288 Loss_G: 0.0437 Convergence: 0.1342 k= 0.031615 lr = 0.0000774\n",
      "[0/25][9300/9765] Loss_D: 0.1234 Loss_G: 0.0707 Convergence: 0.1468 k= 0.031611 lr = 0.0000774\n",
      "[0/25][9310/9765] Loss_D: 0.1287 Loss_G: 0.0401 Convergence: 0.1381 k= 0.031657 lr = 0.0000774\n",
      "[0/25][9320/9765] Loss_D: 0.1240 Loss_G: 0.0536 Convergence: 0.1275 k= 0.031680 lr = 0.0000774\n",
      "[0/25][9330/9765] Loss_D: 0.1250 Loss_G: 0.0457 Convergence: 0.1271 k= 0.031713 lr = 0.0000774\n",
      "[0/25][9340/9765] Loss_D: 0.1220 Loss_G: 0.0551 Convergence: 0.1289 k= 0.031748 lr = 0.0000774\n",
      "[0/25][9350/9765] Loss_D: 0.1261 Loss_G: 0.0469 Convergence: 0.1288 k= 0.031773 lr = 0.0000774\n",
      "[0/25][9360/9765] Loss_D: 0.1148 Loss_G: 0.0497 Convergence: 0.1189 k= 0.031787 lr = 0.0000774\n",
      "[0/25][9370/9765] Loss_D: 0.1171 Loss_G: 0.0413 Convergence: 0.1198 k= 0.031783 lr = 0.0000774\n",
      "[0/25][9380/9765] Loss_D: 0.1142 Loss_G: 0.0470 Convergence: 0.1152 k= 0.031824 lr = 0.0000774\n",
      "[0/25][9390/9765] Loss_D: 0.1181 Loss_G: 0.0504 Convergence: 0.1192 k= 0.031827 lr = 0.0000774\n",
      "[0/25][9400/9765] Loss_D: 0.1099 Loss_G: 0.0447 Convergence: 0.1098 k= 0.031828 lr = 0.0000774\n",
      "[0/25][9410/9765] Loss_D: 0.1192 Loss_G: 0.0579 Convergence: 0.1313 k= 0.031859 lr = 0.0000774\n",
      "[0/25][9420/9765] Loss_D: 0.1273 Loss_G: 0.0456 Convergence: 0.1304 k= 0.031884 lr = 0.0000774\n",
      "[0/25][9430/9765] Loss_D: 0.1162 Loss_G: 0.0433 Convergence: 0.1166 k= 0.031911 lr = 0.0000774\n",
      "[0/25][9440/9765] Loss_D: 0.1246 Loss_G: 0.0551 Convergence: 0.1308 k= 0.031901 lr = 0.0000774\n",
      "[0/25][9450/9765] Loss_D: 0.1357 Loss_G: 0.0441 Convergence: 0.1413 k= 0.031947 lr = 0.0000774\n",
      "[0/25][9460/9765] Loss_D: 0.1134 Loss_G: 0.0617 Convergence: 0.1305 k= 0.031976 lr = 0.0000774\n",
      "[0/25][9470/9765] Loss_D: 0.1227 Loss_G: 0.0427 Convergence: 0.1260 k= 0.031869 lr = 0.0000774\n",
      "[0/25][9480/9765] Loss_D: 0.1157 Loss_G: 0.0508 Convergence: 0.1201 k= 0.031832 lr = 0.0000774\n",
      "[0/25][9490/9765] Loss_D: 0.1072 Loss_G: 0.0543 Convergence: 0.1204 k= 0.031829 lr = 0.0000774\n",
      "[0/25][9500/9765] Loss_D: 0.1260 Loss_G: 0.0427 Convergence: 0.1310 k= 0.031835 lr = 0.0000774\n",
      "[0/25][9510/9765] Loss_D: 0.1375 Loss_G: 0.0452 Convergence: 0.1429 k= 0.031884 lr = 0.0000774\n",
      "[0/25][9520/9765] Loss_D: 0.1243 Loss_G: 0.0467 Convergence: 0.1265 k= 0.031921 lr = 0.0000774\n",
      "[0/25][9530/9765] Loss_D: 0.1225 Loss_G: 0.0594 Convergence: 0.1345 k= 0.031964 lr = 0.0000774\n",
      "[0/25][9540/9765] Loss_D: 0.1206 Loss_G: 0.0483 Convergence: 0.1204 k= 0.031993 lr = 0.0000774\n",
      "[0/25][9550/9765] Loss_D: 0.1130 Loss_G: 0.0568 Convergence: 0.1263 k= 0.032012 lr = 0.0000774\n",
      "[0/25][9560/9765] Loss_D: 0.1087 Loss_G: 0.0574 Convergence: 0.1230 k= 0.032037 lr = 0.0000774\n",
      "[0/25][9570/9765] Loss_D: 0.1124 Loss_G: 0.0433 Convergence: 0.1135 k= 0.032090 lr = 0.0000774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][9580/9765] Loss_D: 0.1227 Loss_G: 0.0597 Convergence: 0.1366 k= 0.032114 lr = 0.0000774\n",
      "[0/25][9590/9765] Loss_D: 0.1275 Loss_G: 0.0521 Convergence: 0.1287 k= 0.032149 lr = 0.0000774\n",
      "[0/25][9600/9765] Loss_D: 0.1191 Loss_G: 0.0444 Convergence: 0.1228 k= 0.032148 lr = 0.0000774\n",
      "[0/25][9610/9765] Loss_D: 0.1097 Loss_G: 0.0400 Convergence: 0.1116 k= 0.032156 lr = 0.0000774\n",
      "[0/25][9620/9765] Loss_D: 0.1238 Loss_G: 0.0440 Convergence: 0.1299 k= 0.032194 lr = 0.0000774\n",
      "[0/25][9630/9765] Loss_D: 0.1149 Loss_G: 0.0406 Convergence: 0.1201 k= 0.032242 lr = 0.0000774\n",
      "[0/25][9640/9765] Loss_D: 0.1270 Loss_G: 0.0482 Convergence: 0.1246 k= 0.032286 lr = 0.0000774\n",
      "[0/25][9650/9765] Loss_D: 0.1243 Loss_G: 0.0566 Convergence: 0.1294 k= 0.032295 lr = 0.0000774\n",
      "[0/25][9660/9765] Loss_D: 0.1155 Loss_G: 0.0412 Convergence: 0.1183 k= 0.032308 lr = 0.0000774\n",
      "[0/25][9670/9765] Loss_D: 0.1171 Loss_G: 0.0470 Convergence: 0.1165 k= 0.032343 lr = 0.0000774\n",
      "[0/25][9680/9765] Loss_D: 0.1116 Loss_G: 0.0441 Convergence: 0.1101 k= 0.032364 lr = 0.0000774\n",
      "[0/25][9690/9765] Loss_D: 0.1168 Loss_G: 0.0411 Convergence: 0.1200 k= 0.032401 lr = 0.0000774\n",
      "[0/25][9700/9765] Loss_D: 0.1121 Loss_G: 0.0494 Convergence: 0.1173 k= 0.032448 lr = 0.0000774\n",
      "[0/25][9710/9765] Loss_D: 0.1309 Loss_G: 0.0639 Convergence: 0.1416 k= 0.032422 lr = 0.0000774\n",
      "[0/25][9720/9765] Loss_D: 0.1220 Loss_G: 0.0398 Convergence: 0.1297 k= 0.032452 lr = 0.0000774\n",
      "[0/25][9730/9765] Loss_D: 0.1323 Loss_G: 0.0483 Convergence: 0.1356 k= 0.032456 lr = 0.0000774\n",
      "[0/25][9740/9765] Loss_D: 0.1274 Loss_G: 0.0461 Convergence: 0.1240 k= 0.032452 lr = 0.0000774\n",
      "[0/25][9750/9765] Loss_D: 0.1221 Loss_G: 0.0389 Convergence: 0.1315 k= 0.032471 lr = 0.0000774\n",
      "[0/25][9760/9765] Loss_D: 0.1181 Loss_G: 0.0501 Convergence: 0.1211 k= 0.032469 lr = 0.0000774\n",
      "[1/25][0/9765] Loss_D: 0.1245 Loss_G: 0.0605 Convergence: 0.1350 k= 0.032462 lr = 0.0000774\n",
      "[1/25][10/9765] Loss_D: 0.1021 Loss_G: 0.0421 Convergence: 0.1029 k= 0.032459 lr = 0.0000735\n",
      "[1/25][20/9765] Loss_D: 0.1128 Loss_G: 0.0484 Convergence: 0.1161 k= 0.032483 lr = 0.0000735\n",
      "[1/25][30/9765] Loss_D: 0.1254 Loss_G: 0.0470 Convergence: 0.1281 k= 0.032538 lr = 0.0000735\n",
      "[1/25][40/9765] Loss_D: 0.1297 Loss_G: 0.0430 Convergence: 0.1329 k= 0.032539 lr = 0.0000735\n",
      "[1/25][50/9765] Loss_D: 0.1300 Loss_G: 0.0462 Convergence: 0.1332 k= 0.032556 lr = 0.0000735\n",
      "[1/25][60/9765] Loss_D: 0.1171 Loss_G: 0.0407 Convergence: 0.1206 k= 0.032596 lr = 0.0000735\n",
      "[1/25][70/9765] Loss_D: 0.1206 Loss_G: 0.0540 Convergence: 0.1263 k= 0.032611 lr = 0.0000735\n",
      "[1/25][80/9765] Loss_D: 0.1153 Loss_G: 0.0483 Convergence: 0.1168 k= 0.032583 lr = 0.0000735\n",
      "[1/25][90/9765] Loss_D: 0.1142 Loss_G: 0.0455 Convergence: 0.1147 k= 0.032589 lr = 0.0000735\n",
      "[1/25][100/9765] Loss_D: 0.1112 Loss_G: 0.0442 Convergence: 0.1098 k= 0.032619 lr = 0.0000735\n",
      "[1/25][110/9765] Loss_D: 0.1073 Loss_G: 0.0417 Convergence: 0.1065 k= 0.032658 lr = 0.0000735\n",
      "[1/25][120/9765] Loss_D: 0.1265 Loss_G: 0.0513 Convergence: 0.1269 k= 0.032641 lr = 0.0000735\n",
      "[1/25][130/9765] Loss_D: 0.1217 Loss_G: 0.0520 Convergence: 0.1251 k= 0.032602 lr = 0.0000735\n",
      "[1/25][140/9765] Loss_D: 0.1256 Loss_G: 0.0531 Convergence: 0.1277 k= 0.032624 lr = 0.0000735\n",
      "[1/25][150/9765] Loss_D: 0.1193 Loss_G: 0.0465 Convergence: 0.1162 k= 0.032678 lr = 0.0000735\n",
      "[1/25][160/9765] Loss_D: 0.1211 Loss_G: 0.0495 Convergence: 0.1210 k= 0.032726 lr = 0.0000735\n",
      "[1/25][170/9765] Loss_D: 0.1228 Loss_G: 0.0413 Convergence: 0.1288 k= 0.032727 lr = 0.0000735\n",
      "[1/25][180/9765] Loss_D: 0.1240 Loss_G: 0.0422 Convergence: 0.1282 k= 0.032757 lr = 0.0000735\n",
      "[1/25][190/9765] Loss_D: 0.1076 Loss_G: 0.0473 Convergence: 0.1115 k= 0.032783 lr = 0.0000735\n",
      "[1/25][200/9765] Loss_D: 0.1115 Loss_G: 0.0392 Convergence: 0.1162 k= 0.032817 lr = 0.0000735\n",
      "[1/25][210/9765] Loss_D: 0.1242 Loss_G: 0.0472 Convergence: 0.1268 k= 0.032825 lr = 0.0000735\n",
      "[1/25][220/9765] Loss_D: 0.1235 Loss_G: 0.0442 Convergence: 0.1268 k= 0.032838 lr = 0.0000735\n",
      "[1/25][230/9765] Loss_D: 0.1181 Loss_G: 0.0497 Convergence: 0.1195 k= 0.032842 lr = 0.0000735\n",
      "[1/25][240/9765] Loss_D: 0.1206 Loss_G: 0.0486 Convergence: 0.1203 k= 0.032847 lr = 0.0000735\n",
      "[1/25][250/9765] Loss_D: 0.1339 Loss_G: 0.0450 Convergence: 0.1353 k= 0.032866 lr = 0.0000735\n",
      "[1/25][260/9765] Loss_D: 0.1219 Loss_G: 0.0542 Convergence: 0.1292 k= 0.032859 lr = 0.0000735\n",
      "[1/25][270/9765] Loss_D: 0.1207 Loss_G: 0.0516 Convergence: 0.1240 k= 0.032866 lr = 0.0000735\n",
      "[1/25][280/9765] Loss_D: 0.1171 Loss_G: 0.0475 Convergence: 0.1170 k= 0.032868 lr = 0.0000735\n",
      "[1/25][290/9765] Loss_D: 0.1045 Loss_G: 0.0421 Convergence: 0.1040 k= 0.032886 lr = 0.0000735\n",
      "[1/25][300/9765] Loss_D: 0.1231 Loss_G: 0.0498 Convergence: 0.1219 k= 0.032873 lr = 0.0000735\n",
      "[1/25][310/9765] Loss_D: 0.1230 Loss_G: 0.0500 Convergence: 0.1203 k= 0.032849 lr = 0.0000735\n",
      "[1/25][320/9765] Loss_D: 0.1151 Loss_G: 0.0420 Convergence: 0.1179 k= 0.032836 lr = 0.0000735\n",
      "[1/25][330/9765] Loss_D: 0.1201 Loss_G: 0.0502 Convergence: 0.1216 k= 0.032828 lr = 0.0000735\n",
      "[1/25][340/9765] Loss_D: 0.1229 Loss_G: 0.0536 Convergence: 0.1268 k= 0.032818 lr = 0.0000735\n",
      "[1/25][350/9765] Loss_D: 0.1177 Loss_G: 0.0473 Convergence: 0.1187 k= 0.032836 lr = 0.0000735\n",
      "[1/25][360/9765] Loss_D: 0.1330 Loss_G: 0.0678 Convergence: 0.1479 k= 0.032762 lr = 0.0000735\n",
      "[1/25][370/9765] Loss_D: 0.1253 Loss_G: 0.0452 Convergence: 0.1244 k= 0.032704 lr = 0.0000735\n",
      "[1/25][380/9765] Loss_D: 0.1205 Loss_G: 0.0472 Convergence: 0.1207 k= 0.032712 lr = 0.0000735\n",
      "[1/25][390/9765] Loss_D: 0.1203 Loss_G: 0.0472 Convergence: 0.1266 k= 0.032732 lr = 0.0000735\n",
      "[1/25][400/9765] Loss_D: 0.1152 Loss_G: 0.0505 Convergence: 0.1188 k= 0.032699 lr = 0.0000735\n",
      "[1/25][410/9765] Loss_D: 0.1197 Loss_G: 0.0518 Convergence: 0.1233 k= 0.032682 lr = 0.0000735\n",
      "[1/25][420/9765] Loss_D: 0.1208 Loss_G: 0.0417 Convergence: 0.1239 k= 0.032686 lr = 0.0000735\n",
      "[1/25][430/9765] Loss_D: 0.1211 Loss_G: 0.0412 Convergence: 0.1257 k= 0.032688 lr = 0.0000735\n",
      "[1/25][440/9765] Loss_D: 0.1231 Loss_G: 0.0445 Convergence: 0.1260 k= 0.032707 lr = 0.0000735\n",
      "[1/25][450/9765] Loss_D: 0.1154 Loss_G: 0.0468 Convergence: 0.1150 k= 0.032744 lr = 0.0000735\n",
      "[1/25][460/9765] Loss_D: 0.1146 Loss_G: 0.0441 Convergence: 0.1148 k= 0.032759 lr = 0.0000735\n",
      "[1/25][470/9765] Loss_D: 0.1202 Loss_G: 0.0462 Convergence: 0.1226 k= 0.032777 lr = 0.0000735\n",
      "[1/25][480/9765] Loss_D: 0.1161 Loss_G: 0.0467 Convergence: 0.1154 k= 0.032777 lr = 0.0000735\n",
      "[1/25][490/9765] Loss_D: 0.1045 Loss_G: 0.0590 Convergence: 0.1232 k= 0.032798 lr = 0.0000735\n",
      "[1/25][500/9765] Loss_D: 0.1171 Loss_G: 0.0450 Convergence: 0.1182 k= 0.032824 lr = 0.0000735\n",
      "[1/25][510/9765] Loss_D: 0.1176 Loss_G: 0.0480 Convergence: 0.1178 k= 0.032845 lr = 0.0000735\n",
      "[1/25][520/9765] Loss_D: 0.1268 Loss_G: 0.0618 Convergence: 0.1384 k= 0.032843 lr = 0.0000735\n",
      "[1/25][530/9765] Loss_D: 0.1392 Loss_G: 0.0495 Convergence: 0.1673 k= 0.032890 lr = 0.0000735\n",
      "[1/25][540/9765] Loss_D: 0.1225 Loss_G: 0.0554 Convergence: 0.1284 k= 0.032804 lr = 0.0000735\n",
      "[1/25][550/9765] Loss_D: 0.1291 Loss_G: 0.0457 Convergence: 0.1331 k= 0.032800 lr = 0.0000735\n",
      "[1/25][560/9765] Loss_D: 0.1284 Loss_G: 0.0442 Convergence: 0.1336 k= 0.032798 lr = 0.0000735\n",
      "[1/25][570/9765] Loss_D: 0.1283 Loss_G: 0.0413 Convergence: 0.1348 k= 0.032782 lr = 0.0000735\n",
      "[1/25][580/9765] Loss_D: 0.1021 Loss_G: 0.0423 Convergence: 0.1038 k= 0.032801 lr = 0.0000735\n",
      "[1/25][590/9765] Loss_D: 0.1248 Loss_G: 0.0433 Convergence: 0.1238 k= 0.032808 lr = 0.0000735\n",
      "[1/25][600/9765] Loss_D: 0.1279 Loss_G: 0.0507 Convergence: 0.1329 k= 0.032821 lr = 0.0000735\n",
      "[1/25][610/9765] Loss_D: 0.1157 Loss_G: 0.0455 Convergence: 0.1180 k= 0.032803 lr = 0.0000735\n",
      "[1/25][620/9765] Loss_D: 0.1249 Loss_G: 0.0547 Convergence: 0.1297 k= 0.032803 lr = 0.0000735\n",
      "[1/25][630/9765] Loss_D: 0.1183 Loss_G: 0.0467 Convergence: 0.1173 k= 0.032827 lr = 0.0000735\n",
      "[1/25][640/9765] Loss_D: 0.1125 Loss_G: 0.0557 Convergence: 0.1224 k= 0.032767 lr = 0.0000735\n",
      "[1/25][650/9765] Loss_D: 0.1161 Loss_G: 0.0459 Convergence: 0.1152 k= 0.032773 lr = 0.0000735\n",
      "[1/25][660/9765] Loss_D: 0.1179 Loss_G: 0.0477 Convergence: 0.1178 k= 0.032786 lr = 0.0000735\n",
      "[1/25][670/9765] Loss_D: 0.1109 Loss_G: 0.0534 Convergence: 0.1191 k= 0.032776 lr = 0.0000735\n",
      "[1/25][680/9765] Loss_D: 0.1190 Loss_G: 0.0524 Convergence: 0.1222 k= 0.032800 lr = 0.0000735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][690/9765] Loss_D: 0.1178 Loss_G: 0.0532 Convergence: 0.1232 k= 0.032797 lr = 0.0000735\n",
      "[1/25][700/9765] Loss_D: 0.1222 Loss_G: 0.0484 Convergence: 0.1227 k= 0.032810 lr = 0.0000735\n",
      "[1/25][710/9765] Loss_D: 0.1182 Loss_G: 0.0616 Convergence: 0.1348 k= 0.032831 lr = 0.0000735\n",
      "[1/25][720/9765] Loss_D: 0.1135 Loss_G: 0.0456 Convergence: 0.1129 k= 0.032837 lr = 0.0000735\n",
      "[1/25][730/9765] Loss_D: 0.1177 Loss_G: 0.0426 Convergence: 0.1201 k= 0.032838 lr = 0.0000735\n",
      "[1/25][740/9765] Loss_D: 0.1184 Loss_G: 0.0460 Convergence: 0.1180 k= 0.032834 lr = 0.0000735\n",
      "[1/25][750/9765] Loss_D: 0.1059 Loss_G: 0.0520 Convergence: 0.1148 k= 0.032834 lr = 0.0000735\n",
      "[1/25][760/9765] Loss_D: 0.1130 Loss_G: 0.0545 Convergence: 0.1229 k= 0.032846 lr = 0.0000735\n",
      "[1/25][770/9765] Loss_D: 0.1322 Loss_G: 0.0556 Convergence: 0.1335 k= 0.032839 lr = 0.0000735\n",
      "[1/25][780/9765] Loss_D: 0.1154 Loss_G: 0.0493 Convergence: 0.1183 k= 0.032803 lr = 0.0000735\n",
      "[1/25][790/9765] Loss_D: 0.1230 Loss_G: 0.0521 Convergence: 0.1252 k= 0.032815 lr = 0.0000735\n",
      "[1/25][800/9765] Loss_D: 0.1235 Loss_G: 0.0561 Convergence: 0.1261 k= 0.032829 lr = 0.0000735\n",
      "[1/25][810/9765] Loss_D: 0.1130 Loss_G: 0.0688 Convergence: 0.1374 k= 0.032724 lr = 0.0000735\n",
      "[1/25][820/9765] Loss_D: 0.1143 Loss_G: 0.0496 Convergence: 0.1168 k= 0.032726 lr = 0.0000735\n",
      "[1/25][830/9765] Loss_D: 0.1180 Loss_G: 0.0560 Convergence: 0.1267 k= 0.032672 lr = 0.0000735\n",
      "[1/25][840/9765] Loss_D: 0.1172 Loss_G: 0.0493 Convergence: 0.1193 k= 0.032671 lr = 0.0000735\n",
      "[1/25][850/9765] Loss_D: 0.1152 Loss_G: 0.0464 Convergence: 0.1151 k= 0.032690 lr = 0.0000735\n",
      "[1/25][860/9765] Loss_D: 0.1128 Loss_G: 0.0407 Convergence: 0.1157 k= 0.032733 lr = 0.0000735\n",
      "[1/25][870/9765] Loss_D: 0.1301 Loss_G: 0.0644 Convergence: 0.1427 k= 0.032704 lr = 0.0000735\n",
      "[1/25][880/9765] Loss_D: 0.1135 Loss_G: 0.0439 Convergence: 0.1134 k= 0.032710 lr = 0.0000735\n",
      "[1/25][890/9765] Loss_D: 0.1192 Loss_G: 0.0477 Convergence: 0.1186 k= 0.032703 lr = 0.0000735\n",
      "[1/25][900/9765] Loss_D: 0.1241 Loss_G: 0.0525 Convergence: 0.1253 k= 0.032709 lr = 0.0000735\n",
      "[1/25][910/9765] Loss_D: 0.1144 Loss_G: 0.0478 Convergence: 0.1164 k= 0.032713 lr = 0.0000735\n",
      "[1/25][920/9765] Loss_D: 0.1276 Loss_G: 0.0484 Convergence: 0.1278 k= 0.032715 lr = 0.0000735\n",
      "[1/25][930/9765] Loss_D: 0.1168 Loss_G: 0.0444 Convergence: 0.1216 k= 0.032736 lr = 0.0000735\n",
      "[1/25][940/9765] Loss_D: 0.1190 Loss_G: 0.0587 Convergence: 0.1325 k= 0.032692 lr = 0.0000735\n",
      "[1/25][950/9765] Loss_D: 0.1258 Loss_G: 0.0430 Convergence: 0.1305 k= 0.032653 lr = 0.0000735\n",
      "[1/25][960/9765] Loss_D: 0.1199 Loss_G: 0.0550 Convergence: 0.1267 k= 0.032597 lr = 0.0000735\n",
      "[1/25][970/9765] Loss_D: 0.1121 Loss_G: 0.0522 Convergence: 0.1191 k= 0.032577 lr = 0.0000735\n",
      "[1/25][980/9765] Loss_D: 0.1128 Loss_G: 0.0499 Convergence: 0.1217 k= 0.032583 lr = 0.0000735\n",
      "[1/25][990/9765] Loss_D: 0.1067 Loss_G: 0.0512 Convergence: 0.1155 k= 0.032582 lr = 0.0000735\n",
      "[1/25][1000/9765] Loss_D: 0.1143 Loss_G: 0.0533 Convergence: 0.1208 k= 0.032534 lr = 0.0000735\n",
      "[1/25][1010/9765] Loss_D: 0.1236 Loss_G: 0.0617 Convergence: 0.1356 k= 0.032532 lr = 0.0000735\n",
      "[1/25][1020/9765] Loss_D: 0.1149 Loss_G: 0.0484 Convergence: 0.1171 k= 0.032529 lr = 0.0000735\n",
      "[1/25][1030/9765] Loss_D: 0.1148 Loss_G: 0.0444 Convergence: 0.1153 k= 0.032521 lr = 0.0000735\n",
      "[1/25][1040/9765] Loss_D: 0.1119 Loss_G: 0.0473 Convergence: 0.1142 k= 0.032531 lr = 0.0000735\n",
      "[1/25][1050/9765] Loss_D: 0.1408 Loss_G: 0.0643 Convergence: 0.1414 k= 0.032483 lr = 0.0000735\n",
      "[1/25][1060/9765] Loss_D: 0.1176 Loss_G: 0.0432 Convergence: 0.1194 k= 0.032450 lr = 0.0000735\n",
      "[1/25][1070/9765] Loss_D: 0.1178 Loss_G: 0.0481 Convergence: 0.1189 k= 0.032449 lr = 0.0000735\n",
      "[1/25][1080/9765] Loss_D: 0.1110 Loss_G: 0.0541 Convergence: 0.1215 k= 0.032472 lr = 0.0000735\n",
      "[1/25][1090/9765] Loss_D: 0.1120 Loss_G: 0.0482 Convergence: 0.1150 k= 0.032467 lr = 0.0000735\n",
      "[1/25][1100/9765] Loss_D: 0.1197 Loss_G: 0.0492 Convergence: 0.1205 k= 0.032451 lr = 0.0000735\n",
      "[1/25][1110/9765] Loss_D: 0.1229 Loss_G: 0.0452 Convergence: 0.1244 k= 0.032437 lr = 0.0000735\n",
      "[1/25][1120/9765] Loss_D: 0.1208 Loss_G: 0.0509 Convergence: 0.1227 k= 0.032422 lr = 0.0000735\n",
      "[1/25][1130/9765] Loss_D: 0.1160 Loss_G: 0.0501 Convergence: 0.1204 k= 0.032411 lr = 0.0000735\n",
      "[1/25][1140/9765] Loss_D: 0.1200 Loss_G: 0.0438 Convergence: 0.1228 k= 0.032416 lr = 0.0000735\n",
      "[1/25][1150/9765] Loss_D: 0.1206 Loss_G: 0.0536 Convergence: 0.1271 k= 0.032411 lr = 0.0000735\n",
      "[1/25][1160/9765] Loss_D: 0.1164 Loss_G: 0.0537 Convergence: 0.1229 k= 0.032410 lr = 0.0000735\n",
      "[1/25][1170/9765] Loss_D: 0.1247 Loss_G: 0.0469 Convergence: 0.1274 k= 0.032410 lr = 0.0000735\n",
      "[1/25][1180/9765] Loss_D: 0.1229 Loss_G: 0.0507 Convergence: 0.1240 k= 0.032397 lr = 0.0000735\n",
      "[1/25][1190/9765] Loss_D: 0.1189 Loss_G: 0.0584 Convergence: 0.1294 k= 0.032374 lr = 0.0000735\n",
      "[1/25][1200/9765] Loss_D: 0.1178 Loss_G: 0.0532 Convergence: 0.1224 k= 0.032348 lr = 0.0000735\n",
      "[1/25][1210/9765] Loss_D: 0.1344 Loss_G: 0.0483 Convergence: 0.1375 k= 0.032335 lr = 0.0000735\n",
      "[1/25][1220/9765] Loss_D: 0.1120 Loss_G: 0.0625 Convergence: 0.1321 k= 0.032301 lr = 0.0000735\n",
      "[1/25][1230/9765] Loss_D: 0.1217 Loss_G: 0.0452 Convergence: 0.1233 k= 0.032304 lr = 0.0000735\n",
      "[1/25][1240/9765] Loss_D: 0.1157 Loss_G: 0.0529 Convergence: 0.1247 k= 0.032274 lr = 0.0000735\n",
      "[1/25][1250/9765] Loss_D: 0.1251 Loss_G: 0.0839 Convergence: 0.1605 k= 0.032189 lr = 0.0000735\n",
      "[1/25][1260/9765] Loss_D: 0.1176 Loss_G: 0.0531 Convergence: 0.1231 k= 0.032160 lr = 0.0000735\n",
      "[1/25][1270/9765] Loss_D: 0.1141 Loss_G: 0.0498 Convergence: 0.1185 k= 0.032168 lr = 0.0000735\n",
      "[1/25][1280/9765] Loss_D: 0.1207 Loss_G: 0.0489 Convergence: 0.1215 k= 0.032177 lr = 0.0000735\n",
      "[1/25][1290/9765] Loss_D: 0.1157 Loss_G: 0.0499 Convergence: 0.1190 k= 0.032166 lr = 0.0000735\n",
      "[1/25][1300/9765] Loss_D: 0.1177 Loss_G: 0.0467 Convergence: 0.1166 k= 0.032177 lr = 0.0000735\n",
      "[1/25][1310/9765] Loss_D: 0.1145 Loss_G: 0.0569 Convergence: 0.1254 k= 0.032131 lr = 0.0000735\n",
      "[1/25][1320/9765] Loss_D: 0.1158 Loss_G: 0.0451 Convergence: 0.1141 k= 0.032129 lr = 0.0000735\n",
      "[1/25][1330/9765] Loss_D: 0.1198 Loss_G: 0.0509 Convergence: 0.1241 k= 0.032083 lr = 0.0000735\n",
      "[1/25][1340/9765] Loss_D: 0.1174 Loss_G: 0.0467 Convergence: 0.1160 k= 0.032050 lr = 0.0000735\n",
      "[1/25][1350/9765] Loss_D: 0.1203 Loss_G: 0.0560 Convergence: 0.1279 k= 0.032040 lr = 0.0000735\n",
      "[1/25][1360/9765] Loss_D: 0.1359 Loss_G: 0.0466 Convergence: 0.1404 k= 0.032015 lr = 0.0000735\n",
      "[1/25][1370/9765] Loss_D: 0.1143 Loss_G: 0.0441 Convergence: 0.1145 k= 0.032001 lr = 0.0000735\n",
      "[1/25][1380/9765] Loss_D: 0.1158 Loss_G: 0.0662 Convergence: 0.1375 k= 0.031968 lr = 0.0000735\n",
      "[1/25][1390/9765] Loss_D: 0.1152 Loss_G: 0.0528 Convergence: 0.1230 k= 0.031944 lr = 0.0000735\n",
      "[1/25][1400/9765] Loss_D: 0.1159 Loss_G: 0.0465 Convergence: 0.1152 k= 0.031921 lr = 0.0000735\n",
      "[1/25][1410/9765] Loss_D: 0.1070 Loss_G: 0.0456 Convergence: 0.1097 k= 0.031906 lr = 0.0000735\n",
      "[1/25][1420/9765] Loss_D: 0.1199 Loss_G: 0.0461 Convergence: 0.1193 k= 0.031862 lr = 0.0000735\n",
      "[1/25][1430/9765] Loss_D: 0.1088 Loss_G: 0.0489 Convergence: 0.1144 k= 0.031842 lr = 0.0000735\n",
      "[1/25][1440/9765] Loss_D: 0.1034 Loss_G: 0.0482 Convergence: 0.1099 k= 0.031824 lr = 0.0000735\n",
      "[1/25][1450/9765] Loss_D: 0.1151 Loss_G: 0.0496 Convergence: 0.1179 k= 0.031814 lr = 0.0000735\n",
      "[1/25][1460/9765] Loss_D: 0.1234 Loss_G: 0.0429 Convergence: 0.1298 k= 0.031808 lr = 0.0000735\n",
      "[1/25][1470/9765] Loss_D: 0.1199 Loss_G: 0.0511 Convergence: 0.1223 k= 0.031760 lr = 0.0000735\n",
      "[1/25][1480/9765] Loss_D: 0.1171 Loss_G: 0.0577 Convergence: 0.1265 k= 0.031710 lr = 0.0000735\n",
      "[1/25][1490/9765] Loss_D: 0.1123 Loss_G: 0.0471 Convergence: 0.1153 k= 0.031690 lr = 0.0000735\n",
      "[1/25][1500/9765] Loss_D: 0.1089 Loss_G: 0.0459 Convergence: 0.1114 k= 0.031635 lr = 0.0000735\n",
      "[1/25][1510/9765] Loss_D: 0.1202 Loss_G: 0.0665 Convergence: 0.1398 k= 0.031592 lr = 0.0000735\n",
      "[1/25][1520/9765] Loss_D: 0.1167 Loss_G: 0.0457 Convergence: 0.1154 k= 0.031581 lr = 0.0000735\n",
      "[1/25][1530/9765] Loss_D: 0.1180 Loss_G: 0.0506 Convergence: 0.1207 k= 0.031570 lr = 0.0000735\n",
      "[1/25][1540/9765] Loss_D: 0.1289 Loss_G: 0.0580 Convergence: 0.1343 k= 0.031528 lr = 0.0000735\n",
      "[1/25][1550/9765] Loss_D: 0.1108 Loss_G: 0.0562 Convergence: 0.1217 k= 0.031470 lr = 0.0000735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][1560/9765] Loss_D: 0.1176 Loss_G: 0.0461 Convergence: 0.1170 k= 0.031431 lr = 0.0000735\n",
      "[1/25][1570/9765] Loss_D: 0.1074 Loss_G: 0.0446 Convergence: 0.1083 k= 0.031410 lr = 0.0000735\n",
      "[1/25][1580/9765] Loss_D: 0.1178 Loss_G: 0.0406 Convergence: 0.1218 k= 0.031401 lr = 0.0000735\n",
      "[1/25][1590/9765] Loss_D: 0.1083 Loss_G: 0.0569 Convergence: 0.1247 k= 0.031384 lr = 0.0000735\n",
      "[1/25][1600/9765] Loss_D: 0.1244 Loss_G: 0.0581 Convergence: 0.1342 k= 0.031386 lr = 0.0000735\n",
      "[1/25][1610/9765] Loss_D: 0.1182 Loss_G: 0.0494 Convergence: 0.1194 k= 0.031371 lr = 0.0000735\n",
      "[1/25][1620/9765] Loss_D: 0.1144 Loss_G: 0.0580 Convergence: 0.1270 k= 0.031350 lr = 0.0000735\n",
      "[1/25][1630/9765] Loss_D: 0.1213 Loss_G: 0.0511 Convergence: 0.1241 k= 0.031316 lr = 0.0000735\n",
      "[1/25][1640/9765] Loss_D: 0.1223 Loss_G: 0.0502 Convergence: 0.1227 k= 0.031294 lr = 0.0000735\n",
      "[1/25][1650/9765] Loss_D: 0.1271 Loss_G: 0.0451 Convergence: 0.1318 k= 0.031271 lr = 0.0000735\n",
      "[1/25][1660/9765] Loss_D: 0.1122 Loss_G: 0.0491 Convergence: 0.1160 k= 0.031255 lr = 0.0000735\n",
      "[1/25][1670/9765] Loss_D: 0.1183 Loss_G: 0.0460 Convergence: 0.1203 k= 0.031219 lr = 0.0000735\n",
      "[1/25][1680/9765] Loss_D: 0.1165 Loss_G: 0.0531 Convergence: 0.1242 k= 0.031193 lr = 0.0000735\n",
      "[1/25][1690/9765] Loss_D: 0.1189 Loss_G: 0.0475 Convergence: 0.1182 k= 0.031170 lr = 0.0000735\n",
      "[1/25][1700/9765] Loss_D: 0.1221 Loss_G: 0.0513 Convergence: 0.1259 k= 0.031163 lr = 0.0000735\n",
      "[1/25][1710/9765] Loss_D: 0.1238 Loss_G: 0.0508 Convergence: 0.1244 k= 0.031131 lr = 0.0000735\n",
      "[1/25][1720/9765] Loss_D: 0.1285 Loss_G: 0.0460 Convergence: 0.1324 k= 0.031116 lr = 0.0000735\n",
      "[1/25][1730/9765] Loss_D: 0.1265 Loss_G: 0.0421 Convergence: 0.1346 k= 0.031081 lr = 0.0000735\n",
      "[1/25][1740/9765] Loss_D: 0.1137 Loss_G: 0.0507 Convergence: 0.1184 k= 0.030988 lr = 0.0000735\n",
      "[1/25][1750/9765] Loss_D: 0.1097 Loss_G: 0.0515 Convergence: 0.1175 k= 0.030967 lr = 0.0000735\n",
      "[1/25][1760/9765] Loss_D: 0.1225 Loss_G: 0.0502 Convergence: 0.1251 k= 0.030941 lr = 0.0000735\n",
      "[1/25][1770/9765] Loss_D: 0.1205 Loss_G: 0.0539 Convergence: 0.1257 k= 0.030905 lr = 0.0000735\n",
      "[1/25][1780/9765] Loss_D: 0.1163 Loss_G: 0.0538 Convergence: 0.1241 k= 0.030915 lr = 0.0000735\n",
      "[1/25][1790/9765] Loss_D: 0.1127 Loss_G: 0.0562 Convergence: 0.1247 k= 0.030865 lr = 0.0000735\n",
      "[1/25][1800/9765] Loss_D: 0.1144 Loss_G: 0.0465 Convergence: 0.1151 k= 0.030811 lr = 0.0000735\n",
      "[1/25][1810/9765] Loss_D: 0.1149 Loss_G: 0.0544 Convergence: 0.1236 k= 0.030786 lr = 0.0000735\n",
      "[1/25][1820/9765] Loss_D: 0.1211 Loss_G: 0.0596 Convergence: 0.1330 k= 0.030749 lr = 0.0000735\n",
      "[1/25][1830/9765] Loss_D: 0.1219 Loss_G: 0.0550 Convergence: 0.1292 k= 0.030718 lr = 0.0000735\n",
      "[1/25][1840/9765] Loss_D: 0.1177 Loss_G: 0.0458 Convergence: 0.1171 k= 0.030712 lr = 0.0000735\n",
      "[1/25][1850/9765] Loss_D: 0.1134 Loss_G: 0.0449 Convergence: 0.1116 k= 0.030684 lr = 0.0000735\n",
      "[1/25][1860/9765] Loss_D: 0.1175 Loss_G: 0.0477 Convergence: 0.1179 k= 0.030666 lr = 0.0000735\n",
      "[1/25][1870/9765] Loss_D: 0.1266 Loss_G: 0.0566 Convergence: 0.1322 k= 0.030642 lr = 0.0000735\n",
      "[1/25][1880/9765] Loss_D: 0.1169 Loss_G: 0.0474 Convergence: 0.1178 k= 0.030631 lr = 0.0000735\n",
      "[1/25][1890/9765] Loss_D: 0.1182 Loss_G: 0.0483 Convergence: 0.1178 k= 0.030567 lr = 0.0000735\n",
      "[1/25][1900/9765] Loss_D: 0.1130 Loss_G: 0.0477 Convergence: 0.1160 k= 0.030527 lr = 0.0000735\n",
      "[1/25][1910/9765] Loss_D: 0.1303 Loss_G: 0.0516 Convergence: 0.1288 k= 0.030508 lr = 0.0000735\n",
      "[1/25][1920/9765] Loss_D: 0.1259 Loss_G: 0.0624 Convergence: 0.1370 k= 0.030457 lr = 0.0000735\n",
      "[1/25][1930/9765] Loss_D: 0.1207 Loss_G: 0.0409 Convergence: 0.1280 k= 0.030482 lr = 0.0000735\n",
      "[1/25][1940/9765] Loss_D: 0.1117 Loss_G: 0.0502 Convergence: 0.1166 k= 0.030457 lr = 0.0000735\n",
      "[1/25][1950/9765] Loss_D: 0.1160 Loss_G: 0.0477 Convergence: 0.1161 k= 0.030431 lr = 0.0000735\n",
      "[1/25][1960/9765] Loss_D: 0.1115 Loss_G: 0.0548 Convergence: 0.1223 k= 0.030379 lr = 0.0000735\n",
      "[1/25][1970/9765] Loss_D: 0.1227 Loss_G: 0.0553 Convergence: 0.1269 k= 0.030318 lr = 0.0000735\n",
      "[1/25][1980/9765] Loss_D: 0.1156 Loss_G: 0.0495 Convergence: 0.1188 k= 0.030256 lr = 0.0000735\n",
      "[1/25][1990/9765] Loss_D: 0.1092 Loss_G: 0.0501 Convergence: 0.1150 k= 0.030245 lr = 0.0000735\n",
      "[1/25][2000/9765] Loss_D: 0.1248 Loss_G: 0.0594 Convergence: 0.1306 k= 0.030173 lr = 0.0000735\n",
      "[1/25][2010/9765] Loss_D: 0.1085 Loss_G: 0.0432 Convergence: 0.1077 k= 0.030096 lr = 0.0000698\n",
      "[1/25][2020/9765] Loss_D: 0.1081 Loss_G: 0.0563 Convergence: 0.1216 k= 0.030097 lr = 0.0000698\n",
      "[1/25][2030/9765] Loss_D: 0.1187 Loss_G: 0.0470 Convergence: 0.1189 k= 0.029981 lr = 0.0000698\n",
      "[1/25][2040/9765] Loss_D: 0.1098 Loss_G: 0.0630 Convergence: 0.1280 k= 0.029926 lr = 0.0000698\n",
      "[1/25][2050/9765] Loss_D: 0.1072 Loss_G: 0.0487 Convergence: 0.1128 k= 0.029876 lr = 0.0000698\n",
      "[1/25][2060/9765] Loss_D: 0.1131 Loss_G: 0.0518 Convergence: 0.1199 k= 0.029887 lr = 0.0000698\n",
      "[1/25][2070/9765] Loss_D: 0.1115 Loss_G: 0.0510 Convergence: 0.1179 k= 0.029852 lr = 0.0000698\n",
      "[1/25][2080/9765] Loss_D: 0.1158 Loss_G: 0.0497 Convergence: 0.1175 k= 0.029802 lr = 0.0000698\n",
      "[1/25][2090/9765] Loss_D: 0.1284 Loss_G: 0.0538 Convergence: 0.1301 k= 0.029778 lr = 0.0000698\n",
      "[1/25][2100/9765] Loss_D: 0.1154 Loss_G: 0.0485 Convergence: 0.1172 k= 0.029731 lr = 0.0000698\n",
      "[1/25][2110/9765] Loss_D: 0.1179 Loss_G: 0.0581 Convergence: 0.1287 k= 0.029702 lr = 0.0000698\n",
      "[1/25][2120/9765] Loss_D: 0.1131 Loss_G: 0.0613 Convergence: 0.1295 k= 0.029685 lr = 0.0000698\n",
      "[1/25][2130/9765] Loss_D: 0.1090 Loss_G: 0.0487 Convergence: 0.1133 k= 0.029662 lr = 0.0000698\n",
      "[1/25][2140/9765] Loss_D: 0.1234 Loss_G: 0.0462 Convergence: 0.1276 k= 0.029663 lr = 0.0000698\n",
      "[1/25][2150/9765] Loss_D: 0.1146 Loss_G: 0.0471 Convergence: 0.1167 k= 0.029697 lr = 0.0000698\n",
      "[1/25][2160/9765] Loss_D: 0.1194 Loss_G: 0.0800 Convergence: 0.1512 k= 0.029554 lr = 0.0000698\n",
      "[1/25][2170/9765] Loss_D: 0.1128 Loss_G: 0.0513 Convergence: 0.1179 k= 0.029539 lr = 0.0000698\n",
      "[1/25][2180/9765] Loss_D: 0.1192 Loss_G: 0.0574 Convergence: 0.1284 k= 0.029449 lr = 0.0000698\n",
      "[1/25][2190/9765] Loss_D: 0.1167 Loss_G: 0.0454 Convergence: 0.1199 k= 0.029445 lr = 0.0000698\n",
      "[1/25][2200/9765] Loss_D: 0.1118 Loss_G: 0.0484 Convergence: 0.1149 k= 0.029399 lr = 0.0000698\n",
      "[1/25][2210/9765] Loss_D: 0.1129 Loss_G: 0.0500 Convergence: 0.1173 k= 0.029356 lr = 0.0000698\n",
      "[1/25][2220/9765] Loss_D: 0.1149 Loss_G: 0.0470 Convergence: 0.1157 k= 0.029346 lr = 0.0000698\n",
      "[1/25][2230/9765] Loss_D: 0.1165 Loss_G: 0.0515 Convergence: 0.1230 k= 0.029315 lr = 0.0000698\n",
      "[1/25][2240/9765] Loss_D: 0.1135 Loss_G: 0.0448 Convergence: 0.1115 k= 0.029280 lr = 0.0000698\n",
      "[1/25][2250/9765] Loss_D: 0.1202 Loss_G: 0.0595 Convergence: 0.1335 k= 0.029260 lr = 0.0000698\n",
      "[1/25][2260/9765] Loss_D: 0.1276 Loss_G: 0.0416 Convergence: 0.1329 k= 0.029256 lr = 0.0000698\n",
      "[1/25][2270/9765] Loss_D: 0.1171 Loss_G: 0.0518 Convergence: 0.1222 k= 0.029249 lr = 0.0000698\n",
      "[1/25][2280/9765] Loss_D: 0.1092 Loss_G: 0.0496 Convergence: 0.1149 k= 0.029217 lr = 0.0000698\n",
      "[1/25][2290/9765] Loss_D: 0.1229 Loss_G: 0.0467 Convergence: 0.1345 k= 0.029184 lr = 0.0000698\n",
      "[1/25][2300/9765] Loss_D: 0.1166 Loss_G: 0.0562 Convergence: 0.1261 k= 0.029228 lr = 0.0000698\n",
      "[1/25][2310/9765] Loss_D: 0.1200 Loss_G: 0.0527 Convergence: 0.1259 k= 0.029074 lr = 0.0000698\n",
      "[1/25][2320/9765] Loss_D: 0.1235 Loss_G: 0.0488 Convergence: 0.1203 k= 0.029111 lr = 0.0000698\n",
      "[1/25][2330/9765] Loss_D: 0.1270 Loss_G: 0.0700 Convergence: 0.1451 k= 0.029012 lr = 0.0000698\n",
      "[1/25][2340/9765] Loss_D: 0.1215 Loss_G: 0.0414 Convergence: 0.1265 k= 0.029005 lr = 0.0000698\n",
      "[1/25][2350/9765] Loss_D: 0.1230 Loss_G: 0.0645 Convergence: 0.1371 k= 0.028978 lr = 0.0000698\n",
      "[1/25][2360/9765] Loss_D: 0.1124 Loss_G: 0.0469 Convergence: 0.1139 k= 0.028948 lr = 0.0000698\n",
      "[1/25][2370/9765] Loss_D: 0.1196 Loss_G: 0.0517 Convergence: 0.1220 k= 0.028920 lr = 0.0000698\n",
      "[1/25][2380/9765] Loss_D: 0.1139 Loss_G: 0.0475 Convergence: 0.1146 k= 0.028883 lr = 0.0000698\n",
      "[1/25][2390/9765] Loss_D: 0.1187 Loss_G: 0.0481 Convergence: 0.1201 k= 0.028862 lr = 0.0000698\n",
      "[1/25][2400/9765] Loss_D: 0.1096 Loss_G: 0.0466 Convergence: 0.1118 k= 0.028833 lr = 0.0000698\n",
      "[1/25][2410/9765] Loss_D: 0.1074 Loss_G: 0.0502 Convergence: 0.1143 k= 0.028770 lr = 0.0000698\n",
      "[1/25][2420/9765] Loss_D: 0.1249 Loss_G: 0.0631 Convergence: 0.1370 k= 0.028707 lr = 0.0000698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][2430/9765] Loss_D: 0.1105 Loss_G: 0.0481 Convergence: 0.1144 k= 0.028692 lr = 0.0000698\n",
      "[1/25][2440/9765] Loss_D: 0.1167 Loss_G: 0.0551 Convergence: 0.1246 k= 0.028651 lr = 0.0000698\n",
      "[1/25][2450/9765] Loss_D: 0.1180 Loss_G: 0.0460 Convergence: 0.1183 k= 0.028648 lr = 0.0000698\n",
      "[1/25][2460/9765] Loss_D: 0.1126 Loss_G: 0.0489 Convergence: 0.1154 k= 0.028623 lr = 0.0000698\n",
      "[1/25][2470/9765] Loss_D: 0.1194 Loss_G: 0.0544 Convergence: 0.1252 k= 0.028587 lr = 0.0000698\n",
      "[1/25][2480/9765] Loss_D: 0.1134 Loss_G: 0.0463 Convergence: 0.1136 k= 0.028553 lr = 0.0000698\n",
      "[1/25][2490/9765] Loss_D: 0.1155 Loss_G: 0.0546 Convergence: 0.1236 k= 0.028535 lr = 0.0000698\n",
      "[1/25][2500/9765] Loss_D: 0.1155 Loss_G: 0.0518 Convergence: 0.1203 k= 0.028503 lr = 0.0000698\n",
      "[1/25][2510/9765] Loss_D: 0.1202 Loss_G: 0.0501 Convergence: 0.1214 k= 0.028478 lr = 0.0000698\n",
      "[1/25][2520/9765] Loss_D: 0.1090 Loss_G: 0.0513 Convergence: 0.1168 k= 0.028421 lr = 0.0000698\n",
      "[1/25][2530/9765] Loss_D: 0.1186 Loss_G: 0.0453 Convergence: 0.1194 k= 0.028405 lr = 0.0000698\n",
      "[1/25][2540/9765] Loss_D: 0.1156 Loss_G: 0.0553 Convergence: 0.1251 k= 0.028316 lr = 0.0000698\n",
      "[1/25][2550/9765] Loss_D: 0.1161 Loss_G: 0.0497 Convergence: 0.1180 k= 0.028276 lr = 0.0000698\n",
      "[1/25][2560/9765] Loss_D: 0.1133 Loss_G: 0.0526 Convergence: 0.1205 k= 0.028219 lr = 0.0000698\n",
      "[1/25][2570/9765] Loss_D: 0.1233 Loss_G: 0.0569 Convergence: 0.1318 k= 0.028242 lr = 0.0000698\n",
      "[1/25][2580/9765] Loss_D: 0.1154 Loss_G: 0.0460 Convergence: 0.1187 k= 0.028200 lr = 0.0000698\n",
      "[1/25][2590/9765] Loss_D: 0.1153 Loss_G: 0.0371 Convergence: 0.1227 k= 0.028298 lr = 0.0000698\n",
      "[1/25][2600/9765] Loss_D: 0.1217 Loss_G: 0.0885 Convergence: 0.1615 k= 0.028208 lr = 0.0000698\n",
      "[1/25][2610/9765] Loss_D: 0.1125 Loss_G: 0.0406 Convergence: 0.1144 k= 0.028115 lr = 0.0000698\n",
      "[1/25][2620/9765] Loss_D: 0.1198 Loss_G: 0.0421 Convergence: 0.1237 k= 0.028178 lr = 0.0000698\n",
      "[1/25][2630/9765] Loss_D: 0.1102 Loss_G: 0.0546 Convergence: 0.1204 k= 0.027963 lr = 0.0000698\n",
      "[1/25][2640/9765] Loss_D: 0.1163 Loss_G: 0.0533 Convergence: 0.1226 k= 0.027928 lr = 0.0000698\n",
      "[1/25][2650/9765] Loss_D: 0.1243 Loss_G: 0.0540 Convergence: 0.1277 k= 0.027882 lr = 0.0000698\n",
      "[1/25][2660/9765] Loss_D: 0.1086 Loss_G: 0.0496 Convergence: 0.1140 k= 0.027826 lr = 0.0000698\n",
      "[1/25][2670/9765] Loss_D: 0.1112 Loss_G: 0.0507 Convergence: 0.1165 k= 0.027782 lr = 0.0000698\n",
      "[1/25][2680/9765] Loss_D: 0.1214 Loss_G: 0.0464 Convergence: 0.1271 k= 0.027770 lr = 0.0000698\n",
      "[1/25][2690/9765] Loss_D: 0.1080 Loss_G: 0.0588 Convergence: 0.1237 k= 0.027689 lr = 0.0000698\n",
      "[1/25][2700/9765] Loss_D: 0.1094 Loss_G: 0.0422 Convergence: 0.1093 k= 0.027690 lr = 0.0000698\n",
      "[1/25][2710/9765] Loss_D: 0.1188 Loss_G: 0.0618 Convergence: 0.1413 k= 0.027652 lr = 0.0000698\n",
      "[1/25][2720/9765] Loss_D: 0.1131 Loss_G: 0.0453 Convergence: 0.1129 k= 0.027580 lr = 0.0000698\n",
      "[1/25][2730/9765] Loss_D: 0.1371 Loss_G: 0.0468 Convergence: 0.1432 k= 0.027580 lr = 0.0000698\n",
      "[1/25][2740/9765] Loss_D: 0.1254 Loss_G: 0.0532 Convergence: 0.1293 k= 0.027536 lr = 0.0000698\n",
      "[1/25][2750/9765] Loss_D: 0.1135 Loss_G: 0.0444 Convergence: 0.1135 k= 0.027487 lr = 0.0000698\n",
      "[1/25][2760/9765] Loss_D: 0.1112 Loss_G: 0.0509 Convergence: 0.1186 k= 0.027458 lr = 0.0000698\n",
      "[1/25][2770/9765] Loss_D: 0.1300 Loss_G: 0.0534 Convergence: 0.1304 k= 0.027408 lr = 0.0000698\n",
      "[1/25][2780/9765] Loss_D: 0.1176 Loss_G: 0.0502 Convergence: 0.1215 k= 0.027373 lr = 0.0000698\n",
      "[1/25][2790/9765] Loss_D: 0.1145 Loss_G: 0.0558 Convergence: 0.1242 k= 0.027345 lr = 0.0000698\n",
      "[1/25][2800/9765] Loss_D: 0.1121 Loss_G: 0.0563 Convergence: 0.1244 k= 0.027312 lr = 0.0000698\n",
      "[1/25][2810/9765] Loss_D: 0.1213 Loss_G: 0.0648 Convergence: 0.1383 k= 0.027236 lr = 0.0000698\n",
      "[1/25][2820/9765] Loss_D: 0.1134 Loss_G: 0.0498 Convergence: 0.1176 k= 0.027180 lr = 0.0000698\n",
      "[1/25][2830/9765] Loss_D: 0.1071 Loss_G: 0.0496 Convergence: 0.1140 k= 0.027123 lr = 0.0000698\n",
      "[1/25][2840/9765] Loss_D: 0.1164 Loss_G: 0.0577 Convergence: 0.1272 k= 0.027064 lr = 0.0000698\n",
      "[1/25][2850/9765] Loss_D: 0.1213 Loss_G: 0.0542 Convergence: 0.1269 k= 0.027001 lr = 0.0000698\n",
      "[1/25][2860/9765] Loss_D: 0.1094 Loss_G: 0.0562 Convergence: 0.1217 k= 0.026902 lr = 0.0000698\n",
      "[1/25][2870/9765] Loss_D: 0.1154 Loss_G: 0.0524 Convergence: 0.1262 k= 0.026826 lr = 0.0000698\n",
      "[1/25][2880/9765] Loss_D: 0.1150 Loss_G: 0.0468 Convergence: 0.1153 k= 0.026778 lr = 0.0000698\n",
      "[1/25][2890/9765] Loss_D: 0.1099 Loss_G: 0.0520 Convergence: 0.1176 k= 0.026703 lr = 0.0000698\n",
      "[1/25][2900/9765] Loss_D: 0.1117 Loss_G: 0.0618 Convergence: 0.1271 k= 0.026614 lr = 0.0000698\n",
      "[1/25][2910/9765] Loss_D: 0.1211 Loss_G: 0.0558 Convergence: 0.1277 k= 0.026566 lr = 0.0000698\n",
      "[1/25][2920/9765] Loss_D: 0.1171 Loss_G: 0.0502 Convergence: 0.1199 k= 0.026495 lr = 0.0000698\n",
      "[1/25][2930/9765] Loss_D: 0.1188 Loss_G: 0.0447 Convergence: 0.1187 k= 0.026466 lr = 0.0000698\n",
      "[1/25][2940/9765] Loss_D: 0.1235 Loss_G: 0.0772 Convergence: 0.1499 k= 0.026375 lr = 0.0000698\n",
      "[1/25][2950/9765] Loss_D: 0.1219 Loss_G: 0.0463 Convergence: 0.1221 k= 0.026378 lr = 0.0000698\n",
      "[1/25][2960/9765] Loss_D: 0.1039 Loss_G: 0.0490 Convergence: 0.1102 k= 0.026319 lr = 0.0000698\n",
      "[1/25][2970/9765] Loss_D: 0.1156 Loss_G: 0.0519 Convergence: 0.1204 k= 0.026252 lr = 0.0000698\n",
      "[1/25][2980/9765] Loss_D: 0.1267 Loss_G: 0.0485 Convergence: 0.1260 k= 0.026234 lr = 0.0000698\n",
      "[1/25][2990/9765] Loss_D: 0.1144 Loss_G: 0.0478 Convergence: 0.1159 k= 0.026216 lr = 0.0000698\n",
      "[1/25][3000/9765] Loss_D: 0.1186 Loss_G: 0.0475 Convergence: 0.1179 k= 0.026169 lr = 0.0000698\n",
      "[1/25][3010/9765] Loss_D: 0.1193 Loss_G: 0.0505 Convergence: 0.1238 k= 0.026115 lr = 0.0000698\n",
      "[1/25][3020/9765] Loss_D: 0.1110 Loss_G: 0.0541 Convergence: 0.1192 k= 0.026102 lr = 0.0000698\n",
      "[1/25][3030/9765] Loss_D: 0.1230 Loss_G: 0.0616 Convergence: 0.1336 k= 0.026008 lr = 0.0000698\n",
      "[1/25][3040/9765] Loss_D: 0.1143 Loss_G: 0.0521 Convergence: 0.1200 k= 0.026001 lr = 0.0000698\n",
      "[1/25][3050/9765] Loss_D: 0.1203 Loss_G: 0.0539 Convergence: 0.1254 k= 0.025976 lr = 0.0000698\n",
      "[1/25][3060/9765] Loss_D: 0.1073 Loss_G: 0.0524 Convergence: 0.1163 k= 0.025884 lr = 0.0000698\n",
      "[1/25][3070/9765] Loss_D: 0.1222 Loss_G: 0.0491 Convergence: 0.1223 k= 0.025883 lr = 0.0000698\n",
      "[1/25][3080/9765] Loss_D: 0.1032 Loss_G: 0.0564 Convergence: 0.1181 k= 0.025786 lr = 0.0000698\n",
      "[1/25][3090/9765] Loss_D: 0.1173 Loss_G: 0.0481 Convergence: 0.1216 k= 0.025805 lr = 0.0000698\n",
      "[1/25][3100/9765] Loss_D: 0.1149 Loss_G: 0.0766 Convergence: 0.1499 k= 0.025703 lr = 0.0000698\n",
      "[1/25][3110/9765] Loss_D: 0.1154 Loss_G: 0.0425 Convergence: 0.1181 k= 0.025629 lr = 0.0000698\n",
      "[1/25][3120/9765] Loss_D: 0.1053 Loss_G: 0.0643 Convergence: 0.1293 k= 0.025606 lr = 0.0000698\n",
      "[1/25][3130/9765] Loss_D: 0.1273 Loss_G: 0.0596 Convergence: 0.1348 k= 0.025505 lr = 0.0000698\n",
      "[1/25][3140/9765] Loss_D: 0.1096 Loss_G: 0.0466 Convergence: 0.1120 k= 0.025488 lr = 0.0000698\n",
      "[1/25][3150/9765] Loss_D: 0.1163 Loss_G: 0.0476 Convergence: 0.1173 k= 0.025380 lr = 0.0000698\n",
      "[1/25][3160/9765] Loss_D: 0.1212 Loss_G: 0.0558 Convergence: 0.1317 k= 0.025355 lr = 0.0000698\n"
     ]
    }
   ],
   "source": [
    "data_loss_D = []\n",
    "data_loss_G = []\n",
    "data_loss_k = []\n",
    "data_loss_global = []\n",
    "for epoch in range(opt.niter):\n",
    "    index = get_permutation()\n",
    "    for i in range(int(mini_batch)):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # prepare real\n",
    "        input.data.copy_(get_samples(index, i))\n",
    "#         real_cpu, _ = data\n",
    "#         batch_size = real_cpu.size(0)\n",
    "#         input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "\n",
    "        # train with real\n",
    "        output = netD(input)\n",
    "        errD_real = criterion_L1(output, input)  # score on real\n",
    "        errD_real.backward()  # backward on score on real\n",
    "        L_x = errD_real.data[0]  # score fore supervision\n",
    "        \n",
    "        # generate fake\n",
    "        #noise.data.resize_(batch_size, nz, 1, 1)\n",
    "        noise.data.uniform_(-1, 1)\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        # train with fake\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion_L1(output, fake.detach())  # score on fake\n",
    "        errD_fake_use = - k * errD_fake\n",
    "        errD_fake_use.backward()  # backward on score on fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #D_G_z1 = errD_fake.data.mean()  # score fore supervision <- generated when calc D loss\n",
    "        errD = errD_real + errD_fake_use  # score fore supervision\n",
    "\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # generate fake\n",
    "#         noise.data.resize_(batch_size, nz, 1, 1)\n",
    "#         noise.data.uniform_(-1, 1)\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        # NOT reuse generated fake samples\n",
    "        output = netD(fake)\n",
    "        errG = (output - fake).abs().mean()  # L1\n",
    "        errG.backward()\n",
    "        \n",
    "        L_G = errG.data[0] # score fore supervision <- generated when calc G loss\n",
    "\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # K STEP\n",
    "        #left -> real\n",
    "        output = netD(input)\n",
    "        errD_real_k = criterion_L1(output, input)  # score on real\n",
    "        #right -> fake\n",
    "        fake = netG(noise)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake_k = criterion_L1(output, fake.detach())  # score on fake\n",
    "        \n",
    "        #Convergence Measure\n",
    "        cm = errD_real_k.data[0] + abs(opt.gamma * errD_real_k.data[0] - L_G)\n",
    "        \n",
    "        k += 0.001 * (opt.gamma * errD_real_k.data[0] - errD_fake_k.data[0])\n",
    "        k = max(min(k, 1), 0)\n",
    "        ############################\n",
    "        # (3) Report & 100 Batch checkpoint\n",
    "        ############################\n",
    "        data_loss_G.append(errG.data[0])\n",
    "        data_loss_D.append(errD.data[0])\n",
    "        data_loss_k.append(k)\n",
    "        data_loss_global.append(cm)\n",
    "        if i % 2000 == 1:\n",
    "            optimizerD = optim.Adam(netD.parameters(), lr=opt.lr*opt.lr_decay, betas=(0.5, 0.999))\n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=opt.lr*opt.lr_decay, betas=(0.5, 0.999))\n",
    "            opt.lr = opt.lr * opt.lr_decay\n",
    "        if i % 10 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Convergence: %.4f k= %.6f lr = %.7f'\n",
    "                    % (epoch, opt.niter, i, int(mini_batch),\n",
    "                     errD.data[0], errG.data[0], cm, k, opt.lr))\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                              opt.base_dir+'%d_fake_samples_epoch_%03d.png' % (i, epoch), normalize = True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), opt.base_dir+'netG_epoch_%d.pth' % (epoch))\n",
    "    torch.save(netD.state_dict(), opt.base_dir+'netD_epoch_%d.pth' % (epoch))\n",
    "    np.save(opt.base_dir+'k.npy', np.array(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
